# #Meta推出新世界模型##LeCun亲自介绍世界模型#就在昨晚，Meta震撼发布世界模型V-JEPA 2，人类距离高级机器智能（AMI）又近了一步！【图1】Meta首席AI科学家Yann ...

**URL**: https://weibo.com/6105753431/Pwan638h5

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23Meta%E6%8E%A8%E5%87%BA%E6%96%B0%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23Meta%E6%8E%A8%E5%87%BA%E6%96%B0%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#Meta推出新世界模型#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23LeCun%E4%BA%B2%E8%87%AA%E4%BB%8B%E7%BB%8D%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23LeCun%E4%BA%B2%E8%87%AA%E4%BB%8B%E7%BB%8D%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#LeCun亲自介绍世界模型#</span></a><br><br>就在昨晚，Meta震撼发布世界模型V-JEPA 2，人类距离高级机器智能（AMI）又近了一步！【图1】<br><br>Meta首席AI科学家Yann LeCun出镜，亲自为大家介绍究竟什么才是世界模型：【视频2】<br><br>“世界模型就像现实的抽象数字孪生，AI可借此理解世界、预测行为后果，从而规划行动以实现目标。它无需数百万次试错就能学习新事物，因为世界模型提供了对世界运作方式的根本理解。”<br><br>简单来说，就是让要让AI智能体也能“三思而后行”的能力，具备理解、预测与规划的能力。<br><br>根据Meta的介绍，V-JEPA 2拥有12亿参数，能够像人类一样认知世界、规划陌生任务执行方案，并能高效适应不断变化环境。<br><br>一起来看看，V-JEPA 2究竟在理解、预测与规划上做得怎么样：<br><br>1、理解<br><br>世界模型能理解对世界的观察，包括识别视频中的物体、动作和运动。<br><br>当视频里有人跳水时，V-JEPA 2能理解他的动作细节，比如“跳水、向前、1.5周空翻、无转体”。<br><br>2、预测<br><br>世界模型能预测世界会如何发展，以及当AI采取行动时世界会怎样变化。【视频3】<br><br>比如，看到有人快要煎完蛋，旁边放着盘子和盐，V-JEPA 2就能预测这双手下一步会拿起盘子。【视频4】<br><br>3、规划<br>基于预测能力，世界模型应当有助于规划实现特定目标的一系列行动。<br><br>根据下达的指令，机器人能够主动前往厨房并拿回水杯。【视频5】<br><br>V-JEPA 2的背后是Meta2022年推出的联合嵌入预测架构(JEPA)，包含两个主要组件：【视频6】<br><br>- 编码器：接收原始视频输入，输出能够捕捉观察到的世界状态有用语义信息的嵌入。<br><br>- 预测器：接收视频嵌入和关于预测内容的额外上下文，输出预测的嵌入。<br><br>V-JEPA 2完全使用视频进行自监督学习，无需进行额外人工标注。<br><br>训练可以分为两个阶段：首先在100万小时以上的互联网视频和图像上进行无动作预训练，随后仅使用62小时未标记的机器人轨迹数据（Droid数据集）进行后训练。<br><br>在各项基准测试中，V-JEPA 2的表现也相当亮眼：<br><br>- 在Something-Something v2数据集上，准确率达到77.3%。<br><br>- 在人类动作预测方面，它超越了之前的模型，在Epic-Kitchens-100上的召回率<a href="https://weibo.com/n/5%E8%BE%BE%E5%88%B039">@5达到39</a>.7%，比之前的最高水平提高了44%。【图7】<br><br>- 与语言模型对齐后，形成的多模态语言模型在多个测试中获得最高分，击败了所有其他8B级模型，甚至在某些指标上超越了PerceptionLM。【图8】<br><br>除此之外，Meta还同步推出了三个基准模型，用于评估现有模型通过视频进行物理世界推理的能力。<br><br>Meta表示：虽然人类在这三项基准测试中准确率能达到85%-95%，但包括V-JEPA 2在内的顶级模型与人类表现之间仍存在显著差距，这为模型改进指明了重要方向。<br><br>- IntPhys 2：专门用于衡量模型区分物理合理与不合理场景的能力【图9】<br><br>- Minimal Video Pairs (MVPBench)：通过多选题形式评估视频语言模型的物理理解能力【图10】<br><br>- CausalVQA：用于评估视频-语言模型在回答物理因果关系问题方面的能力【图11】<br><br>目前，V-JEPA 2只能在单一时间尺度上学习和预测。未来，Meta将重点开发具有层级结构的JEPA模型，使其能够跨越不同时空尺度进行学习、推理和规划。<br><br>我们离Yann LeCun所描述的图景“现实世界AI助手能处理家务和体力任务，能改善数十亿人日常生活的系统”成为现实，又更近了一步。<img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i2ccmu076jj30yk0zktph.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i2ccoylujpj31hc0u0gn6.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i2ccoym7hgj30zk0k0mxl.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i2ccoyuxl7j30zk0k0aax.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i2ccozte9pj30zk0k0jss.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i2ccnzqapvg31hc0g47wn.gif" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i2cco04jprj30x50dptdf.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i2cco1w34ij30vb0n5wmx.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i2ccok0mtbg30mg06o1l3.gif" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax2.sinaimg.cn/orj480/006Fd7o3ly1i2ccoz3xlxj31hc0u0gn6.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/UXreKy3Flx08oYJJgWas010412016oxI0E010.mp4?label=mp4_720p&amp;template=1280x720.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1749704711&amp;ssig=ZxyFzeVlAs&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/T9AsEl0Xlx08oYJHW5KU01041200AaSy0E010.mp4?label=mp4_hd&amp;template=852x480.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1749704711&amp;ssig=sJXaW0qp1l&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/1pjg7hVplx08oYJHPsGA01041200nc6z0E010.mp4?label=mp4_ld&amp;template=640x360.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1749704711&amp;ssig=QIKYxK2MFk&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5176662967648315" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

Meta近日发布了新一代世界模型V-JEPA 2，该模型拥有12亿参数，通过自监督学习理解世界、预测行为并规划行动。它能识别视频中的物体动作（如跳水细节），预测后续行为（如煎蛋后拿盘子），并执行任务（如取水杯）。基于联合嵌入预测架构(JEPA)，V-JEPA 2在动作识别（Something-Something v2准确率77.3%）和物理推理测试中表现优异，但较人类水平仍有差距。未来Meta计划开发多尺度JEPA模型，推动AI助手实现日常任务处理，向高级机器智能(AMI)迈进。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-12T04:06:01Z
- **目录日期**: 2025-06-12
