# How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies

**URL**: http://arxiv.org/abs/2506.09977v1

## 原始摘要

Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.


## AI 摘要

研究表明，人类在更新信念时更倾向于基于解释的修订方式，而非经典信念修正理论所预测的最小化调整。通过三项用户实验发现，无论是否提供解释，人们面对矛盾信息时都会依赖解释来调整信念体系，即使这种调整并非理论上的"最小修改"。这一发现对AI系统设计具有重要启示：为了更好地模拟人类推理并与人类思维对齐，AI系统需要支持基于解释的、可能非最小化的信念修正机制。这挑战了传统信念修正理论，为开发更符合人类认知特点的AI系统提供了实证依据。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-12T22:01:20Z
- **目录日期**: 2025-06-12
