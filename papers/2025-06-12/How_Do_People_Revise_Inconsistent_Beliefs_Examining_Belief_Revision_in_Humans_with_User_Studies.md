# How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies

**URL**: http://arxiv.org/abs/2506.09977v1

## 原始摘要

Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.


## AI 摘要

人类在接收新信息时倾向于基于解释来修正信念，而非遵循经典信念修正理论的最小化原则。三项用户研究表明，无论解释是由外部提供还是自行构建，人们都偏好通过解释引导的非最小化信念调整。这一发现与认知心理学证据一致，揭示了人类信念修正的实际模式与理论框架的差异。对AI系统设计具有重要意义，建议开发能兼容解释性、非最小化信念修正的模型，以更好地模拟人类推理并实现人机认知对齐。研究为构建更符合人类思维方式的AI提供了实证基础。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-12T17:01:48Z
- **目录日期**: 2025-06-12
