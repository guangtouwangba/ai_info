# EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits

**URL**: http://arxiv.org/abs/2506.09988v1

## 原始摘要

Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.


## AI 摘要

随着生成式AI的进步，文本引导的图像编辑技术日益普及，亟需系统化的质量评估框架。为此，研究者提出EditInspector基准，基于人工标注的编辑验证模板，全面评估文本引导编辑效果。该研究测试了前沿视觉语言模型在准确性、伪影检测、视觉质量等维度的表现，发现现有模型难以全面评估编辑效果，且容易产生幻觉描述。针对这些问题，研究者提出两种新方法，在伪影检测和差异描述生成方面优于现有最优模型。该工作为提升AI辅助图像编辑的可靠性提供了重要基准和方法创新。（99字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-12T11:01:14Z
- **目录日期**: 2025-06-12
