# #用多模态LLM超越YOLOv3##强化学习突破多模态感知极限# 超越YOLOv3、Faster-RCNN，首个在COCO2017 val set上突破30AP的纯多模态开源LLM来啦！华中科技大学、北京...

**URL**: https://weibo.com/6105753431/Pq5QRhi24

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E7%94%A8%E5%A4%9A%E6%A8%A1%E6%80%81LLM%E8%B6%85%E8%B6%8AYOLOv3%23&amp;extparam=%23%E7%94%A8%E5%A4%9A%E6%A8%A1%E6%80%81LLM%E8%B6%85%E8%B6%8AYOLOv3%23" data-hide=""><span class="surl-text">#用多模态LLM超越YOLOv3#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AA%81%E7%A0%B4%E5%A4%9A%E6%A8%A1%E6%80%81%E6%84%9F%E7%9F%A5%E6%9E%81%E9%99%90%23&amp;extparam=%23%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AA%81%E7%A0%B4%E5%A4%9A%E6%A8%A1%E6%80%81%E6%84%9F%E7%9F%A5%E6%9E%81%E9%99%90%23" data-hide=""><span class="surl-text">#强化学习突破多模态感知极限#</span></a> <br><br>超越YOLOv3、Faster-RCNN，首个在COCO2017 val set上突破30AP的纯多模态开源LLM来啦！<br><br>华中科技大学、北京邮电大学等多所高校研究团队共同推出的Perception-R1（PR1），在视觉推理中最基础的感知层面，探究rule-based RL能给模型感知pattern带来的增益。<br><br>PR1重点关注当下主流的纯视觉（计数，通用目标检测）以及视觉语言（grounding，OCR）任务，实验结果展现出在模型感知策略上的巨大潜力。<br><br>目前论文和代码模型均已开源，作者希望其工作能给社区提供一个强大的baseline来支持后续研究。<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FIA2cnGfoC-hv8n_g4yCcDg" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">用多模态LLM超越YOLOv3！强化学习突破多模态感知极限｜开源</span></a><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3ly1i126gvuochj30om06twfc.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

华中科技大学与北京邮电大学等团队联合推出多模态大模型Perception-R1（PR1），首次在COCO2017验证集上突破30AP，超越YOLOv3和Faster-RCNN。PR1通过基于规则的强化学习（RL）增强模型感知能力，专注于纯视觉（目标检测、计数）和视觉语言（OCR、grounding）任务，展现出多模态感知策略的潜力。该研究为社区提供了开源代码与模型，旨在成为后续多模态研究的强基线。论文强调RL对提升感知模式的作用，实验验证了其在视觉推理基础层面的突破性表现。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-03T05:02:53Z
- **目录日期**: 2025-05-03
