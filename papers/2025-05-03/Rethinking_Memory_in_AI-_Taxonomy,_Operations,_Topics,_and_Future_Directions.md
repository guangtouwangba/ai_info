# Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

**URL**: http://arxiv.org/abs/2505.00675v1

## 原始摘要

Memory is a fundamental component of AI systems, underpinning large language
models (LLMs) based agents. While prior surveys have focused on memory
applications with LLMs, they often overlook the atomic operations that underlie
memory dynamics. In this survey, we first categorize memory representations
into parametric, contextual structured, and contextual unstructured and then
introduce six fundamental memory operations: Consolidation, Updating, Indexing,
Forgetting, Retrieval, and Compression. We systematically map these operations
to the most relevant research topics across long-term, long-context, parametric
modification, and multi-source memory. By reframing memory systems through the
lens of atomic operations and representation types, this survey provides a
structured and dynamic perspective on research, benchmark datasets, and tools
related to memory in AI, clarifying the functional interplay in LLMs based
agents while outlining promising directions for future research\footnote{The
paper list, datasets, methods and tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.


## AI 摘要

这篇综述探讨了AI系统中的记忆机制，重点关注大语言模型（LLMs）代理中的记忆动态。作者将记忆表示分为参数化、结构化上下文和非结构化上下文三类，并提出了六种基本记忆操作：巩固、更新、索引、遗忘、检索和压缩。通过从原子操作和表示类型的角度重构记忆系统，该研究为AI记忆相关的研究、基准数据集和工具提供了结构化视角，阐明了LLMs代理中的功能互动，并指出了未来研究方向。相关资源可在GitHub获取。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-03T09:01:16Z
- **目录日期**: 2025-05-03
