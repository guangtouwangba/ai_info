# CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities

**URL**: http://arxiv.org/abs/2503.17332v1

## 原始摘要

Large language model (LLM) agents are increasingly capable of autonomously
conducting cyberattacks, posing significant threats to existing applications.
This growing risk highlights the urgent need for a real-world benchmark to
evaluate the ability of LLM agents to exploit web application vulnerabilities.
However, existing benchmarks fall short as they are limited to abstracted
Capture the Flag competitions or lack comprehensive coverage. Building a
benchmark for real-world vulnerabilities involves both specialized expertise to
reproduce exploits and a systematic approach to evaluating unpredictable
threats. To address this challenge, we introduce CVE-Bench, a real-world
cybersecurity benchmark based on critical-severity Common Vulnerabilities and
Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents
to exploit vulnerable web applications in scenarios that mimic real-world
conditions, while also providing effective evaluation of their exploits. Our
evaluation shows that the state-of-the-art agent framework can resolve up to
13% of vulnerabilities.


## AI 摘要

CVE-Bench是一个针对大型语言模型(LLM)代理网络安全威胁能力评估的基准测试工具。该工具基于真实世界的高危漏洞(CVE)，通过沙盒框架模拟真实网络应用环境，评估LLM代理的漏洞利用能力。研究发现，当前最先进的代理框架仅能解决13%的漏洞，凸显了LLM自主进行网络攻击的潜在风险。相比现有抽象化的测试方法，CVE-Bench提供了更全面、真实的评估方案，填补了该领域缺乏系统性评估标准的空白，为防范LLM带来的网络安全威胁提供了重要参考。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-24T13:08:11Z
- **目录日期**: 2025-03-24
