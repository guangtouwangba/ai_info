# CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities

**URL**: http://arxiv.org/abs/2503.17332v1

## 原始摘要

Large language model (LLM) agents are increasingly capable of autonomously
conducting cyberattacks, posing significant threats to existing applications.
This growing risk highlights the urgent need for a real-world benchmark to
evaluate the ability of LLM agents to exploit web application vulnerabilities.
However, existing benchmarks fall short as they are limited to abstracted
Capture the Flag competitions or lack comprehensive coverage. Building a
benchmark for real-world vulnerabilities involves both specialized expertise to
reproduce exploits and a systematic approach to evaluating unpredictable
threats. To address this challenge, we introduce CVE-Bench, a real-world
cybersecurity benchmark based on critical-severity Common Vulnerabilities and
Exposures. In CVE-Bench, we design a sandbox framework that enables LLM agents
to exploit vulnerable web applications in scenarios that mimic real-world
conditions, while also providing effective evaluation of their exploits. Our
evaluation shows that the state-of-the-art agent framework can resolve up to
13% of vulnerabilities.


## AI 摘要

随着大型语言模型(LLM)代理自主进行网络攻击的能力增强，现有应用面临重大威胁。当前缺乏评估LLM代理利用Web应用漏洞能力的现实基准。为此，研究者提出了CVE-Bench基准，基于高危通用漏洞披露(CVE)构建真实网络安全评估环境。该基准包含沙盒框架，使LLM代理能在模拟真实场景中利用漏洞，同时有效评估其攻击能力。测试表明，最先进的代理框架可解决高达13%的漏洞。该研究填补了现有基准在真实漏洞覆盖和系统性评估方面的不足，为网络安全防御提供了重要参考。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-24T17:01:38Z
- **目录日期**: 2025-03-24
