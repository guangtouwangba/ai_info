# HCAST: Human-Calibrated Autonomy Software Tasks

**URL**: http://arxiv.org/abs/2503.17354v1

## 原始摘要

To understand and predict the societal impacts of highly autonomous AI
systems, we need benchmarks with grounding, i.e., metrics that directly connect
AI performance to real-world effects we care about. We present HCAST
(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning
engineering, cybersecurity, software engineering, and general reasoning tasks.
We collect 563 human baselines (totaling over 1500 hours) from people skilled
in these domains, working under identical conditions as AI agents, which lets
us estimate that HCAST tasks take humans between one minute and 8+ hours.
Measuring the time tasks take for humans provides an intuitive metric for
evaluating AI capabilities, helping answer the question "can an agent be
trusted to complete a task that would take a human X hours?" We evaluate the
success rates of AI agents built on frontier foundation models, and we find
that current agents succeed 70-80% of the time on tasks that take humans less
than one hour, and less than 20% of the time on tasks that take humans more
than 4 hours.


## AI 摘要

为了理解和预测高度自主AI系统的社会影响，需要建立具有实际基础的基准，即直接将AI性能与我们所关心的现实世界效果相连接的指标。本文介绍了HCAST（人类校准的自主软件任务），一个包含189个机器学习工程、网络安全、软件工程和一般推理任务的基准。我们收集了563个来自这些领域熟练人员的人类基线数据（总计超过1500小时），这些人员在相同条件下工作，使我们能够估计HCAST任务对人类来说需要1分钟到8小时以上。通过测量人类完成任务的时间，我们提供了一个直观的指标来评估AI能力，帮助回答“一个代理能否被信任完成一个需要人类X小时的任务？”的问题。我们评估了基于前沿基础模型的AI代理的成功率，发现当前代理在人类需要不到一小时的任务上成功率为70-80%，而在人类需要超过4小时的任务上成功率不到20%。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-24T07:01:21Z
- **目录日期**: 2025-03-24
