# Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii

**URL**: http://arxiv.org/abs/2505.01372v1

## 原始摘要

Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.


## AI 摘要

这篇摘要探讨了机制可解释性(MI)如何通过因果解释理解神经网络，并指出当前缺乏统一的评估标准限制了进展。作者提出基于科学哲学的"解释美德框架"(包含贝叶斯、库恩、多伊奇和法则论四个视角)，系统评估和改进MI解释。研究发现"紧凑证明"能兼顾多种解释优点，是可行方向。框架建议：(1)明确定义解释简洁性，(2)聚焦统一性解释，(3)推导神经网络通用原则。改进MI方法能增强对AI系统的监测、预测和调控能力。该研究为神经网络解释提供了哲学基础和新研究方向。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-05T15:01:24Z
- **目录日期**: 2025-05-05
