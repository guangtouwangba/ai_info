# Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii

**URL**: http://arxiv.org/abs/2505.01372v1

## 原始摘要

Mechanistic Interpretability (MI) aims to understand neural networks through
causal explanations. Though MI has many explanation-generating methods,
progress has been limited by the lack of a universal approach to evaluating
explanations. Here we analyse the fundamental question "What makes a good
explanation?" We introduce a pluralist Explanatory Virtues Framework drawing on
four perspectives from the Philosophy of Science - the Bayesian, Kuhnian,
Deutschian, and Nomological - to systematically evaluate and improve
explanations in MI. We find that Compact Proofs consider many explanatory
virtues and are hence a promising approach. Fruitful research directions
implied by our framework include (1) clearly defining explanatory simplicity,
(2) focusing on unifying explanations and (3) deriving universal principles for
neural networks. Improved MI methods enhance our ability to monitor, predict,
and steer AI systems.


## AI 摘要

本文探讨了机制可解释性(MI)领域如何评估神经网络解释的有效性，提出了基于科学哲学的多元解释美德框架，整合了贝叶斯、库恩、德意志和法则论四种视角。研究发现，紧凑证明(Compact Proofs)能兼顾多种解释美德，是较有前景的方法。该框架指出三个关键研究方向：(1)明确定义解释简洁性；(2)聚焦统一性解释；(3)推导神经网络的普适原理。改进的MI方法将提升我们监控、预测和引导AI系统的能力，为理解神经网络提供更系统的评估标准。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-05T12:01:21Z
- **目录日期**: 2025-05-05
