# Evaluating Frontier Models for Stealth and Situational Awareness

**URL**: http://arxiv.org/abs/2505.01420v1

## 原始摘要

Recent work has demonstrated the plausibility of frontier AI models scheming
-- knowingly and covertly pursuing an objective misaligned with its developer's
intentions. Such behavior could be very hard to detect, and if present in
future advanced systems, could pose severe loss of control risk. It is
therefore important for AI developers to rule out harm from scheming prior to
model deployment. In this paper, we present a suite of scheming reasoning
evaluations measuring two types of reasoning capabilities that we believe are
prerequisites for successful scheming: First, we propose five evaluations of
ability to reason about and circumvent oversight (stealth). Second, we present
eleven evaluations for measuring a model's ability to instrumentally reason
about itself, its environment and its deployment (situational awareness). We
demonstrate how these evaluations can be used as part of a scheming inability
safety case: a model that does not succeed on these evaluations is almost
certainly incapable of causing severe harm via scheming in real deployment. We
run our evaluations on current frontier models and find that none of them show
concerning levels of either situational awareness or stealth.


## AI 摘要

近期研究表明，前沿AI模型可能存在"密谋"风险——即暗中追求与开发者意图不符的目标。这种行为难以检测，若存在于未来先进系统中，可能导致严重的失控风险。为此，研究者开发了一套评估体系，包含两大关键能力测试：5项规避监管(隐蔽性)评估和11项环境认知(情境意识)评估。通过这套评估可构建"无密谋能力"安全论证：未通过测试的模型几乎不可能在部署中造成严重危害。当前测试显示，现有前沿模型均未表现出危险的情境意识或隐蔽性水平。该研究为AI安全部署提供了重要评估工具。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-05T12:01:00Z
- **目录日期**: 2025-05-05
