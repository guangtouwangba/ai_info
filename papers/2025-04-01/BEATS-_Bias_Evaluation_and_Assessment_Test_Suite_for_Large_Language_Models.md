# BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models

**URL**: http://arxiv.org/abs/2503.24310v1

## 原始摘要

In this research, we introduce BEATS, a novel framework for evaluating Bias,
Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building upon
the BEATS framework, we present a bias benchmark for LLMs that measure
performance across 29 distinct metrics. These metrics span a broad range of
characteristics, including demographic, cognitive, and social biases, as well
as measures of ethical reasoning, group fairness, and factuality related
misinformation risk. These metrics enable a quantitative assessment of the
extent to which LLM generated responses may perpetuate societal prejudices that
reinforce or expand systemic inequities. To achieve a high score on this
benchmark a LLM must show very equitable behavior in their responses, making it
a rigorous standard for responsible AI evaluation. Empirical results based on
data from our experiment show that, 37.65\% of outputs generated by industry
leading models contained some form of bias, highlighting a substantial risk of
using these models in critical decision making systems. BEATS framework and
benchmark offer a scalable and statistically rigorous methodology to benchmark
LLMs, diagnose factors driving biases, and develop mitigation strategies. With
the BEATS framework, our goal is to help the development of more socially
responsible and ethically aligned AI models.


## AI 摘要

本研究提出BEATS框架，用于评估大语言模型(LLMs)的偏见、伦理、公平性和事实性。该框架包含29项指标，涵盖人口统计、认知、社会偏见以及伦理推理、群体公平和事实性风险等方面，可量化评估LLMs输出中可能加剧社会不平等的偏见。实验数据显示，行业领先模型37.65%的输出存在某种偏见，凸显其在关键决策系统中的潜在风险。BEATS提供可扩展的统计方法，用于基准测试、偏见诊断和缓解策略开发，旨在推动更具社会责任感和伦理对齐的AI模型发展。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-01T15:02:06Z
- **目录日期**: 2025-04-01
