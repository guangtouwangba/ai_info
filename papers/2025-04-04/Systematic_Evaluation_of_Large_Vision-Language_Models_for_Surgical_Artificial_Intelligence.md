# Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence

**URL**: http://arxiv.org/abs/2504.02799v1

## 原始摘要

Large Vision-Language Models offer a new paradigm for AI-driven image
understanding, enabling models to perform tasks without task-specific training.
This flexibility holds particular promise across medicine, where
expert-annotated data is scarce. Yet, VLMs' practical utility in
intervention-focused domains--especially surgery, where decision-making is
subjective and clinical scenarios are variable--remains uncertain. Here, we
present a comprehensive analysis of 11 state-of-the-art VLMs across 17 key
visual understanding tasks in surgical AI--from anatomy recognition to skill
assessment--using 13 datasets spanning laparoscopic, robotic, and open
procedures. In our experiments, VLMs demonstrate promising generalizability, at
times outperforming supervised models when deployed outside their training
setting. In-context learning, incorporating examples during testing, boosted
performance up to three-fold, suggesting adaptability as a key strength. Still,
tasks requiring spatial or temporal reasoning remained difficult. Beyond
surgery, our findings offer insights into VLMs' potential for tackling complex
and dynamic scenarios in clinical and broader real-world applications.


## AI 摘要

大型视觉语言模型(VLM)为AI驱动的图像理解提供了新范式，无需任务特定训练即可执行任务。在医学领域，尤其是专家标注数据稀缺的手术场景，VLM展现出应用潜力。研究评估了11种先进VLM在17项外科AI任务(从解剖识别到技能评估)的表现，使用13个涵盖不同手术类型的数据集。结果表明，VLM具有良好的泛化能力，有时甚至超越监督模型；上下文学习可将性能提升三倍，突显其适应性强。但涉及时空推理的任务仍具挑战。这些发现不仅适用于手术领域，也为VLM在复杂动态临床和现实场景的应用提供了参考。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-04T09:01:27Z
- **目录日期**: 2025-04-04
