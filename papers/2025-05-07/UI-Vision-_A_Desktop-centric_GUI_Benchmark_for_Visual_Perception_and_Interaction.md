# UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction

**URL**: http://arxiv.org/abs/2503.15661v2

## 原始摘要

Autonomous agents that navigate Graphical User Interfaces (GUIs) to automate
tasks like document editing and file management can greatly enhance computer
workflows. While existing research focuses on online settings, desktop
environments, critical for many professional and everyday tasks, remain
underexplored due to data collection challenges and licensing issues. We
introduce UI-Vision, the first comprehensive, license-permissive benchmark for
offline, fine-grained evaluation of computer use agents in real-world desktop
environments. Unlike online benchmarks, UI-Vision provides: (i) dense,
high-quality annotations of human demonstrations, including bounding boxes, UI
labels, and action trajectories (clicks, drags, and keyboard inputs) across 83
software applications, and (ii) three fine-to-coarse grained tasks-Element
Grounding, Layout Grounding, and Action Prediction-with well-defined metrics to
rigorously evaluate agents' performance in desktop environments. Our evaluation
reveals critical limitations in state-of-the-art models like UI-TARS-72B,
including issues with understanding professional software, spatial reasoning,
and complex actions like drag-and-drop. These findings highlight the challenges
in developing fully autonomous computer use agents. By releasing UI-Vision as
open-source, we aim to advance the development of more capable agents for
real-world desktop tasks.


## AI 摘要

研究人员推出了首个综合性、开源许可的桌面环境智能代理基准测试平台UI-Vision，用于离线评估计算机操作代理在真实桌面环境中的表现。该平台包含83款软件的人类操作标注数据（界面元素框、标签和操作轨迹），并提供元素定位、布局定位和动作预测三个层级的评估任务。测试发现，当前最先进的UI-TARS-72B等模型在专业软件理解、空间推理和拖拽等复杂操作上仍存在明显不足。该开源项目旨在推动开发更强大的桌面任务自动化代理，解决现有研究因数据收集和许可问题对桌面环境关注不足的现状。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-07T20:02:12Z
- **目录日期**: 2025-05-07
