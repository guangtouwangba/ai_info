# Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review

**URL**: http://arxiv.org/abs/2505.02828v1

## 原始摘要

Explainable Artificial Intelligence (XAI) has emerged as a pillar of
Trustworthy AI and aims to bring transparency in complex models that are opaque
by nature. Despite the benefits of incorporating explanations in models, an
urgent need is found in addressing the privacy concerns of providing this
additional information to end users. In this article, we conduct a scoping
review of existing literature to elicit details on the conflict between privacy
and explainability. Using the standard methodology for scoping review, we
extracted 57 articles from 1,943 studies published from January 2019 to
December 2024. The review addresses 3 research questions to present readers
with more understanding of the topic: (1) what are the privacy risks of
releasing explanations in AI systems? (2) what current methods have researchers
employed to achieve privacy preservation in XAI systems? (3) what constitutes a
privacy preserving explanation? Based on the knowledge synthesized from the
selected studies, we categorize the privacy risks and preservation methods in
XAI and propose the characteristics of privacy preserving explanations to aid
researchers and practitioners in understanding the requirements of XAI that is
privacy compliant. Lastly, we identify the challenges in balancing privacy with
other system desiderata and provide recommendations for achieving privacy
preserving XAI. We expect that this review will shed light on the complex
relationship of privacy and explainability, both being the fundamental
principles of Trustworthy AI.


## AI 摘要

可解释人工智能（XAI）作为可信AI的核心，旨在提升复杂模型透明度，但解释信息可能引发隐私风险。本文通过文献综述（2019-2024年57篇研究）探讨三大问题：(1)XAI解释可能泄露训练数据特征、模型参数等隐私；(2)现有保护方法包括差分隐私、联邦学习等；(3)隐私保护型解释需满足匿名化、最小披露等特性。研究归纳了XAI中的隐私风险类型和保护技术，提出隐私合规解释的特征框架，并指出平衡隐私与其他系统需求（如准确性）的挑战。该综述揭示了可信AI两大原则——隐私性与可解释性间的复杂关系，为隐私保护型XAI发展提供方向。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-07T02:29:53Z
- **目录日期**: 2025-05-07
