# #华为昇腾2秒吃透一道高数大题##华为揭秘准万亿MoE训练系统#现在，请大家一起数一下“1”、“2”。OK，短短2秒钟时间，一个准万亿MoE大模型就已经吃透如何解一道...

**URL**: https://weibo.com/6105753431/Pudl0BUJA

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE2%E7%A7%92%E5%90%83%E9%80%8F%E4%B8%80%E9%81%93%E9%AB%98%E6%95%B0%E5%A4%A7%E9%A2%98%23&amp;extparam=%23%E5%8D%8E%E4%B8%BA%E6%98%87%E8%85%BE2%E7%A7%92%E5%90%83%E9%80%8F%E4%B8%80%E9%81%93%E9%AB%98%E6%95%B0%E5%A4%A7%E9%A2%98%23" data-hide=""><span class="surl-text">#华为昇腾2秒吃透一道高数大题#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%8D%8E%E4%B8%BA%E6%8F%AD%E7%A7%98%E5%87%86%E4%B8%87%E4%BA%BFMoE%E8%AE%AD%E7%BB%83%E7%B3%BB%E7%BB%9F%23&amp;extparam=%23%E5%8D%8E%E4%B8%BA%E6%8F%AD%E7%A7%98%E5%87%86%E4%B8%87%E4%BA%BFMoE%E8%AE%AD%E7%BB%83%E7%B3%BB%E7%BB%9F%23" data-hide=""><span class="surl-text">#华为揭秘准万亿MoE训练系统#</span></a><br><br>现在，请大家一起数一下“1”、“2”。<br><br>OK，短短2秒钟时间，一个准万亿MoE大模型就已经吃透如何解一道高等数学大题了！<br><br>而且啊，这个大模型还是不用GPU来训练，全流程都是大写的“国产”的那种。<br><br>这，就是华为通过“昇腾+Pangu Ultra MoE”这套组合拳解锁的效果——<br><br>不仅实现了国产算力与国产模型全流程自主可控的训练闭环，更是在集群训练系统性能方面达到行业领先水平。<br><br>有多领先？来看一组数据：<br><br>- 预训练阶段：昇腾Atlas 800T A2万卡集群MFU提升至41%<br>- 后训练阶段：单CloudMatrix 384超节点吞吐达35K Tokens/s<br><br>值得一提的是，华为还首次把背后的一大秘籍给亮了出来。<br><br>具体来说，华为在这次发布的技术报告中，披露了在昇腾CloudMatrix 384超节点上，高效打通大稀疏比MoE强化学习后训练框架的关键技术。<br><br>此举可以说是让以强化学习（RL）为核心机制的后训练，进入到了超节点集群时代。<br><br>在深入华为Pangu Ultra MoE训练系统全流程之前，老规矩，我们还是先来了解一下此前的技术痛点：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FEnKbKpq8YAcqGnYJ7K9Bdg" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1xicdzbn7j30u00glgvv.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

华为发布基于昇腾AI和Pangu Ultra MoE的国产大模型训练系统，实现全流程自主可控。该系统在2秒内即可解析高等数学大题，采用准万亿参数MoE架构，完全基于国产算力（非GPU）。关键技术突破包括：预训练阶段万卡集群效率达41% MFU，后训练阶段单节点吞吐35K tokens/秒，并首次公开了大稀疏比MoE强化学习训练框架。这标志着国产AI在集群训练性能上达到行业领先水平，推动强化学习进入超节点集群时代。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-30T07:03:04Z
- **目录日期**: 2025-05-30
