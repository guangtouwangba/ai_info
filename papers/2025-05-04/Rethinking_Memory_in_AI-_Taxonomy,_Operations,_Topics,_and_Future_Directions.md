# Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

**URL**: http://arxiv.org/abs/2505.00675v1

## 原始摘要

Memory is a fundamental component of AI systems, underpinning large language
models (LLMs) based agents. While prior surveys have focused on memory
applications with LLMs, they often overlook the atomic operations that underlie
memory dynamics. In this survey, we first categorize memory representations
into parametric, contextual structured, and contextual unstructured and then
introduce six fundamental memory operations: Consolidation, Updating, Indexing,
Forgetting, Retrieval, and Compression. We systematically map these operations
to the most relevant research topics across long-term, long-context, parametric
modification, and multi-source memory. By reframing memory systems through the
lens of atomic operations and representation types, this survey provides a
structured and dynamic perspective on research, benchmark datasets, and tools
related to memory in AI, clarifying the functional interplay in LLMs based
agents while outlining promising directions for future research\footnote{The
paper list, datasets, methods and tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.


## AI 摘要

这篇综述系统性地探讨了AI系统中的记忆机制，重点关注基于大语言模型（LLMs）的智能体。作者将记忆表征分为参数化、结构化上下文和非结构化上下文三类，并提出了六种基本记忆操作：巩固、更新、索引、遗忘、检索和压缩。通过将这些操作映射到长期记忆、长上下文、参数修改和多源记忆等研究领域，该研究为AI记忆系统提供了结构化视角，阐明了LLMs中记忆功能的相互作用，同时指出了未来研究方向。相关资源（论文列表、数据集和方法工具）已在GitHub开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-04T10:01:23Z
- **目录日期**: 2025-05-04
