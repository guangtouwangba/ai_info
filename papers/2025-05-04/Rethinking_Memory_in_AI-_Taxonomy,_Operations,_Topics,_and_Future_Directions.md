# Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

**URL**: http://arxiv.org/abs/2505.00675v1

## 原始摘要

Memory is a fundamental component of AI systems, underpinning large language
models (LLMs) based agents. While prior surveys have focused on memory
applications with LLMs, they often overlook the atomic operations that underlie
memory dynamics. In this survey, we first categorize memory representations
into parametric, contextual structured, and contextual unstructured and then
introduce six fundamental memory operations: Consolidation, Updating, Indexing,
Forgetting, Retrieval, and Compression. We systematically map these operations
to the most relevant research topics across long-term, long-context, parametric
modification, and multi-source memory. By reframing memory systems through the
lens of atomic operations and representation types, this survey provides a
structured and dynamic perspective on research, benchmark datasets, and tools
related to memory in AI, clarifying the functional interplay in LLMs based
agents while outlining promising directions for future research\footnote{The
paper list, datasets, methods and tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.


## AI 摘要

这篇综述系统地探讨了AI系统中的记忆机制，重点关注基于大语言模型（LLMs）的智能体。作者将记忆表征分为参数化、上下文结构化和非结构化三类，并提出了六种基本记忆操作：巩固、更新、索引、遗忘、检索和压缩。通过从原子操作和表征类型的角度重构记忆系统，该研究为AI记忆相关的研究、基准数据集和工具提供了结构化视角，阐明了LLMs智能体中的功能互动，同时指出了未来研究方向。相关资源可在GitHub仓库获取。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-04T14:01:21Z
- **目录日期**: 2025-05-04
