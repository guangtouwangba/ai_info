# #68页论文爆锤大模型竞技场##曝Llama4曾刷大模型竞技场27次#大模型竞技场的可信度，再次被锤。最近一篇名为《排行榜幻觉》（The Leaderboard Illusion）的论文在...

**URL**: https://weibo.com/6105753431/PpYl4prJg

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%2368%E9%A1%B5%E8%AE%BA%E6%96%87%E7%88%86%E9%94%A4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AB%9E%E6%8A%80%E5%9C%BA%23&amp;extparam=%2368%E9%A1%B5%E8%AE%BA%E6%96%87%E7%88%86%E9%94%A4%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AB%9E%E6%8A%80%E5%9C%BA%23" data-hide=""><span class="surl-text">#68页论文爆锤大模型竞技场#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E6%9B%9DLlama4%E6%9B%BE%E5%88%B7%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AB%9E%E6%8A%80%E5%9C%BA27%E6%AC%A1%23&amp;extparam=%23%E6%9B%9DLlama4%E6%9B%BE%E5%88%B7%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AB%9E%E6%8A%80%E5%9C%BA27%E6%AC%A1%23" data-hide=""><span class="surl-text">#曝Llama4曾刷大模型竞技场27次#</span></a><br><br>大模型竞技场的可信度，再次被锤。<br><br>最近一篇名为《排行榜幻觉》（The Leaderboard Illusion）的论文在学术圈引发关注。【图1】  <br>它指出，如今被视为LLM领域首选排行榜的Chatbot Arena，存在诸多系统问题。比如：<br><br>少数大厂可以私下测试多个模型版本，Llama4在发布前甚至测了27个版本，然后只公开最佳表现。  <br>数据访问不平等，专有模型获得的用户反馈数据显著多于开源模型。  <br>试用Arena数据训练，可提升模型性能高达112%。  <br>205个模型被悄悄静默弃用，远超过官方列出的47个。  <br>大神卡帕西也站出来表示，他个人也察觉出了一些异样。【图2】<br><br>有一段时间，Claude-3.5是我觉得最好用的模型，但是在竞技场中排名很低。当时我在网上也看到了类似的反馈。<br><br>对于最新质疑，大模型竞技场官方Lmrena.ai已经给出回应：【图3】  <br>确实帮助厂商进行测试，最后发布最受欢迎的版本；<br><br>但这不代表竞技场有偏见，排行榜反映数百万人类的个人真实偏好。<br><br>快速刷榜不符合模型进步实际情况【图4】  <br>具体来看这项研究，它收集了243个模型的200+万场竞技场battle，并结合私人真实测试，通过模拟实验确定了不同情况下对模型排名的影响。<br><br>主要挖掘出了4方面问题。【图5】<br><br>第一，私人测试和有选择性的结果报告。【图6】<br><br>少数大模型厂商（如Meta、Google、Amazon）被允许私下测试多个模型变体，并只公开最佳表现的版本。<br><br>比如，Meta在Llama 4发布前曾私下测试27个变体，加上多模态、代码等榜单，Meta可能一共测试过43个变体。【图7】<br><br>这种“最佳N选1”策略导致排名膨胀。<br><br>例如，当测试5个变体时，期望分数增加了约20分；当测试20个变体时，增加了约40分；当测试50个变体时，增加了约50分。【图8】<br><br>研究团队认为，当多个大模型厂商采用这种策略时，他们实际上是在相互竞争各自变体分布的最大值，而非真实的模型能力。<br><br>我们观察到，像Google、OpenAI和xAI在短时间内轮番霸榜，表明他们都在采用类似的策略。<br><br>例如，2024年11月期间，Google的Gemini (Exp 1114)、OpenAI的ChatGPT-4o (20241120)和Google的Gemini (Exp 1121)在一周内先后占据榜首。类似地，2025年3月4日，OpenAI的GPT-4.5和xAI的Grok-3同一天争夺榜首位置。<br><br>这种排行榜的快速变化不太可能反映真实的技术进步，因为开发和完善一个全新的基础模型通常需要数月时间。<br><br>相反，这很可能是多个大模型厂商同时使用“最佳N选1”策略的结果，每个提供商都试图优化自己变体池中的最大值。<br><br>此外，团队还发现大模型厂商可以撤回表现不好的模型。【图9】<br><br>第二，数据访问不平等。专有模型获得的用户反馈数据显著多于开源模型。【图10】<br><br>Google和OpenAI分别获得了约19.2%和20.4%的所有测试数据，而全部83个开放权重模型仅获得约29.7%的数据。【图11】<br><br>第三，大模型厂商使用竞技场数据进行训练，排名可以显著提升。<br><br>我们观察到，将竞技场训练数据比例从0%增加到70%，在ArenaHard上的胜率从23.5%提高到了49.9%，实现了一倍多的增长。<br><br>这还是一个保守估计，因为部分提供商拥有数据访问优势。<br><br>第四，研究发现，许多模型被”静默弃用”（减少采样率至接近0%）。<br><br>在243个公开模型中，有205个被静默弃用，远超过官方列出的47个。这种做法特别影响开源和开放权重模型，会导致排名不可靠。【图12】<br><br>在提出问题后，研究团队还给出了5点改进建议：<br><br>禁止提交后撤回分数  <br>限制每个提供商的非正式模型数量  <br>公平应用模型弃用政策，所有模型一视同仁  <br>实施公平采样方法  <br>提高模型弃用透明度，即时通知被淘汰模型  <br>这项研究由Cohere团队、普林斯顿大学、斯坦福大学等机构研究人员共同提出。<br><br>其中Cohere也是一家大模型厂商，由Transformer作者Aidan Gomez等人创办，推出了Command R+系列模型。<br><br>“竞技场不应该是唯一基准参考”  <br>大模型竞技场诞生2年来，因为机制的特殊性，其参考价值越来越高，大厂发模型也必来这里打榜，甚至是将未发布模型提前在此预热造势。<br><br>它最大的优势在于基于人类偏好评估，用户可以在同一平台上同时运行多个聊天机器人模型，如GPT-4、ChatGPT-3.5等，并针对相同的问题或任务进行比较分析，可以更直观感受不同模型的差异。<br><br>最近一段时间，由于Llama4刷榜风波，给竞技场的可信度也造成了一定影响。【图13】<br><br>对于这篇质疑论文，官方现在已做出回应。反驳了一些问题：<br><br>LMArena模拟的缺陷：图7/8中的模拟存在问题。这就像说：NBA球员的平均三分命中率是35%。斯蒂芬·库里拥有NBA球员最高的三分命中率42%。这不公平，因为他来自NBA球员的分布，而所有球员都有相同的潜在平均水平。<br><br>数据不实：文章中的许多数据并不反映现实：请参阅几天前发布的博客了解来自不同提供商测试模型数量的实际统计数据。例如，开放模型占比为40%，而非文章声称的8.8%！<br><br>112%性能提升的误导性说法：这一说法基于LLM评判基准而非竞技场中的实际人类评估。<br><br>政策并非“不透明”：我们设计并公开分享了政策，且这一政策已存在一年多。<br><br>模型提供商并非只选择“最佳分数披露”：任何列在公共排行榜上的模型都必须是向所有人开放且有长期支持计划的生产模型。我们会继续使用新数据对模型进行至少一个月的测试。这些要点一直在我们的政策中明确说明。<br><br>展示非公开发布模型的分数毫无意义：对于通过API或开放权重不公开可用的预发布模型显示分数没有意义，因为社区无法使用这些模型或自行测试。这会违反我们一年多前就制定的政策。我们制定该政策正是为了明确这一规则：如果模型在排行榜上，它应该可供使用。<br><br>模型移除不平等或不透明的说法不实：排行榜旨在反映社区兴趣，对最佳AI模型进行排名。我们也会淘汰不再向公众开放的模型，这些标准在我们与社区进行私人测试的整个期间都已在政策中公开说明。<br><br>至于情况到底如何，可能还要等子弹飞一会儿。<br><br>不过这倒是也给AI社区提了个醒，或许不能只参考一个榜单了。<br><br>卡帕西就给出了一个备选项：OpenRouter。<br><br>OpenRouter可以提供一个统一API接口来访问使用不同模型，而且更加关注实际使用案例。<br><br>尽管在多样性和使用量上还不够优秀，但我认为它有很大潜力。<br><br>参考链接：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fabs%2F2504.20879" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1198knodmj30tm0k07fg.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1198kmryxj30zk0e078y.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1198m2ubej30vu0k0aji.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1198lvednj30zk0gvgs7.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1198lnm8uj30zk0gqq7j.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1198m05vjj30zk0hkwjw.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1198m9nizj30zk0hvtbv.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1198mfdeij30zk0hsae5.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1198m935gj30wk0k0wid.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1198m89myj30zk0hz0wv.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1198mlkauj30zk0jydoy.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1198mcx7nj30zk0iw0za.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

《排行榜幻觉》论文指出Chatbot Arena存在四大问题：1）大厂可私下测试多个模型版本（如Llama4测27版仅公开最佳），导致排名虚高；2）专有模型获20%用户数据，开源模型仅29.7%；3）使用竞技场数据训练可使模型性能提升112%；4）205个模型被静默弃用，远超官方公布的47个。研究团队建议限制非正式模型数量、提高透明度等。竞技场官方回应称排行榜反映真实用户偏好，否认数据不公指控。该争议引发对单一评估体系可靠性质疑，建议参考OpenRouter等替代方案。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-04T12:04:00Z
- **目录日期**: 2025-05-04
