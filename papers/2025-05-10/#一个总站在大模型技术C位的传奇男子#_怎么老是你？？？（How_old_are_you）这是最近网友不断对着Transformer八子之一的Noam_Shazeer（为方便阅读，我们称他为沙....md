# #一个总站在大模型技术C位的传奇男子# 怎么老是你？？？（How old are you）这是最近网友不断对着Transformer八子之一的Noam Shazeer（为方便阅读，我们称他为沙...

**URL**: https://weibo.com/6105753431/Pr9rYz7yR

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E4%B8%80%E4%B8%AA%E6%80%BB%E7%AB%99%E5%9C%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AFC%E4%BD%8D%E7%9A%84%E4%BC%A0%E5%A5%87%E7%94%B7%E5%AD%90%23&amp;extparam=%23%E4%B8%80%E4%B8%AA%E6%80%BB%E7%AB%99%E5%9C%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AFC%E4%BD%8D%E7%9A%84%E4%BC%A0%E5%A5%87%E7%94%B7%E5%AD%90%23" data-hide=""><span class="surl-text">#一个总站在大模型技术C位的传奇男子#</span></a> <br>怎么老是你？？？（How old are you）<br><br>这是最近网友不断对着Transformer八子之一的Noam Shazeer（为方便阅读，我们称他为沙哥）发出的灵魂疑问。<br><br>尤其是最近Meta FAIR研究员朱泽园分享了他们《Physics of Language Models》项目的系列新进展后，有网友发现，其中提到的3-token因果卷积相关内容，沙哥等又早在三年前就有相关研究。<br><br>是的，“又”。<br><br>因为你只要梳理一遍他的工作履历，就不难发现，AI界大大小小的突破背后，总是能发现他的名字。<br><br>“不是搞个人崇拜，但为什么总是Noam Shazeer？”<br><br>朱泽园也自己也站出来表示，沙哥成果超前：<br><br>我也觉得Shazeer可能是个时间旅行者。<br><br>我原本不相信他们的gated MLP（在写第3.3部分的时候，因为门控多层感知机让训练不稳定），但现在我信服了（在添加了Canon层之后，我们在第4.1部分对比了多层感知机和门控多层感知机）。<br><br>正式认识一下，沙哥是谁？<br><br>他是Transformer八位作者中被公认是“贡献最大”的那位，也是半路跑去创业Character.AI，又被谷歌“买回来”那位。<br><br>他并非OpenAI的明星科学家，也不似DeepMind创始人般频繁曝光，但若细察当今LLM的核心技术，其奠基性贡献隐然贯穿始终。<br><br>从引用量超17万次的《Attention is all you need》，到将MoE引入LLM的谷歌早期研究，再到Adafactor算法、多查询注意力、用于Transformer的门控线性层（GLU）……<br><br>有人感慨，其实我们现在就是生活在“Noam Shazeer时代”。<br><br>因为如今主流模型架构的演变，就是在其奠定的基础上持续推进。<br><br>所以，他都做了什么？<br><a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUSqd0EDbJtbU87hV7jdV5Q" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">一个「always」站在大模型技术C位的传奇男子</span></a><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3ly1i1a82ia0idj30u00ye4ku.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i1a82nrqb4j30u00mnam7.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i1a82rfab5j30u00h2goy.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

Noam Shazeer（昵称"沙哥"）是Transformer八位核心作者中公认贡献最大的一位，其研究成果贯穿当前大模型技术发展的关键节点。作为《Attention Is All You Need》的核心作者（论文引用超17万次），他先后主导了MoE架构引入LLM、Adafactor优化算法、多查询注意力机制、门控线性层（GLU）等突破性工作。近期Meta研究员发现其3-token因果卷积研究竟比行业早三年，引发"沙哥可能是时间旅行者"的调侃。这位低调的AI先驱虽未频繁曝光，但现代大模型架构多建立在其奠定的技术基础之上，堪称"站在大模型技术C位的传奇男子"。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-10T14:03:36Z
- **目录日期**: 2025-05-10
