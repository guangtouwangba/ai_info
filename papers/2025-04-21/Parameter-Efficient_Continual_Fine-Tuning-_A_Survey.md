# Parameter-Efficient Continual Fine-Tuning: A Survey

**URL**: http://arxiv.org/abs/2504.13822v1

## 原始摘要

The emergence of large pre-trained networks has revolutionized the AI field,
unlocking new possibilities and achieving unprecedented performance. However,
these models inherit a fundamental limitation from traditional Machine Learning
approaches: their strong dependence on the \textit{i.i.d.} assumption hinders
their adaptability to dynamic learning scenarios. We believe the next
breakthrough in AI lies in enabling efficient adaptation to evolving
environments -- such as the real world -- where new data and tasks arrive
sequentially. This challenge defines the field of Continual Learning (CL), a
Machine Learning paradigm focused on developing lifelong learning neural
models. One alternative to efficiently adapt these large-scale models is known
Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of
adapting the model to a particular data or scenario by performing small and
efficient modifications, achieving similar performance to full fine-tuning.
However, these techniques still lack the ability to adjust the model to
multiple tasks continually, as they suffer from the issue of Catastrophic
Forgetting. In this survey, we first provide an overview of CL algorithms and
PEFT methods before reviewing the state-of-the-art on Parameter-Efficient
Continual Fine-Tuning (PECFT). We examine various approaches, discuss
evaluation metrics, and explore potential future research directions. Our goal
is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning,
guide researchers in this field, and pave the way for novel future research
directions.


## AI 摘要

这篇摘要探讨了大型预训练模型在动态学习环境中的适应性问题。尽管这些模型取得了突破性进展，但仍受限于传统机器学习对独立同分布(i.i.d.)假设的依赖，难以适应持续变化的任务和数据。作者指出持续学习(CL)和参数高效微调(PEFT)是解决这一挑战的关键方向。PEFT方法虽能高效适应特定任务，但仍面临灾难性遗忘问题。文章综述了CL算法、PEFT方法及参数高效持续微调(PECFT)的最新进展，分析了各种方法、评估指标和未来研究方向，旨在促进CL与PEFT的协同发展，为终身学习模型开辟新路径。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-21T21:01:24Z
- **目录日期**: 2025-04-21
