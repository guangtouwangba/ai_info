# A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values

**URL**: http://arxiv.org/abs/2505.07797v1

## 原始摘要

Reinforcement learning agents can achieve superhuman performance, but their
decisions are often difficult to interpret. This lack of transparency limits
deployment, especially in safety-critical settings where human trust and
accountability are essential. In this work, we develop a theoretical framework
for explaining reinforcement learning through the influence of state features,
which represent what the agent observes in its environment. We identify three
core elements of the agent-environment interaction that benefit from
explanation: behaviour (what the agent does), performance (what the agent
achieves), and value estimation (what the agent expects to achieve). We treat
state features as players cooperating to produce each element and apply Shapley
values, a principled method from cooperative game theory, to identify the
influence of each feature. This approach yields a family of mathematically
grounded explanations with clear semantics and theoretical guarantees. We use
illustrative examples to show how these explanations align with human intuition
and reveal novel insights. Our framework unifies and extends prior work, making
explicit the assumptions behind existing approaches, and offers a principled
foundation for more interpretable and trustworthy reinforcement learning.


## AI 摘要

该研究提出了一个理论框架，用于通过状态特征的影响来解释强化学习（RL）的决策过程。框架聚焦三个核心交互元素：行为（agent的行动）、性能（agent的成果）和估值（agent的预期）。通过将状态特征视为合作产生结果的"玩家"，并应用博弈论中的Shapley值量化各特征贡献，该方法提供了数学严谨、语义清晰的解释族。研究展示了该框架如何与人类直觉一致并揭示新见解，统一了现有方法，明确了其隐含假设，为构建更可解释、可信赖的RL系统奠定了理论基础，尤其适用于安全关键领域。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-13T16:01:20Z
- **目录日期**: 2025-05-13
