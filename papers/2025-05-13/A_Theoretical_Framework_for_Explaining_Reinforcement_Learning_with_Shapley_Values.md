# A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values

**URL**: http://arxiv.org/abs/2505.07797v1

## 原始摘要

Reinforcement learning agents can achieve superhuman performance, but their
decisions are often difficult to interpret. This lack of transparency limits
deployment, especially in safety-critical settings where human trust and
accountability are essential. In this work, we develop a theoretical framework
for explaining reinforcement learning through the influence of state features,
which represent what the agent observes in its environment. We identify three
core elements of the agent-environment interaction that benefit from
explanation: behaviour (what the agent does), performance (what the agent
achieves), and value estimation (what the agent expects to achieve). We treat
state features as players cooperating to produce each element and apply Shapley
values, a principled method from cooperative game theory, to identify the
influence of each feature. This approach yields a family of mathematically
grounded explanations with clear semantics and theoretical guarantees. We use
illustrative examples to show how these explanations align with human intuition
and reveal novel insights. Our framework unifies and extends prior work, making
explicit the assumptions behind existing approaches, and offers a principled
foundation for more interpretable and trustworthy reinforcement learning.


## AI 摘要

这篇论文提出了一个基于状态特征影响力的理论框架，用于解释强化学习模型的决策过程。针对智能体与环境交互中的三个核心要素（行为、性能和价值估计），作者将状态特征视为合作产生这些要素的参与者，并运用合作博弈论中的Shapley值方法来量化每个特征的影响力。该方法提供了一系列具有明确语义和理论保证的数学解释，能产生符合人类直觉的新见解。该框架统一并扩展了现有工作，明确了各种方法的潜在假设，为开发更可解释、更可信的强化学习系统奠定了理论基础。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-13T14:01:03Z
- **目录日期**: 2025-05-13
