# #Transformer作者提出新框架##Transformer作者新方法更接近人类思考#Transformer作者Llion Jones创办的Sakana AI公司，提出了新框架CTM，主打让AI像人类大脑一样...

**URL**: https://weibo.com/6105753431/PrD3Ts1kx

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23Transformer%E4%BD%9C%E8%80%85%E6%8F%90%E5%87%BA%E6%96%B0%E6%A1%86%E6%9E%B6%23&amp;extparam=%23Transformer%E4%BD%9C%E8%80%85%E6%8F%90%E5%87%BA%E6%96%B0%E6%A1%86%E6%9E%B6%23" data-hide=""><span class="surl-text">#Transformer作者提出新框架#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23Transformer%E4%BD%9C%E8%80%85%E6%96%B0%E6%96%B9%E6%B3%95%E6%9B%B4%E6%8E%A5%E8%BF%91%E4%BA%BA%E7%B1%BB%E6%80%9D%E8%80%83%23&amp;extparam=%23Transformer%E4%BD%9C%E8%80%85%E6%96%B0%E6%96%B9%E6%B3%95%E6%9B%B4%E6%8E%A5%E8%BF%91%E4%BA%BA%E7%B1%BB%E6%80%9D%E8%80%83%23" data-hide=""><span class="surl-text">#Transformer作者新方法更接近人类思考#</span></a><br><br>Transformer作者Llion Jones创办的Sakana AI公司，提出了新框架CTM，主打让AI像人类大脑一样“思考”。<br><br>CTM受两个大脑机制启发：<br><br>1. 神经元级时序处理（Neuron-level Temporal Processing）：这是种“带记忆的神经元”，它让整个模型具备了复杂的时间动态，类似生物神经元中的突触可塑性和时间依赖激活。<br><br>在普通神经网络中，每个神经元只计算当前输入，激活后就完事。但CTM里，每个神经元都有自己独立的小模型（MLP），可以记住并处理一段时间内的输入历史。<br><br>2. 神经同步（Neural Synchronization）：相当于“AI的脑电波”，直接用作模型的中间表示，用来观察数据、进行推理和给出输出。<br><br>有了它，CTM不只关注每个神经元的输出值，而是看它们在时间上的协同关系，也就是哪些神经元一起活跃、一起沉默。<br><br>这两个机制共同支撑起了一个新的维度：内部时间（Internal Ticks），也可以理解为模型“内部思考的步数”。<br><br>不像传统模型那样一步步处理外部输入，CTM每一步都在进行一次全模型内部的同步演化，也就是说，即使输入的是张静态图片，CTM也能在“脑中反复思考”几十次，再决定输出什么。<br><br>从实验结果看，这种架构不仅有趣，还确实有效：<br><br>- 在ImageNet图像分类任务上，CTM展现出更可解释的注意力模式，预测过程像“在环顾四周思考”，并能动态决定“思考多少步”再输出；<br><br>- 在“2D迷宫求解”任务中，CTM可以不依赖位置编码，而是边走边“构图”，成功走出更大更复杂的迷宫，这说明它真的在进行某种内部空间规划；【视频2】<br><br>- 在序列推理（比如Parity任务） 上，CTM展示出明确的“规划式注意力”，有些模型会先扫一遍全局再统一输出，有些则会逐步计算，策略高度可解释；<br><br>- 在”问答版MNIST任务“中，CTM靠神经元的同步模式，能记住很久以前的输入，而不是传统的短期激活记忆；<br><br>- 在强化学习（如MiniGrid） 场景下，CTM也表现出可持续使用历史激活参与决策的能力，表明它在动态环境中，依旧能“连贯思考”。<br><br>综上，CTM的设计让“思考”成为了AI的内建结构，它不是被迫一次输出，而是可以在内部自己选择“想几步”。<br><br>这背后的原理也很好理解：时间本身，就是智能的一部分。<br><br>人脑处理信息，不是快就是好，而是通过在时间中展开推理、整合、调整。<br><br>而CTM正是引入这一“时间维度”来激活更复杂的智能行为。<br><br>目前，CTM并非以性能为主打，它的重点在于探索一种更接近人脑的计算方式。<br><br>Sakana也明确表示，他们关注的是模型行为是否合理、有解释性、有泛化潜力，而不是打破某个排行榜。<br><br>未来是否能靠这类模型让AI变得更像人，让我们拭目以待。<br><br>感兴趣的小伙伴可以点击：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fpub.sakana.ai%2Fctm%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>论文：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fabs%2F2505.05522" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>源码：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fgithub.com%2FSakanaAI%2Fcontinuous-thought-machines%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1dupgnbq0j30zk0gx4ea.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i1duspjwwlj31rq0u0juj.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i1dusguw4mj30u00u03yi.jpg" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax4.sinaimg.cn/orj480/006Fd7o3ly1i1duspotw5j31rq0u0juj.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/zvZcex1ilx08od8RCxZK01041200ypzg0E010.mp4?label=mp4_720p&amp;template=1528x720.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1747130560&amp;ssig=vey2%2BcK5AY&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/PiSpdJjqlx08od8RnRyw01041200i9mE0E010.mp4?label=mp4_hd&amp;template=1016x480.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1747130560&amp;ssig=RKSctPfgkZ&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/yJChxSISlx08od8Rqu9a01041200bby80E010.mp4?label=mp4_ld&amp;template=764x360.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1747130560&amp;ssig=Rp1evR3owM&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5165849292767293" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

Transformer作者Llion Jones创立的Sakana AI公司提出新框架CTM，模拟人类大脑思考机制。CTM包含两大核心：1）带记忆的神经元，通过独立MLP处理时间序列输入；2）神经同步机制，观察神经元协同活动模式。该框架引入"内部时间"维度，允许模型进行多步内部思考后再输出。实验显示，CTM在图像分类、迷宫求解等任务中展现出更接近人类的可解释性行为，如动态规划注意力、空间构图能力等。该研究重点在于探索类脑计算方式，而非追求性能突破，为AI的类人推理能力提供了新思路。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-13T09:03:34Z
- **目录日期**: 2025-05-13
