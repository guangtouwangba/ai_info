# UniSync: A Unified Framework for Audio-Visual Synchronization

**URL**: http://arxiv.org/abs/2503.16357v1

## 原始摘要

Precise audio-visual synchronization in speech videos is crucial for content
quality and viewer comprehension. Existing methods have made significant
strides in addressing this challenge through rule-based approaches and
end-to-end learning techniques. However, these methods often rely on limited
audio-visual representations and suboptimal learning strategies, potentially
constraining their effectiveness in more complex scenarios. To address these
limitations, we present UniSync, a novel approach for evaluating audio-visual
synchronization using embedding similarities. UniSync offers broad
compatibility with various audio representations (e.g., Mel spectrograms,
HuBERT) and visual representations (e.g., RGB images, face parsing maps, facial
landmarks, 3DMM), effectively handling their significant dimensional
differences. We enhance the contrastive learning framework with a margin-based
loss component and cross-speaker unsynchronized pairs, improving discriminative
capabilities. UniSync outperforms existing methods on standard datasets and
demonstrates versatility across diverse audio-visual representations. Its
integration into talking face generation frameworks enhances synchronization
quality in both natural and AI-generated content.


## AI 摘要

UniSync是一种新颖的音频-视频同步评估方法，通过嵌入相似性来解决现有方法在复杂场景中的局限性。它兼容多种音频（如梅尔频谱、HuBERT）和视觉（如RGB图像、面部解析图、面部标志、3DMM）表示，有效处理其维度差异。通过引入基于边界的损失组件和跨说话者非同步对，增强了对比学习框架的区分能力。UniSync在标准数据集上优于现有方法，并在多种音频-视觉表示中表现出广泛的适用性。其集成到说话人脸生成框架中，提高了自然和AI生成内容的同步质量。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-21T21:02:18Z
- **目录日期**: 2025-03-21
