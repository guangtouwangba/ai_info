# Survey on Evaluation of LLM-based Agents

**URL**: http://arxiv.org/abs/2503.16416v1

## 原始摘要

The emergence of LLM-based agents represents a paradigm shift in AI, enabling
autonomous systems to plan, reason, use tools, and maintain memory while
interacting with dynamic environments. This paper provides the first
comprehensive survey of evaluation methodologies for these increasingly capable
agents. We systematically analyze evaluation benchmarks and frameworks across
four critical dimensions: (1) fundamental agent capabilities, including
planning, tool use, self-reflection, and memory; (2) application-specific
benchmarks for web, software engineering, scientific, and conversational
agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating
agents. Our analysis reveals emerging trends, including a shift toward more
realistic, challenging evaluations with continuously updated benchmarks. We
also identify critical gaps that future research must address-particularly in
assessing cost-efficiency, safety, and robustness, and in developing
fine-grained, and scalable evaluation methods. This survey maps the rapidly
evolving landscape of agent evaluation, reveals the emerging trends in the
field, identifies current limitations, and proposes directions for future
research.


## AI 摘要

本文首次全面综述了基于大型语言模型（LLM）的智能体评估方法，分析了四个关键维度的评估基准和框架：基础能力（如规划、工具使用、自我反思和记忆）、应用特定基准（如网络、软件工程、科学和对话智能体）、通用智能体基准以及评估框架。研究发现，评估趋势正转向更现实、更具挑战性的持续更新基准。同时，指出了未来研究需解决的关键问题，如成本效率、安全性和鲁棒性评估，以及开发细粒度、可扩展的评估方法。本文揭示了智能体评估领域的快速演变趋势和当前局限，并提出了未来研究方向。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-21T19:01:23Z
- **目录日期**: 2025-03-21
