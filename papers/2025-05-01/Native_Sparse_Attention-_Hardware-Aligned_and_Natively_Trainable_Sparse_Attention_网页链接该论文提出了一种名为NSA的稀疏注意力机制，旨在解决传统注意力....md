# Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention 网页链接该论文提出了一种名为NSA的稀疏注意力机制，旨在解决传统注意力...

**URL**: https://weibo.com/1870858943/PjqOTBJ4E

## 原始摘要

Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention <a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fwww.aminer.cn%2Fpub%2F67b3fbf2ae8580e7ff49bfcc%2Fnative-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>该论文提出了一种名为NSA的稀疏注意力机制，旨在解决传统注意力机制在处理长文本上下文时的高计算成本问题。NSA通过结合算法创新与硬件优化，实现了高效的长文本上下文建模。它采用动态分层稀疏策略，通过粗糙的令牌压缩和细粒度的令牌选择，既保留了全局上下文的感知能力，又确保了局部精度。该研究在稀疏注意力机制设计上提出了两个关键性创新点。<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%A4%A7%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23%E5%A4%A7%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#大模型#</span></a><a href="https://m.weibo.cn/p/index?extparam=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;containerid=100808f068f0dad74789bee210163c40a4b50d" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://n.sinaimg.cn/photo/5213b46e/20180926/timeline_card_small_super_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">人工智能</span></a><img style="" src="https://tvax3.sinaimg.cn/large/6f830abfly1hznizqy0akj21o00z37wh.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

该论文提出了一种名为NSA（Native Sparse Attention）的新型稀疏注意力机制，旨在解决传统注意力机制在长文本处理中的高计算成本问题。NSA通过动态分层稀疏策略，结合粗粒度令牌压缩和细粒度令牌选择，在保持全局上下文感知能力的同时提高局部精度。其创新点在于算法与硬件协同优化，实现了高效的长文本建模。该方法显著降低了计算开销，为大模型的长序列处理提供了更高效的解决方案。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-01T17:04:38Z
- **目录日期**: 2025-05-01
