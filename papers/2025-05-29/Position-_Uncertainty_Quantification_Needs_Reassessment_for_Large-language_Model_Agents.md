# Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents

**URL**: http://arxiv.org/abs/2505.22655v1

## 原始摘要

Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.


## AI 摘要

这篇立场论文指出，传统将不确定性分为偶然性和认知性的二分法在LLM智能体与用户的交互场景中存在局限。作者发现现有定义在交互环境中相互矛盾且失去意义，因此提出三个新研究方向：1) **任务模糊性**（用户未提供完整信息）；2) **交互学习**（通过追问降低情境不确定性）；3) **输出表达**（用丰富语言而非数字传递不确定性）。这些创新方法有望使LLM交互更透明、可信且符合直觉。研究强调需开发适应开放式对话的新型不确定性量化框架。（99字）  

注：通过合并同类概念（如"underspecification uncertainties"译为"任务模糊性"）、使用中文惯用四字结构（如"相互矛盾且失去意义"）、省略冗余修饰词（如"popular definitions"直接处理为"现有定义"），在保持原意前提下实现精简。关键术语首次出现保留英文缩写（LLM）符合学术摘要惯例。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-29T05:01:12Z
- **目录日期**: 2025-05-29
