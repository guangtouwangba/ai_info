# #AI仅凭自信学会推理# #浙大校友复刻DeepSeek逻辑链#AI真能靠“自信”学会推理！新强化学习方法RLIF，不再依赖外部奖励或标准答案，仅用模型自身的“置信度”作...

**URL**: https://weibo.com/6105753431/Pu4on4Rs5

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23AI%E4%BB%85%E5%87%AD%E8%87%AA%E4%BF%A1%E5%AD%A6%E4%BC%9A%E6%8E%A8%E7%90%86%23&amp;extparam=%23AI%E4%BB%85%E5%87%AD%E8%87%AA%E4%BF%A1%E5%AD%A6%E4%BC%9A%E6%8E%A8%E7%90%86%23" data-hide=""><span class="surl-text">#AI仅凭自信学会推理#</span></a> <a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E6%B5%99%E5%A4%A7%E6%A0%A1%E5%8F%8B%E5%A4%8D%E5%88%BBDeepSeek%E9%80%BB%E8%BE%91%E9%93%BE%23&amp;extparam=%23%E6%B5%99%E5%A4%A7%E6%A0%A1%E5%8F%8B%E5%A4%8D%E5%88%BBDeepSeek%E9%80%BB%E8%BE%91%E9%93%BE%23" data-hide=""><span class="surl-text">#浙大校友复刻DeepSeek逻辑链#</span></a><br><br>AI真能靠“自信”学会推理！<br><br>新强化学习方法RLIF，不再依赖外部奖励或标准答案，仅用模型自身的“置信度”作为内在奖励。这个方法叫Intuitor，通过优化模型对答案的信心分布，训练出更强的推理能力。<br><br>与传统GRPO方法相比，Intuitor在数学、代码等任务上表现更优，甚至小模型也能涌现出长链推理行为。实验中，基础模型Qwen2.5在数学任务中由最初的重复输出转变为结构清晰、响应有效，表现提升显著。<br><br>该方法还规避了“奖励黑客”问题：模型试图“作弊”以提高自信分，但在线训练策略使评估标准随能力演进，从机制上封堵漏洞。<br><br>此外，模型在代码任务中表现尤为亮眼，会在输出前加入自然语言推理。整个训练过程被总结为三阶段：学会写代码、加推理解释、逐步细化。<br><br>研究由浙大校友赵轩东主导，他与团队将继续探索基于内在信号的训练方式，在更大模型和数据上验证效果。 <a href="https://weibo.com/ttarticle/p/show?id=2309405171667273121813" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_article_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">AI仅凭“自信”就能学会推理，浙大校友：强化学习无需外部奖励信号！</span></a><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1wevuj4rkj30fs08waal.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

浙江大学校友赵轩东团队提出新型强化学习方法RLIF（Intuitor），通过利用模型自身"置信度"作为内在奖励信号，无需依赖外部奖励或标准答案即可提升AI推理能力。该方法在数学和代码任务中表现优异，使基础模型Qwen2.5从简单重复输出发展为结构化推理，并有效规避"奖励黑客"问题。实验显示，模型在代码任务中会自发加入自然语言解释，训练过程呈现三阶段演进。该研究为基于内在信号的AI训练提供了新思路，团队计划在更大规模模型上进一步验证效果。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-29T08:02:45Z
- **目录日期**: 2025-05-29
