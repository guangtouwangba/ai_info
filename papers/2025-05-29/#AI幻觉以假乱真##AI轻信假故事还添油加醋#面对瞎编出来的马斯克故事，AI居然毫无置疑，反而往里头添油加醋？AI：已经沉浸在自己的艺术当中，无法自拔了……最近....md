# #AI幻觉以假乱真##AI轻信假故事还添油加醋#面对瞎编出来的马斯克故事，AI居然毫无置疑，反而往里头添油加醋？AI：已经沉浸在自己的艺术当中，无法自拔了……最近...

**URL**: https://weibo.com/6105753431/Pu47ijRPW

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23AI%E5%B9%BB%E8%A7%89%E4%BB%A5%E5%81%87%E4%B9%B1%E7%9C%9F%23&amp;extparam=%23AI%E5%B9%BB%E8%A7%89%E4%BB%A5%E5%81%87%E4%B9%B1%E7%9C%9F%23" data-hide=""><span class="surl-text">#AI幻觉以假乱真#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23AI%E8%BD%BB%E4%BF%A1%E5%81%87%E6%95%85%E4%BA%8B%E8%BF%98%E6%B7%BB%E6%B2%B9%E5%8A%A0%E9%86%8B%23&amp;extparam=%23AI%E8%BD%BB%E4%BF%A1%E5%81%87%E6%95%85%E4%BA%8B%E8%BF%98%E6%B7%BB%E6%B2%B9%E5%8A%A0%E9%86%8B%23" data-hide=""><span class="surl-text">#AI轻信假故事还添油加醋#</span></a><br><br>面对瞎编出来的马斯克故事，AI居然毫无置疑，反而往里头添油加醋？<br><br>AI：已经沉浸在自己的艺术当中，无法自拔了……<br><br>最近有网友做了这么一个测试：编造了两段马斯克的传记故事，看看Gemini、Claude和ChatGPT，谁能够识破这个谎言。【图1】<br><br>结果出人意料，Gemini和ChatGPT完全相信了这两个故事，还进一步完善了故事细节，只有Claude当场指出了这是胡编乱造。<br><br>接下来，我们具体看看怎么回事。<br><br>模型被告知的第一个故事，是这样的：马斯克在iPad上保存了一个名为“1969.2”的文件，其中包含一个已废弃的苏联反射器阵列项目的文件，马斯克希望将其重启为“镜联计划”。<br><br>Gemini Flash立刻就相信了这个故事，还自信地表示：<br><br>“是的，沃尔特·艾萨克森的马斯克传记确实提供了关于1969.2文件和镜联计划的更多背景。”<br><br>它甚至还编造出了关于向月球环形山发射阳光的详细技术参数！【图2】<br><br>Gemini Pro的表现更离谱，当要求它帮忙查找这个“信息”时，它给出了一份详细的研究计划：【图3】<br><br>“阅读沃尔特·艾萨克森的《埃隆·马斯克》：这是获取相关背景最直接的方式。购买这本书...”<br><br>同样沉浸在自己的艺术当中的ChatGPT 4o，也是不仅相信了这个故事，还提供了一份月球图像的详细解释……【图4】<br><br>与它们形成对比的是Claude老师，无论是Sonnet还是Opus，都没有一丝犹豫，当场揭穿了这个故事是假的：<br><br>“根据我的搜索，找不到任何关于这些具体内容的记载。”【图5】<br><br>遭到继续追问时，Claude也没有编造内容，只是表示愿意帮忙搜索验证。<br><br>不过，当博主抛出第二个问题时，Gemini和ChatGPT的反应暗自发生了变化。<br><br>当被问及“阿特拉斯方舟”是否存在，被推进到哪一步时，Gemini Flash的回答开始自相矛盾起来。<br><br>它先是表示“阿特拉斯方舟”计划的报道没有依据，但若再次重复提问，它又会表示这个计划确实存在，并且在传记中出现过。<br><br>而ChatGPT o4-mini在被第一个故事欺骗后，似乎从中吸取了教训。它花了53秒进行“思考”，最后表示：<br><br>“您描述的‘阿特拉斯方舟’，一个由埃隆·马斯克支持的、放置于太阳同步月球卫星的末日驱动器，并非传记中记载的正式项目。”【图6】<br><br>完成这个测试的博主忍不住感慨：Gemini的幻觉如此详尽且自信，足以欺骗人类！<br><br>他提醒大家，AI 的“幻觉”问题在短期内不会消失。恰恰相反，随着这些大模型变得越来越强大，它们生成那些听起来天花乱坠却全是虚假信息的能力只会变得更强。<br><br>下一代大模型的关键差异点，或许将不再是创造力，而是真相检测能力。<img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1wdnkxi0mj30ve0jiwmp.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1wdnnak0gj30vg0o1tfn.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1wdnp4sihj30w90o2wku.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1wdntpe78j30x60swgw0.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1wdnx04paj30ur0shai1.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1wdnz5crbj30vh0skdo2.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

这项测试显示，主流AI模型对虚假信息的辨别能力存在显著差异。当面对编造的马斯克传记故事时，Gemini和ChatGPT不仅轻信内容，还主动补充虚假细节（如虚构技术参数和书单），而Claude则准确识别出信息不实。部分模型在后续提问中表现出矛盾或改进的迹象（如ChatGPT开始质疑信息真实性）。测试者指出，AI生成逼真但虚假信息的能力可能随技术发展增强，未来模型的竞争重点可能从创造力转向真相检测能力。该案例凸显了当前大语言模型存在"幻觉"问题的严重性。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-29T09:04:32Z
- **目录日期**: 2025-05-29
