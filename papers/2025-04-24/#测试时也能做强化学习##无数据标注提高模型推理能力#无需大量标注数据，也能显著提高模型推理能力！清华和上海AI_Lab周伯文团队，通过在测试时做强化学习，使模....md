# #测试时也能做强化学习##无数据标注提高模型推理能力#无需大量标注数据，也能显著提高模型推理能力！清华和上海AI Lab周伯文团队，通过在测试时做强化学习，使模...

**URL**: https://weibo.com/6105753431/PoK91DHi2

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E6%B5%8B%E8%AF%95%E6%97%B6%E4%B9%9F%E8%83%BD%E5%81%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%23&amp;extparam=%23%E6%B5%8B%E8%AF%95%E6%97%B6%E4%B9%9F%E8%83%BD%E5%81%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%23" data-hide=""><span class="surl-text">#测试时也能做强化学习#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E6%97%A0%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%23&amp;extparam=%23%E6%97%A0%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%23" data-hide=""><span class="surl-text">#无数据标注提高模型推理能力#</span></a><br><br>无需大量标注数据，也能显著提高模型推理能力！<br><br>清华和上海AI Lab周伯文团队，通过在测试时做强化学习，使模型的数学能力暴增了159%！<br><br>而且模型在多个数据集上的成绩均大幅提升，尤其是Qwen-2.5-Math-7B，它做AIME 2024竞赛题的成绩直接提高了159%。【图1】<br><br>实验过程中，强化学习的数据均由被训练的模型自身生成。<br><br>作者还发现，训练后的模型性能，准确性已经超过了用于训练它的伪标签（测试时强化学习过程中产生）。<br><br>DeepMind工程师评价，这种测试时强化学习的方式将改变LLM的格局：<br><br>它利用预训练模型和特定任务的提示进行实时自适应，而无需大量带标签的数据集，这是向前迈出的重要一步。【图2】<br><br>模型自己生成强化学习数据：作者提出的测试时强化学习（TTRL）过程是测试时扩展和测试时训练的结合，具体可以分为“生成、投票、强化”三个大步骤。【图3】<br><br>第一步生成的目的，是让模型针对每个输入的prompt，生成尽可能多样化的候选答案，该过程通过测试时推理来实现。<br><br>其思路是在推理阶段增加计算资源以获得更好的性能，具体到TTRL采用的是增加采样数量的方式，即对每个prompt，让模型采样生成N个不同的答案，而不是只生成一个确定性最高的输出。<br><br>作者的实验中，当在AIME 2024数据集上应用TTRL训练Qwen2.5-Math-7B模型时，每个prompt采样64次(N=64)，温度系数设为1.0，以鼓励模型生成多样化的答案。<br><br>投票过程从上一步生成的N个候选答案出发，通过多数投票的方式来估计正确答案，并将其作为伪标签。【图4】<br><br>TTRL在实际应用投票机制时还引入了一个参数 Maj<a href="https://weibo.com/n/N">@N</a>，表示多数投票的估计准确率。<br><br>它衡量的是伪标签与真实标签的一致性。通过控制Maj<a href="https://weibo.com/n/N">@N</a>，可以权衡伪标签的质量和数量。<br><br>最后一步利用强化学习，基于上一步估计出的伪标签，来优化语言模型的策略，使其倾向于给出正确答案。<br><br>TTRL采用GRPO算法，还加入了重要性采样和蒙特卡洛估计等技术，以提高训练效率和稳定性。<br><br>模型数学能力大幅提升  <br>为了评估TTRL的效果，作者在AIME 2024、AMC和MATH-500三个数据集上对调整前后的三款模型进行了测试。<br><br>在AIME 2024数据集上，对于Qwen2.5-Math-7B基础模型，TTRL将其准确率从16.7%提高到43.3%，提升幅度高达159.3%，超越了所有在大规模标注数据上训练的模型。  <br><br>在AMC数据集上，Qwen2.5-Math-7B、Qwen2.5-Math-1.5B和LLaMA模型的准确率分别获得了74.9%、63.1%和68.4%的大幅提高。  <br><br>MATH-500数据集上的表现更为突出，Qwen2.5-Math-7B和Qwen2.5-Math-1.5B分别实现了66.4%和142.4%的惊人提升，LLaMA模型的准确率也提高了29.3%。  <br>平均而言，TTRL使Qwen2.5-Math-7B模型在三个数据集上的性能提高了84.1%。【图5】<br><br>进一步的泛化性实验表明，在一个数据集上应用TTRL后，性能的提高可以自然迁移到其他数据集，甚至是从未参与训练的任务。【图6】<br><br>为了分析TTRL方法有效的原因，作者比较了TTRL训练前后模型的多数投票性能。<br><br>结果，应用TTRL后，模型的多数投票准确率（Maj<a href="https://weibo.com/n/64">@64</a>）显著高于原始的Qwen模型，说明通过多数投票得到的伪标签质量优于单个模型输出。【图7】<br><br>并且强化学习具备纠错能力。即使伪标签并非完全准确，强化学习也可以通过奖惩机制引导模型朝着正确方向优化。<br><br>从AIME 2024上标签准确率和奖励准确率的变化曲线中可以看到，即使在标签准确率较低的阶段，奖励准确率也能维持在90%以上。【图8】<br><br>作者简介  <br>这项研究的领导者是清华大学C3I课题组博士生张开颜和上海AI实验室青年研究员崔淦渠。<br><br>张开颜的导师是上海人工智能实验室主任、首席科学家周伯文教授；崔淦渠则毕业于清华NLP实验室，读博期间导师是刘知远副教授。<br><br>本文共同一作是张开颜和同样来自清华的Yuxin Zuo，周伯文和C3I课题组博士后丁宁是本文的通讯作者。【图9】<br><br>论文地址：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fabs%2F2504.16084" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i0rwh5wb8hj30zk0cqgs5.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i0rwh37oruj30zk099gq6.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3ly1i0rwh7jqqlj30zk0dzwit.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i0rwh8q3dcj30zk0hodl0.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i0rwh85unpj30qf0k0ahq.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3ly1i0rwh85q3tj30zk0evdj0.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i0rwh5k8b8j30zk0c60un.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i0rwh781e4j30zk0du441.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i0rwh7lcqjj30zk0e5gp9.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

清华大学与上海AI Lab周伯文团队提出"测试时强化学习(TTRL)"方法，通过"生成-投票-强化"三步骤显著提升大模型数学推理能力。该方法无需人工标注数据，由模型自身生成候选答案并通过多数投票产生伪标签，再使用GRPO算法进行强化学习优化。实验显示，Qwen-2.5-Math-7B模型在AIME竞赛题准确率提升159%，多个数据集平均提升84.1%，且具备跨任务泛化能力。该方法突破传统依赖标注数据的限制，被DeepMind评价为"改变LLM格局的重要一步"。论文已发布于arXiv。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-24T08:28:56Z
- **目录日期**: 2025-04-24
