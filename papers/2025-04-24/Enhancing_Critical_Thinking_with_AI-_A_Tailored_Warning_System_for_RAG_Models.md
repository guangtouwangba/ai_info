# Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models

**URL**: http://arxiv.org/abs/2504.16883v1

## 原始摘要

Retrieval-Augmented Generation (RAG) systems offer a powerful approach to
enhancing large language model (LLM) outputs by incorporating fact-checked,
contextually relevant information. However, fairness and reliability concerns
persist, as hallucinations can emerge at both the retrieval and generation
stages, affecting users' reasoning and decision-making. Our research explores
how tailored warning messages -- whose content depends on the specific context
of hallucination -- shape user reasoning and actions in an educational quiz
setting. Preliminary findings suggest that while warnings improve accuracy and
awareness of high-level hallucinations, they may also introduce cognitive
friction, leading to confusion and diminished trust in the system. By examining
these interactions, this work contributes to the broader goal of AI-augmented
reasoning: developing systems that actively support human reflection, critical
thinking, and informed decision-making rather than passive information
consumption.


## AI 摘要

检索增强生成(RAG)系统通过整合经过验证的上下文信息来提升大语言模型的输出质量，但仍存在公平性和可靠性问题。研究表明，在幻觉(错误信息)出现时提供针对性警告虽能提高用户准确性和意识，但也可能引发认知摩擦，导致困惑并降低系统信任度。这项在教育测验场景中的探索发现，警告对高层次幻觉有效，但需平衡其对用户决策的潜在干扰。该研究旨在推动AI辅助推理系统的发展，使其能更主动支持人类反思、批判性思维和知情决策，而非被动信息消费。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-24T18:01:54Z
- **目录日期**: 2025-04-24
