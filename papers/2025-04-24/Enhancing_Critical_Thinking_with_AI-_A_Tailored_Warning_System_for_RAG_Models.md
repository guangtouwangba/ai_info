# Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models

**URL**: http://arxiv.org/abs/2504.16883v1

## 原始摘要

Retrieval-Augmented Generation (RAG) systems offer a powerful approach to
enhancing large language model (LLM) outputs by incorporating fact-checked,
contextually relevant information. However, fairness and reliability concerns
persist, as hallucinations can emerge at both the retrieval and generation
stages, affecting users' reasoning and decision-making. Our research explores
how tailored warning messages -- whose content depends on the specific context
of hallucination -- shape user reasoning and actions in an educational quiz
setting. Preliminary findings suggest that while warnings improve accuracy and
awareness of high-level hallucinations, they may also introduce cognitive
friction, leading to confusion and diminished trust in the system. By examining
these interactions, this work contributes to the broader goal of AI-augmented
reasoning: developing systems that actively support human reflection, critical
thinking, and informed decision-making rather than passive information
consumption.


## AI 摘要

检索增强生成（RAG）系统通过整合经过事实核查的上下文相关信息，能有效提升大语言模型（LLM）的输出质量。然而，检索和生成阶段仍可能出现幻觉（错误信息），影响用户推理和决策的公平性与可靠性。本研究探讨了在教育测试场景中，根据幻觉具体情境定制的警告信息如何影响用户推理和行为。初步发现表明，警告虽能提高对高级别幻觉的识别准确性和意识，但也可能引发认知摩擦，导致用户困惑并降低系统信任度。该研究旨在推动AI辅助推理系统的发展，使其更积极地支持人类反思、批判性思维和知情决策。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-24T15:02:10Z
- **目录日期**: 2025-04-24
