# Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models

**URL**: http://arxiv.org/abs/2504.16883v1

## 原始摘要

Retrieval-Augmented Generation (RAG) systems offer a powerful approach to
enhancing large language model (LLM) outputs by incorporating fact-checked,
contextually relevant information. However, fairness and reliability concerns
persist, as hallucinations can emerge at both the retrieval and generation
stages, affecting users' reasoning and decision-making. Our research explores
how tailored warning messages -- whose content depends on the specific context
of hallucination -- shape user reasoning and actions in an educational quiz
setting. Preliminary findings suggest that while warnings improve accuracy and
awareness of high-level hallucinations, they may also introduce cognitive
friction, leading to confusion and diminished trust in the system. By examining
these interactions, this work contributes to the broader goal of AI-augmented
reasoning: developing systems that actively support human reflection, critical
thinking, and informed decision-making rather than passive information
consumption.


## AI 摘要

检索增强生成(RAG)系统通过整合事实核查的上下文信息提升大语言模型输出，但仍存在公平性和可靠性问题。研究发现，在教育测验场景中，针对特定幻觉情境的警告信息虽能提高准确性和对高级幻觉的认知，但也可能引发认知摩擦，导致用户困惑并降低系统信任度。该研究揭示了人机交互中的关键矛盾，为开发支持人类反思、批判性思维和知情决策(而非被动信息消费)的AI系统提供了重要参考。初步结果表明，平衡警告效果与用户体验是提升AI辅助推理系统的关键挑战。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-24T20:01:57Z
- **目录日期**: 2025-04-24
