# Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models

**URL**: http://arxiv.org/abs/2504.16883v1

## 原始摘要

Retrieval-Augmented Generation (RAG) systems offer a powerful approach to
enhancing large language model (LLM) outputs by incorporating fact-checked,
contextually relevant information. However, fairness and reliability concerns
persist, as hallucinations can emerge at both the retrieval and generation
stages, affecting users' reasoning and decision-making. Our research explores
how tailored warning messages -- whose content depends on the specific context
of hallucination -- shape user reasoning and actions in an educational quiz
setting. Preliminary findings suggest that while warnings improve accuracy and
awareness of high-level hallucinations, they may also introduce cognitive
friction, leading to confusion and diminished trust in the system. By examining
these interactions, this work contributes to the broader goal of AI-augmented
reasoning: developing systems that actively support human reflection, critical
thinking, and informed decision-making rather than passive information
consumption.


## AI 摘要

检索增强生成(RAG)系统通过整合经过验证的上下文相关信息来提升大语言模型的输出质量，但仍存在公平性和可靠性问题。研究表明，在幻觉可能出现的检索和生成阶段，定制化的警告信息能提高用户对高级别幻觉的识别准确率，但也可能引发认知摩擦，导致困惑并降低系统可信度。该研究探讨了在教育测验场景中，这类警告如何影响用户推理和决策行为，旨在推动AI辅助推理系统的发展——使其能主动支持人类反思、批判性思维和知情决策，而非被动消费信息。初步结果显示警告信息具有双重效应。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-24T21:01:52Z
- **目录日期**: 2025-04-24
