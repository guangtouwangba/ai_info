# Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models

**URL**: http://arxiv.org/abs/2504.16883v1

## 原始摘要

Retrieval-Augmented Generation (RAG) systems offer a powerful approach to
enhancing large language model (LLM) outputs by incorporating fact-checked,
contextually relevant information. However, fairness and reliability concerns
persist, as hallucinations can emerge at both the retrieval and generation
stages, affecting users' reasoning and decision-making. Our research explores
how tailored warning messages -- whose content depends on the specific context
of hallucination -- shape user reasoning and actions in an educational quiz
setting. Preliminary findings suggest that while warnings improve accuracy and
awareness of high-level hallucinations, they may also introduce cognitive
friction, leading to confusion and diminished trust in the system. By examining
these interactions, this work contributes to the broader goal of AI-augmented
reasoning: developing systems that actively support human reflection, critical
thinking, and informed decision-making rather than passive information
consumption.


## AI 摘要

检索增强生成（RAG）系统通过整合经过验证的上下文相关信息来提升大语言模型（LLM）的输出质量，但仍存在公平性和可靠性问题，因为检索和生成阶段都可能产生幻觉（错误信息），影响用户的推理和决策。研究探讨了在教育测验场景中，针对特定幻觉情境的定制警告信息如何影响用户推理和行为。初步结果表明，警告虽能提高准确性和对高级幻觉的警觉，但也可能引发认知摩擦，导致混乱并削弱系统信任。该研究旨在推动AI辅助推理的发展，构建能主动支持人类反思、批判性思维和知情决策的系统。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-24T04:02:07Z
- **目录日期**: 2025-04-24
