# Self-reflecting Large Language Models: A Hegelian Dialectical Approach

**URL**: http://arxiv.org/abs/2501.14917v5

## 原始摘要

Investigating NLP through a philosophical lens has recently caught
researcher's eyes as it connects computational methods with classical schools
of philosophy. This paper introduces a philosophical approach inspired by the
\textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a
self-dialectical approach to emulate internal critiques and then synthesize new
ideas by resolving the opposing points of view. Moreover, this paper
investigates the effect of LLMs' temperature for generation by establishing a
dynamic annealing approach, which promotes the creativity in the early stages
and gradually refines it by focusing on the nuances, as well as a
fixed-temperature strategy for generation. We assess the effectiveness of our
proposed method in generating novel ideas and in improving the reasoning
abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent
Majority Voting (MAMV) strategy to assess the validity and novelty of the
generated ideas, which proves useful in the absence of domain experts. Our
experiments demonstrate promising results in generating ideas and enhancing
problem-solving performance.


## AI 摘要

这篇论文提出了一种受黑格尔辩证法启发的哲学方法，用于增强大语言模型(LLMs)的自我反思能力。通过自辩证方法模拟内部批判并综合对立观点来产生新想法。研究探讨了动态退火策略(早期促进创意，后期聚焦细节)和固定温度策略对生成效果的影响，评估了该方法在创新想法生成和问题解决推理方面的有效性。此外，作者采用多智能体多数投票(MAMV)策略来评估生成想法的有效性和新颖性，在缺乏领域专家时特别有用。实验结果表明，该方法在创意生成和问题解决性能提升方面取得了良好效果。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-14T09:01:57Z
- **目录日期**: 2025-05-14
