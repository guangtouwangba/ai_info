# #小米开源首个AI推理模型##小米LLM团队模型首秀#小米开源了首个推理模型MiMo，主打一个“小而强”。打开论文，右上角就是一个大大的小米Logo，论文署名则是“小...

**URL**: https://weibo.com/6105753431/PrMPaAkyp

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%B0%8F%E7%B1%B3%E5%BC%80%E6%BA%90%E9%A6%96%E4%B8%AAAI%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23%E5%B0%8F%E7%B1%B3%E5%BC%80%E6%BA%90%E9%A6%96%E4%B8%AAAI%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#小米开源首个AI推理模型#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%B0%8F%E7%B1%B3LLM%E5%9B%A2%E9%98%9F%E6%A8%A1%E5%9E%8B%E9%A6%96%E7%A7%80%23&amp;extparam=%23%E5%B0%8F%E7%B1%B3LLM%E5%9B%A2%E9%98%9F%E6%A8%A1%E5%9E%8B%E9%A6%96%E7%A7%80%23" data-hide=""><span class="surl-text">#小米LLM团队模型首秀#</span></a><br><br>小米开源了首个推理模型MiMo，主打一个“小而强”。<br><br>打开论文，右上角就是一个大大的小米Logo，论文署名则是“小米LLM核心团队“。【图1】<br><br>MiMo-7B全系列包含4个模型版本：<br><br>1. MiMo-7B-Base：基础预训练模型，训练数据规模达25万亿tokens，强调“推理密度”，并引入MTP（Multi-Token Prediction，多token预测）机制，显著提升生成速度和质量；<br><br>2. MiMo-7B-SFT：在Base模型上进行监督微调（Supervised Fine-Tuning），作为强化学习（RL）的热启动版本；<br><br>3. MiMo-7B-RL-Zero：直接从Base模型进行冷启动的强化学习训练；<br><br>4. MiMo-7B-RL：在SFT模型基础上热启动再强化，当前为性能最强版本<br><br>按理说，推理任务往往得靠大参数模型来扛，但MiMo仅7B的体量，就超越了多个大模型：<br><br>- 在AIME 2025数学测试中，MiMo-7B-RL取得55.4分，高于OpenAI的o1-mini（50.7）与阿里巴巴的Qwen-32B（32.4）；<br><br>- 在代码测试基准LiveCodeBench v5中，MiMo-7B通过率达57.8%，领先于o1-mini（53.8）和Qwen-32B（41.9）；<br><br>- 在32K长上下文的逻辑追踪和推理任务中，MiMo也进入了领先梯队。<br><br>这不是单纯“微调”出来的结果，拆解来看，MiMo的能力进步主要靠两个阶段的创新联动：<br><br>1、预训练阶段，打下推理的底子：<br><br>- 数据不仅量大（25T tokens），还特别聚焦推理密度，自主生成了约2000亿条推理相关语料；<br><br>- 采用“三阶段训练法”，逐层提升难度，让模型逐步掌握复杂推理结构；<br><br>- 引入MTP机制，提前预判生成路径，提升推理效率与准确率。<br><br>2、后训练阶段，用RL强化逻辑能力：<br><br>- 提出“Test Difficulty Driven Reward”（按题目难度打分）机制，有效缓解代码任务中reward稀疏问题；<br><br>- 引入Easy Data Re-Sampling策略，提高训练样本利用效率；<br><br>- 搭建Seamless Rollout引擎，实现训练过程全异步处理，训练提速2.29倍，验证阶段提速1.96倍。<br><br>目前，小米已在GitHub、HuggingFace、ModelScope三个平台全面开源MiMo全系列模型，包括Base、SFT及两种RL版本，相关代码与训练方案也已全部公开。<br><br>感兴趣的小伙伴可以点击：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fgithub.com%2FXiaomiMiMo%2FMiMo" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>论文：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fwww.arxiv.org%2Fabs%2F2505.07608" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1f1tz2kmwj310e0w8k47.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i1f1win52fj315d0u0mzf.jpg" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax2.sinaimg.cn/orj480/006Fd7o3ly1i1f1whr9t3j315d0u0mzf.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/zHiwZYFFlx08oeNgVF0s010412006V2x0E010.mp4?label=mp4_720p&amp;template=992x720.25.0&amp;ori=0&amp;ps=1Cx9YB1mmR49jS&amp;Expires=1747213355&amp;ssig=l12qrl0BfX&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/x89kcxbmlx08oeNgR6l2010412003pl50E010.mp4?label=mp4_hd&amp;template=660x480.25.0&amp;ori=0&amp;ps=1Cx9YB1mmR49jS&amp;Expires=1747213355&amp;ssig=1bN9qIwI5a&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/yRv4n4J1lx08oeNgHkWk0104120022Hj0E010.mp4?label=mp4_ld&amp;template=496x360.25.0&amp;ori=0&amp;ps=1Cx9YB1mmR49jS&amp;Expires=1747213355&amp;ssig=8X2uIiLHRO&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5166224800415753" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

小米开源首个AI推理模型MiMo-7B，包含Base、SFT和两个强化学习版本（RL-Zero/RL）。该7B参数模型在数学（AIME 2025）、代码（LiveCodeBench）和长文本推理任务中超越部分大模型，如Qwen-32B。其优势源于：1）预训练阶段采用25万亿token高推理密度数据、三阶段训练法和多token预测（MTP）；2）强化学习阶段创新难度驱动奖励机制和异步训练引擎，提速2.29倍。模型及代码已在GitHub等平台开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-14T08:02:55Z
- **目录日期**: 2025-05-14
