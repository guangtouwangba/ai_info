# #一组动画秒懂L1和L2范数##L1和L2怎么影响解形状#在优化问题中，L1范数鼓励稀疏，L2范数倾向均衡，但为什么会这样？Keenan Crane用5个短视频，从柱状图到二维几...

**URL**: https://weibo.com/6105753431/PnQLAdQLC

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E4%B8%80%E7%BB%84%E5%8A%A8%E7%94%BB%E7%A7%92%E6%87%82L1%E5%92%8CL2%E8%8C%83%E6%95%B0%23&amp;extparam=%23%E4%B8%80%E7%BB%84%E5%8A%A8%E7%94%BB%E7%A7%92%E6%87%82L1%E5%92%8CL2%E8%8C%83%E6%95%B0%23" data-hide=""><span class="surl-text">#一组动画秒懂L1和L2范数#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23L1%E5%92%8CL2%E6%80%8E%E4%B9%88%E5%BD%B1%E5%93%8D%E8%A7%A3%E5%BD%A2%E7%8A%B6%23&amp;extparam=%23L1%E5%92%8CL2%E6%80%8E%E4%B9%88%E5%BD%B1%E5%93%8D%E8%A7%A3%E5%BD%A2%E7%8A%B6%23" data-hide=""><span class="surl-text">#L1和L2怎么影响解形状#</span></a><br><br>在优化问题中，L1范数鼓励稀疏，L2范数倾向均衡，但为什么会这样？<br><br>Keenan Crane用5个短视频，从柱状图到二维几何图，再到不等式可视化，给出了非常直观的解释。<br><br>如果你在接触特征选择、正则化、Compressed Sensing，这组动画能让你快速建立直觉。<br><br>视频1：最小化L2范数，固定L1约束（柱状图视角）  <br>问题是min ‖x‖₂²，约束是‖x‖₁=1。  <br>每个向量分量被表示为一个柱子，总高度固定，目标是最小化这些柱子高度的平方和。  <br>动画中，柱子逐渐趋于等高，因为某个柱子一旦过高，其平方惩罚迅速变大。  <br>最终最优解是所有分量相等，L2鼓励均衡分配。<br><br>视频2：最小化L1范数，固定L2约束（柱状图视角）  <br>问题是min ‖x‖₁，约束是‖x‖₂²=1。  <br>总能量（平方和）固定，目标是让总高度（L1范数）尽量小。  <br>动画中，最终解表现为一个柱子为1，其余全为0。  <br>这是一个典型的稀疏解，L1鼓励集中在少数分量上。<br><br>视频3：L1平方 ≥ L2平方的几何解释（柱状图+面积）  <br>不等式‖x‖₂² ≤ ‖x‖₁²总是成立。  <br>动画用柱状图表示每个分量，同时用面积做直观比较：<br>一个边长为‖x‖₁的正方形代表L1平方，柱子组成的蓝色小方块面积为L2平方。  <br>无论柱子怎么变化，L2的面积总不会超过L1。  <br>直观说明：固定L1时，为降低L2只能平均分；<br>固定L2时，为降低L1就要压缩非零分量。<br><br>视频4：最小化L2范数，固定L1约束（二维几何视角）  <br>问题仍是min ‖x‖₂²，s.t. ‖x‖₁=1。  <br>L1约束在二维中画出来是一个菱形，L2的等高线是同心圆。  <br>动画显示最小的圆与菱形边中点相切，对应点在(±0.5, ±0.5)。  <br>几何上也清晰呈现：L2的最小解分布在多个方向上，是“平均”取值。<br><br>视频5：最小化L1范数，固定L2约束（二维几何视角）  <br>问题是min ‖x‖₁，s.t. ‖x‖₂²=1。  <br>这次约束是一个圆，目标函数等高线是菱形。  <br>动画中最小的菱形刚好在圆上与坐标轴交点相切，最优解在(±1,0)或(0,±1)。  <br>说明解落在坐标轴上，仅一个分量非零，是标准的稀疏向量。<br><br>总结一下就是：<br><br>- L2最小化时，会抑制单一分量过大，促使向量分量趋于均衡；<br>- L1最小化时，则倾向减少非零分量数量，实现稀疏化；<br>- 正是这两种范数的不同特性，使得我们能构造出如Lasso、Compressed Sensing等稀疏建模方法。<br><br>这5个视频从柱状图直觉、不等式推导到二维几何视角，构成一个完整的理解链条，通俗易懂，非常值得反复观看。<img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i0l4fqxucvj31hc0u0gmr.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i0l4ftkp3fj31hc0u00u2.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i0l4ftbze1j31hc0u0gmt.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i0l4fu7ckhj31hc0u0abe.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i0l4fu8wzbj31hc0u0ta6.jpg" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax2.sinaimg.cn/orj480/006Fd7o3ly1i0l4frlgj3j31hc0u0gmr.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/P7QbK6aolx08nzv9TXzi010412000MTK0E010.mp4?label=mp4_720p&amp;template=1280x720.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1744988557&amp;ssig=ISzwEjSWzd&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/ZfkQkSWSlx08nzv9MqPm010412000waD0E010.mp4?label=mp4_hd&amp;template=852x480.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1744988557&amp;ssig=KOwTKnyvxs&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/VyjDyuCxlx08nzv9HyEg010412000nrL0E010.mp4?label=mp4_ld&amp;template=640x360.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1744988557&amp;ssig=TikTFhhUvT&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5156836567875627" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

这组动画直观展示了L1和L2范数的关键区别：L2最小化（固定L1约束）会促使各分量趋于均衡（柱状图等高/二维中点解），因其平方惩罚对大值更敏感；而L1最小化（固定L2约束）则产生稀疏解（单柱为1/二维坐标轴解），因其线性惩罚鼓励集中。几何上，L1约束形成菱形（促进角点解），L2约束形成圆形（促进均匀解）。这种特性解释了为何L1正则化能用于特征选择（如Lasso），而L2适合防止过拟合（如岭回归）。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-18T14:03:26Z
- **目录日期**: 2025-04-18
