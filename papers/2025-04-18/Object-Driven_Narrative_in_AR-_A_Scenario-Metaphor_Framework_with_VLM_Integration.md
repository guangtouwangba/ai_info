# Object-Driven Narrative in AR: A Scenario-Metaphor Framework with VLM Integration

**URL**: http://arxiv.org/abs/2504.13119v1

## 原始摘要

Most adaptive AR storytelling systems define environmental semantics using
simple object labels and spatial coordinates, limiting narratives to rigid,
pre-defined logic. This oversimplification overlooks the contextual
significance of object relationships-for example, a wedding ring on a
nightstand might suggest marital conflict, yet is treated as just "two objects"
in space. To address this, we explored integrating Vision Language Models
(VLMs) into AR pipelines. However, several challenges emerged: First, stories
generated with simple prompt guidance lacked narrative depth and spatial usage.
Second, spatial semantics were underutilized, failing to support meaningful
storytelling. Third, pre-generated scripts struggled to align with AR
Foundation's object naming and coordinate systems. We propose a scene-driven AR
storytelling framework that reimagines environments as active narrative agents,
built on three innovations: 1. State-aware object semantics: We decompose
object meaning into physical, functional, and metaphorical layers, allowing
VLMs to distinguish subtle narrative cues between similar objects. 2.
Structured narrative interface: A bidirectional JSON layer maps VLM-generated
metaphors to AR anchors, maintaining spatial and semantic coherence. 3. STAM
evaluation framework: A three-part experimental design evaluates narrative
quality, highlighting both strengths and limitations of VLM-AR integration. Our
findings show that the system can generate stories from the environment itself,
not just place them on top of it. In user studies, 70% of participants reported
seeing real-world objects differently when narratives were grounded in
environmental symbolism. By merging VLMs' generative creativity with AR's
spatial precision, this framework introduces a novel object-driven storytelling
paradigm, transforming passive spaces into active narrative landscapes.


## AI 摘要

当前AR叙事系统大多依赖简单物体标签和空间坐标，导致叙事逻辑僵化。为解决这一问题，研究者探索将视觉语言模型(VLM)融入AR流程，但面临叙事深度不足、空间语义利用不充分、脚本与AR系统不兼容等挑战。为此，研究提出场景驱动的AR叙事框架，包含三大创新：1)状态感知的物体语义分层(物理/功能/隐喻)；2)双向JSON接口连接VLM隐喻与AR锚点；3)STAM评估体系。用户研究显示70%参与者因环境象征性叙事而改变对实物的认知。该框架通过结合VLM创造力与AR空间精度，实现了从"空间放置故事"到"环境生成故事"的范式转变。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-18T21:01:31Z
- **目录日期**: 2025-04-18
