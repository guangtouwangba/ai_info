# MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators

**URL**: http://arxiv.org/abs/2505.22777v2

## 原始摘要

As the capabilities of chatbots and their underlying LLMs continue to
dramatically improve, evaluating their performance has increasingly become a
major blocker to their further development. A major challenge is the available
benchmarking datasets, which are largely static, outdated, and lacking in
multilingual coverage, limiting their ability to capture subtle linguistic and
cultural variations. This paper introduces MEDAL, an automated multi-agent
framework for generating, evaluating, and curating more representative and
diverse open-domain dialogue evaluation benchmarks. Our approach leverages
several state-of-the-art LLMs to generate user-chatbot multilingual dialogues,
conditioned on varied seed contexts. A strong LLM (GPT-4.1) is then used for a
multidimensional analysis of the performance of the chatbots, uncovering
noticeable cross-lingual performance differences. Guided by this large-scale
evaluation, we curate a new meta-evaluation multilingual benchmark and
human-annotate samples with nuanced quality judgments. This benchmark is then
used to assess the ability of several reasoning and non-reasoning LLMs to act
as evaluators of open-domain dialogues. We find that current LLMs struggle to
detect nuanced issues, particularly those involving empathy and reasoning.


## AI 摘要

随着聊天机器人及其底层大语言模型(LLM)能力的快速提升，性能评估成为发展瓶颈。现有基准数据集多为静态、过时且缺乏多语言覆盖，难以捕捉细微的语言文化差异。研究团队提出MEDAL框架，利用多个先进LLM生成多语言对话，通过GPT-4.1进行多维度分析，发现显著的跨语言性能差异。基于大规模评估结果，团队构建了新的多语言元评估基准，并人工标注了包含细微质量判断的样本。测试发现，当前LLM在评估开放域对话时，难以识别涉及同理心和推理的细微问题。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-09T23:01:56Z
- **目录日期**: 2025-06-09
