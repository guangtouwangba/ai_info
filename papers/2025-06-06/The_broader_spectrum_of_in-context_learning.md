# The broader spectrum of in-context learning

**URL**: http://arxiv.org/abs/2412.03782v3

## 原始摘要

The ability of language models to learn a task from a few examples in context
has generated substantial interest. Here, we provide a perspective that
situates this type of supervised few-shot learning within a much broader
spectrum of meta-learned in-context learning. Indeed, we suggest that any
distribution of sequences in which context non-trivially decreases loss on
subsequent predictions can be interpreted as eliciting a kind of in-context
learning. We suggest that this perspective helps to unify the broad set of
in-context abilities that language models exhibit -- such as adapting to tasks
from instructions or role play, or extrapolating time series. This perspective
also sheds light on potential roots of in-context learning in lower-level
processing of linguistic dependencies (e.g. coreference or parallel
structures). Finally, taking this perspective highlights the importance of
generalization, which we suggest can be studied along several dimensions: not
only the ability to learn something novel, but also flexibility in learning
from different presentations, and in applying what is learned. We discuss
broader connections to past literature in meta-learning and goal-conditioned
agents, and other perspectives on learning and adaptation. We close by
suggesting that research on in-context learning should consider this broader
spectrum of in-context capabilities and types of generalization.


## AI 摘要

这篇论文提出了一个更广泛的视角来看待语言模型的上下文学习能力。作者认为，任何能够通过上下文显著降低后续预测损失的序列分布都可以被视为引发某种形式的上下文学习。这一观点有助于统一语言模型展现的各种上下文能力，如根据指令调整任务、角色扮演或时间序列外推等。研究还揭示了上下文学习可能源于语言依赖关系（如共指或平行结构）的低层次处理，并强调了泛化能力的重要性——包括学习新内容、从不同呈现方式中学习以及应用所学知识等方面。作者建议未来研究应考虑更广泛的上下文能力和泛化类型。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-06T14:02:31Z
- **目录日期**: 2025-06-06
