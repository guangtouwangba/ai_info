# #推理模型倾向于知难而退##苹果新研究发现推理模型越难越摆烂#越困难的问题，推理模型一定会越努力解决吧！十分反直觉的是：当问题难到一定程度，推理模型反而就...

**URL**: https://weibo.com/6105753431/Pvh05EbzG

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E5%80%BE%E5%90%91%E4%BA%8E%E7%9F%A5%E9%9A%BE%E8%80%8C%E9%80%80%23&amp;extparam=%23%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E5%80%BE%E5%90%91%E4%BA%8E%E7%9F%A5%E9%9A%BE%E8%80%8C%E9%80%80%23" data-hide=""><span class="surl-text">#推理模型倾向于知难而退#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%8B%B9%E6%9E%9C%E6%96%B0%E7%A0%94%E7%A9%B6%E5%8F%91%E7%8E%B0%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E8%B6%8A%E9%9A%BE%E8%B6%8A%E6%91%86%E7%83%82%23&amp;extparam=%23%E8%8B%B9%E6%9E%9C%E6%96%B0%E7%A0%94%E7%A9%B6%E5%8F%91%E7%8E%B0%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%E8%B6%8A%E9%9A%BE%E8%B6%8A%E6%91%86%E7%83%82%23" data-hide=""><span class="surl-text">#苹果新研究发现推理模型越难越摆烂#</span></a><br><br>越困难的问题，推理模型一定会越努力解决吧！<br><br>十分反直觉的是：当问题难到一定程度，推理模型反而就开始“摆烂”了。<br><br>不久前，苹果机器学习研究中心的研究团队对这样一个问题产生了好奇：推理模型（LRM）真的在“思考”吗？问题的复杂度如何影响了它们的表现？【图1】<br><br>于是，他们深入观察了几个推理模型（Claude 3.7 Sonnet、DeepSeek-R1、o3-mini、DeepSeek-R1-Qwen-32B）的“思考”过程，得出了几项有趣的结论：<br><br>- 中等复杂度是推理模型的“舒适区”：实验表明，推理模型只有在中等复杂度下的表现优于标准语言模型。一旦问题复杂度超过某个阈值，推理模型和标准语言模型的准确率都会彻底“崩盘”。【图2】<br><br>- 越难越“不想思考”：随着问题难度增加，推理模型一开始会进行更多“思考”，但很快它们的“思考量”反而会减少，即使它们还有充足的token预算。【图3】<br><br>- 面对简单问题，模型会在早期就找到答案，但继续"过度思考"出错；中等问题时，模型会先探索错误的路径，但在后期能找到正确的解决方案；困难问题则完全失败。【图4】<br><br>- 即使直接给模型解决问题的算法，让它们只需按步骤执行，推理模型仍然在相同的复杂度点上失败。这表明，AI在处理符号操作上存在根本性的局限。【图5】<br><br>可能有人就要问了，这个实验如何进行的？<br><br>具体来说，研究团队研究设计了四种可控的谜题环境：汉诺塔、跳棋、过河问题和积木世界，来探索推理模型的思考过程。【图6】<br><br>他们通过调整谜题中的元素数量（比如汉诺塔的盘子数量），来控制问题的复杂度。问题规模越大，需要的推理步骤就越多，复杂度也就越高。<br><br>这种实验环境的好处是，可以排除数学和编程基准测试中可能存在的“数据污染”，精确地控制问题的组合复杂度，同时保持逻辑结构的一致性。让研究人员能够分析最终答案以及AI内部的推理痕迹，从而深入了解推理模型的“思考”过程。<br><br>因此，从最终结果来看，尽管这些推理模型通过强化学习训练获得了复杂的自我反思机制，但它们局限性可能仍然比想象中严重。<br><br>论文地址：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmachinelearning.apple.com%2Fresearch%2Fillusion-of-thinking" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i25k9ewno6j30zk0yjarj.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i25k9g3ia8j30xm0jhwnx.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i25k9i0svyj30xc0laalv.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i25k9m8af4j32321581kx.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i25k9nk3xnj30zk0eltg3.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i25k9pahgoj30zk0d5jvu.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

苹果机器学习团队研究发现，推理模型（如Claude 3.7 Sonnet等）存在"知难而退"现象：面对中等复杂度问题时表现最佳，但难度超过阈值后准确率会骤降。实验通过汉诺塔等可控谜题发现，模型在困难任务中会减少"思考量"（即使有充足计算资源），且在符号操作上存在根本局限。有趣的是，简单问题会导致"过度思考"出错，中等问题能后期修正，而困难问题则完全失败。研究表明，当前AI的推理能力存在比预期更严重的局限性。论文链接：https://machinelearning.apple.com/research/illusion-of-thinking

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-06T06:01:20Z
- **目录日期**: 2025-06-06
