# Visual Graph Arena: Evaluating Visual Conceptualization of Vision and Multimodal Large Language Models

**URL**: http://arxiv.org/abs/2506.06242v1

## 原始摘要

Recent advancements in multimodal large language models have driven
breakthroughs in visual question answering. Yet, a critical gap persists,
`conceptualization'-the ability to recognize and reason about the same concept
despite variations in visual form, a basic ability of human reasoning. To
address this challenge, we introduce the Visual Graph Arena (VGA), a dataset
featuring six graph-based tasks designed to evaluate and improve AI systems'
capacity for visual abstraction. VGA uses diverse graph layouts (e.g.,
Kamada-Kawai vs. planar) to test reasoning independent of visual form.
Experiments with state-of-the-art vision models and multimodal LLMs reveal a
striking divide: humans achieved near-perfect accuracy across tasks, while
models totally failed on isomorphism detection and showed limited success in
path/cycle tasks. We further identify behavioral anomalies suggesting
pseudo-intelligent pattern matching rather than genuine understanding. These
findings underscore fundamental limitations in current AI models for visual
understanding. By isolating the challenge of representation-invariant
reasoning, the VGA provides a framework to drive progress toward human-like
conceptualization in AI visual models. The Visual Graph Arena is available at:
\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}


## AI 摘要

近期多模态大语言模型在视觉问答方面取得突破，但AI系统仍缺乏人类的关键能力——"概念化"，即识别和推理同一概念在不同视觉形式中的表现。为此，研究者提出了视觉图竞技场(VGA)数据集，包含6个基于图的任务，用于评估AI的视觉抽象能力。实验显示，人类在任务中表现近乎完美，而顶尖视觉模型和多模态大模型在图同构检测上完全失败，在路径/环任务中表现有限。这表明当前AI模型存在根本性局限，仅能进行表面模式匹配而非真正理解。VGA为推进AI视觉概念化能力提供了评估框架。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-10T02:31:50Z
- **目录日期**: 2025-06-10
