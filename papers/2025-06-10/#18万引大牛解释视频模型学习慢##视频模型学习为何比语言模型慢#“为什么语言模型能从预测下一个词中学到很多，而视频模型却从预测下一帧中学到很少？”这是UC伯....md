# #18万引大牛解释视频模型学习慢##视频模型学习为何比语言模型慢#“为什么语言模型能从预测下一个词中学到很多，而视频模型却从预测下一帧中学到很少？”这是UC伯...

**URL**: https://weibo.com/6105753431/PvU9hl8q0

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%2318%E4%B8%87%E5%BC%95%E5%A4%A7%E7%89%9B%E8%A7%A3%E9%87%8A%E8%A7%86%E9%A2%91%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%85%A2%23&amp;extparam=%2318%E4%B8%87%E5%BC%95%E5%A4%A7%E7%89%9B%E8%A7%A3%E9%87%8A%E8%A7%86%E9%A2%91%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E6%85%A2%23" data-hide=""><span class="surl-text">#18万引大牛解释视频模型学习慢#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%A7%86%E9%A2%91%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B8%BA%E4%BD%95%E6%AF%94%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%85%A2%23&amp;extparam=%23%E8%A7%86%E9%A2%91%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E4%B8%BA%E4%BD%95%E6%AF%94%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%85%A2%23" data-hide=""><span class="surl-text">#视频模型学习为何比语言模型慢#</span></a><br><br>“为什么语言模型能从预测下一个词中学到很多，而视频模型却从预测下一帧中学到很少？”<br><br>这是UC伯克利大学计算机副教授Sergey Levine最新提出的灵魂一问。<br><br>他同时是Google Brain的研究员，参与了Google知名机器人大模型PALM-E、RT1和RT2等项目。<br><br>Sergey Levine在谷歌学术的被引用次数高达18万次。【图1】<br><br>“柏拉图洞穴”是一个很古老的哲学比喻，通常被用来说明人们对世界认知的局限性。<br><br>Sergey Levine的这篇文章以《柏拉图洞穴中的语言模型》为题，又想要揭示AI的哪些缺陷呢？【图2】<br><br>在文章的开头，作者提到人工智能就是在研究能够反映人类智能的灵活性和适应性的假想智能。<br><br>一些研究者推测，人类心智的复杂性和灵活性源自于大脑中应用的一个单一算法，通过这个算法可以实现所有多样化的能力。<br><br>也就是说，AI如果能复现这个终极算法，人工智能就能通过经验自主获取多元能力，达到人类智能的高度。<br><br>在这个探索过程中，语言模型取得了非常成功的突破。<br><br>甚至，LLMs实现能力跃升背后的算法（下一词预测+强化学习微调），也非常简单。<br><br>单一终极算法假设似乎就是AI模型的答案……<br><br>然而，这个假设对视频模型并不适用。<br><br>早在基于Transformer的语言模型出现之前，AI研究人员就已经忙着研究一个看似非常相似的问题：<br><br>“就像LLM通过预测来自网络文本数据的下一个词来学习一样，视频模型也可能通过预测视频数据的下一个帧来学习。”<br><br>甚至从数据上来说，视频数据比文本数据包含的信息更丰富，那么预测下一帧得到的认知会远比预测下一词得到的认知更全面。<br><br>比如，一个飞往宇宙探索的机器人，在那里没有人能提供文本，但是它仍然能获取丰富的视频数据。<br><br>基于这些证据，我们可以认为能够“观察”到更多物理世界的视频模型应该比语言模型更加强大。<br><br>然而，事情并没有按研究人员所期望的那样发展。<br><br>尽管视频预测模型可以生成逼真的视频，但在解决复杂问题、进行复杂推理方面，语言模型仍然是主要且唯一的选择。<br><br>这与LLMs对物理世界的“观察”更少，却获得了更复杂的认知能力形成了鲜明对比。<br><br>就像文章中举出的例子：我们并不能用Veo 3估算夏威夷群岛的岩石体积是否比珠穆朗玛峰更大，但ChatGPT却可以回答这个问题。<br><br>这是因为LLMs只需要调用人类总结的地理知识（文本中已有相关数据或推理路径）。<br><br>简单来说，视频数据是物理世界的直接映射，而非人类认知的加工产物。<br><br>视频模型需自主归纳物理规律，而LLMs却可以 “抄近路” 模仿人类已有的推理结果。<br><br>LLMs仅接触文本 “影子”（人类认知的投影），却比直接观察物理世界的视频模型更具推理能力。<br><br>作者认为，这是LLMs只会对人类进行“脑部扫描”，而非真正学会了像人类一样推理问题。<br><br>就像是AI系统存在于「柏拉图洞穴」中。<br><br>AI通过语言模型学习人类的知识和思维方式，但这些知识就像洞穴墙壁上的影子，是人类智慧的间接反映。<br><br>它们并没有真正理解世界，其能力是对人类认知的 “逆向工程”，而不是自主探索。<br><br>而视频模型目前连影子都无法认知……<br><br>那么，AI该如何走出洞穴呢？<br><br>作者认为既然LLMs已经了实现人类认知的部分模拟（如推理、生成），那么就可以将它可 “心智原型”，为通用AI提供起点。<br><br>而长期目标则是突破“影子依赖”，不再依赖人类中介（类似文本数据），让AI通过传感器直接与物理世界交互，自主探索。<br><br>对此，有评论者提出：视觉、语言、行动系统就像独立的洞穴，如果能够通过共享结构建立桥梁，可能就不需要逃离“洞穴”，跨模态连接就成了探索过程中的挑战，需要找到一个连接这些模态的统一的方法。【图3】<br><br>对于AI的“洞穴困境”，你有怎样的看法呢？<img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i2ad3jqx5yj30h608it9j.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3ly1i2ad3n1rsnj30zk0830un.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i2ad3p8jiwj30zk0cjjy2.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

UC伯克利教授Sergey Levine提出核心问题：语言模型通过预测下一个词获得强大推理能力，而视频模型预测下一帧却学习效率低下。他认为语言模型只是对人类认知的"逆向工程"，如同柏拉图洞穴中看到的影子，并未真正理解物理世界。视频模型虽直接观察现实，但缺乏人类加工的知识捷径。解决方案是：短期以语言模型为"心智原型"，长期让AI通过传感器直接探索物理世界。跨模态连接可能是突破"认知洞穴"的关键。该观点引发对AI本质学习方式的深度思考。（98字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-10T18:03:44Z
- **目录日期**: 2025-06-10
