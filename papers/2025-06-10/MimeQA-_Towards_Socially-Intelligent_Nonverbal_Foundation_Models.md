# MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models

**URL**: http://arxiv.org/abs/2502.16671v2

## 原始摘要

As AI becomes more closely integrated with peoples' daily activities,
socially intelligent AI that can understand and interact seamlessly with humans
in daily lives is increasingly important. However, current works in AI social
reasoning all rely on language-only or language-dominant approaches to
benchmark and training models, resulting in systems that are improving in
verbal communication but struggle with nonverbal social understanding. To
address this limitation, we tap into a novel data source rich in nonverbal
social interactions -- mime videos. Mimes refer to the art of expression
through gesture and movement without spoken words, which presents unique
challenges and opportunities in interpreting nonverbal social communication. We
contribute a new dataset called MimeQA, obtained by sourcing 8 hours of videos
clips from YouTube and developing a comprehensive video question-answering
benchmark comprising 806 carefully annotated and verified question-answer
pairs, designed to probe nonverbal social reasoning capabilities. Using MimeQA,
we evaluate state-of-the-art video large language models (vLLMs) and find that
they achieve low overall accuracy, ranging from 20-30%, while humans score 86%.
Our analysis reveals that vLLMs often fail to ground imagined objects and
over-rely on the text prompt while ignoring subtle nonverbal interactions. We
hope to inspire future work in AI models that embody true social intelligence
capable of interpreting non-verbal human interactions.


## AI 摘要

随着AI与日常生活的深度融合，具备社交智能的AI系统日益重要。然而当前AI社交推理研究主要依赖纯语言或语言主导的方法，导致系统虽提升了语言交流能力，却在非语言社交理解方面表现欠佳。为此，研究者开发了基于哑剧视频（无语言表达）的新数据集MimeQA，包含8小时视频片段和806个精心标注的问答对，用于测试非语言社交推理能力。实验显示，当前视频大语言模型(vLLMs)准确率仅20-30%，远低于人类的86%。分析表明，vLLMs常无法理解虚拟物体，过度依赖文本提示而忽略细微的非语言互动。该研究旨在推动真正具备非语言社交理解能力的AI发展。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-10T01:29:55Z
- **目录日期**: 2025-06-10
