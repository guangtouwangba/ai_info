# HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization

**URL**: http://arxiv.org/abs/2506.07972v1

## 原始摘要

While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.


## AI 摘要

当前大语言模型（LLM）在推理和问题解决方面取得进展，但现有评估方法存在不足：封闭式问题易饱和和记忆化，主观比较缺乏一致性。为此，研究者提出HeuriGym框架，通过组合优化问题（目标明确、解空间大）评估LLM生成的启发式算法。该框架让LLM提出启发式策略，通过代码执行反馈迭代优化。测试9个前沿模型在计算机系统、物流、生物等领域的9个问题，发现其在工具使用、规划和自适应推理上的局限。提出质量-产出指数（QYI）量化性能，顶尖模型得分仅0.6，远低于专家基线1.0。开源基准旨在推动LLM在科学和工程领域的实用问题解决能力。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-10T23:01:51Z
- **目录日期**: 2025-06-10
