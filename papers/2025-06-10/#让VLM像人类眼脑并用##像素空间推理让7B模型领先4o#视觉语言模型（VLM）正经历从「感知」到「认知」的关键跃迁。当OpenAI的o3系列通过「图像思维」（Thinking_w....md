# #让VLM像人类眼脑并用##像素空间推理让7B模型领先4o#视觉语言模型（VLM）正经历从「感知」到「认知」的关键跃迁。当OpenAI的o3系列通过「图像思维」（Thinking w...

**URL**: https://weibo.com/6105753431/PvRbYxBJS

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%AE%A9VLM%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%9C%BC%E8%84%91%E5%B9%B6%E7%94%A8%23&amp;extparam=%23%E8%AE%A9VLM%E5%83%8F%E4%BA%BA%E7%B1%BB%E7%9C%BC%E8%84%91%E5%B9%B6%E7%94%A8%23" data-hide=""><span class="surl-text">#让VLM像人类眼脑并用#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%83%8F%E7%B4%A0%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86%E8%AE%A97B%E6%A8%A1%E5%9E%8B%E9%A2%86%E5%85%884o%23&amp;extparam=%23%E5%83%8F%E7%B4%A0%E7%A9%BA%E9%97%B4%E6%8E%A8%E7%90%86%E8%AE%A97B%E6%A8%A1%E5%9E%8B%E9%A2%86%E5%85%884o%23" data-hide=""><span class="surl-text">#像素空间推理让7B模型领先4o#</span></a><br><br>视觉语言模型（VLM）正经历从「感知」到「认知」的关键跃迁。<br><br>当OpenAI的o3系列通过「图像思维」（Thinking with Images）让模型学会缩放、标记视觉区域时，我们看到了多模态交互的全新可能。<br><br>然而，当前主流VLM仍被困在「文本茧房」中——依赖文本token间接翻译视觉信息，在高清图像中的微小物体、视频里的动态细节等场景中，常常因缺乏直接视觉操作能力而「视而不见」。<br><br>来自滑铁卢大学、港科大、中科大的研究团队，首次将推理战场从文本空间拓展到像素空间，提出「像素空间推理」（Pixel-Space Reasoning）范式。<br><br>这项突破让VLM能像人类一样「眼脑并用」：通过原生视觉操作直接与视觉信息对话，在像素级精度上解锁视觉理解的新维度。<br><br>更多细节，欢迎查看：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FwjW2UIi6x4fNU0sqSRzxzQ" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">首创像素空间推理，7B模型领先GPT-4o，让VLM能像人类一样「眼脑并用」</span></a><br><br>论文地址：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fpdf%2F2505.15966" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>项目主页：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Ftiger-ai-lab.github.io%2FPixel-Reasoner%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>模型试玩：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fhuggingface.co%2Fspaces%2FTIGER-Lab%2FPixel-Reasoner" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i2a01k91f7j314u0k0wer.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i2a017jmusj30zk0cl787.jpg" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax3.sinaimg.cn/orj480/006Fd7o3ly1i2a01jvs6rj314u0k0wer.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/0PMOySRglx08oVuxPHdC010412007nab0E010.mp4?label=mp4_720p&amp;template=1470x720.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1749532116&amp;ssig=dUm0QpX2IF&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/yp7MFihclx08oVuxzJO0010412003kCT0E010.mp4?label=mp4_hd&amp;template=980x480.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1749532116&amp;ssig=8oDRDHhSA1&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/KkgH4MN2lx08oVuxvia4010412001QCW0E010.mp4?label=mp4_ld&amp;template=732x360.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1749532116&amp;ssig=%2FjdFkMRIw0&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5175926229762066" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

滑铁卢大学、港科大和中科大的研究团队提出"像素空间推理"新范式，突破当前视觉语言模型(VLM)依赖文本token的局限，使模型能直接在像素层面处理视觉信息。这项技术让7B参数模型在视觉理解上超越GPT-4o，实现类似人类"眼脑并用"的认知方式，可精准识别高清图像中的微小物体和视频动态细节。研究标志着VLM从"感知"到"认知"的关键跃迁，为多模态交互开辟新方向。论文、项目主页和演示模型已公开。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-10T04:09:07Z
- **目录日期**: 2025-06-10
