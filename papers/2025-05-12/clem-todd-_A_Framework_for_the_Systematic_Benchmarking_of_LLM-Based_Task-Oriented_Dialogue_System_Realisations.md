# clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations

**URL**: http://arxiv.org/abs/2505.05445v1

## 原始摘要

The emergence of instruction-tuned large language models (LLMs) has advanced
the field of dialogue systems, enabling both realistic user simulations and
robust multi-turn conversational agents. However, existing research often
evaluates these components in isolation-either focusing on a single user
simulator or a specific system design-limiting the generalisability of insights
across architectures and configurations. In this work, we propose clem todd
(chat-optimized LLMs for task-oriented dialogue systems development), a
flexible framework for systematically evaluating dialogue systems under
consistent conditions. clem todd enables detailed benchmarking across
combinations of user simulators and dialogue systems, whether existing models
from literature or newly developed ones. It supports plug-and-play integration
and ensures uniform datasets, evaluation metrics, and computational
constraints. We showcase clem todd's flexibility by re-evaluating existing
task-oriented dialogue systems within this unified setup and integrating three
newly proposed dialogue systems into the same evaluation pipeline. Our results
provide actionable insights into how architecture, scale, and prompting
strategies affect dialogue performance, offering practical guidance for
building efficient and effective conversational AI systems.


## AI 摘要

本文介绍了一个名为clem todd（面向任务型对话系统开发的优化LLM框架）的灵活评估框架，旨在统一评估对话系统及其用户模拟器。现有研究通常孤立评估单一组件，限制了跨架构的通用性。clem todd支持即插即用的集成，确保数据集、指标和计算条件一致。通过该框架，作者重新评估了现有系统，并整合了三种新系统，比较了架构、规模和提示策略对对话性能的影响，为构建高效对话AI提供了实用指导。该框架有助于在统一条件下系统化分析不同组合的表现差异。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-12T01:29:21Z
- **目录日期**: 2025-05-12
