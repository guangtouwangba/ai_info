# Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users

**URL**: http://arxiv.org/abs/2503.22610v1

## 原始摘要

This paper explores the effectiveness of Multimodal Large Language models
(MLLMs) as assistive technologies for visually impaired individuals. We conduct
a user survey to identify adoption patterns and key challenges users face with
such technologies. Despite a high adoption rate of these models, our findings
highlight concerns related to contextual understanding, cultural sensitivity,
and complex scene understanding, particularly for individuals who may rely
solely on them for visual interpretation. Informed by these results, we collate
five user-centred tasks with image and video inputs, including a novel task on
Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals
that further advancements are necessary to overcome limitations related to
cultural context, multilingual support, Braille reading comprehension,
assistive object recognition, and hallucinations. This work provides critical
insights into the future direction of multimodal AI for accessibility,
underscoring the need for more inclusive, robust, and trustworthy visual
assistance technologies.


## AI 摘要

本文研究了多模态大语言模型(MLLMs)作为视障人士辅助技术的有效性。通过用户调查发现，尽管采用率高，但存在情境理解、文化敏感性和复杂场景理解等挑战。研究提出了5项以用户为中心的图像视频任务，包括新型光学盲文识别任务。对12个MLLMs的系统评估显示，这些模型在文化背景、多语言支持、盲文阅读理解、辅助物体识别和幻觉等方面仍需改进。研究为无障碍多模态AI的未来发展提供了关键见解，强调了开发更具包容性、鲁棒性和可信赖的视觉辅助技术的必要性。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-31T22:01:45Z
- **目录日期**: 2025-03-31
