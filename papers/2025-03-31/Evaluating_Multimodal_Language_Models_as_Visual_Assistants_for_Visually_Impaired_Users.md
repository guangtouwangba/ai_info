# Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users

**URL**: http://arxiv.org/abs/2503.22610v1

## 原始摘要

This paper explores the effectiveness of Multimodal Large Language models
(MLLMs) as assistive technologies for visually impaired individuals. We conduct
a user survey to identify adoption patterns and key challenges users face with
such technologies. Despite a high adoption rate of these models, our findings
highlight concerns related to contextual understanding, cultural sensitivity,
and complex scene understanding, particularly for individuals who may rely
solely on them for visual interpretation. Informed by these results, we collate
five user-centred tasks with image and video inputs, including a novel task on
Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals
that further advancements are necessary to overcome limitations related to
cultural context, multilingual support, Braille reading comprehension,
assistive object recognition, and hallucinations. This work provides critical
insights into the future direction of multimodal AI for accessibility,
underscoring the need for more inclusive, robust, and trustworthy visual
assistance technologies.


## AI 摘要

本研究调查了多模态大语言模型(MLLMs)作为视障辅助技术的效果。用户调查显示，尽管采用率高，但仍存在情境理解、文化敏感性和复杂场景识别等挑战，特别是对依赖视觉解释的用户。研究提出了五项以用户为中心的图像/视频任务，包括创新的光学盲文识别。对12个MLLMs的系统评估表明，这些模型在文化背景、多语言支持、盲文理解、辅助物品识别和幻觉问题等方面仍需改进。该研究为无障碍多模态AI的未来发展提供了关键见解，强调需要更具包容性、稳健性和可信度的视觉辅助技术。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-31T12:01:48Z
- **目录日期**: 2025-03-31
