# Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users

**URL**: http://arxiv.org/abs/2503.22610v1

## 原始摘要

This paper explores the effectiveness of Multimodal Large Language models
(MLLMs) as assistive technologies for visually impaired individuals. We conduct
a user survey to identify adoption patterns and key challenges users face with
such technologies. Despite a high adoption rate of these models, our findings
highlight concerns related to contextual understanding, cultural sensitivity,
and complex scene understanding, particularly for individuals who may rely
solely on them for visual interpretation. Informed by these results, we collate
five user-centred tasks with image and video inputs, including a novel task on
Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals
that further advancements are necessary to overcome limitations related to
cultural context, multilingual support, Braille reading comprehension,
assistive object recognition, and hallucinations. This work provides critical
insights into the future direction of multimodal AI for accessibility,
underscoring the need for more inclusive, robust, and trustworthy visual
assistance technologies.


## AI 摘要

该研究探讨了多模态大语言模型(MLLMs)作为视障人士辅助技术的有效性。通过用户调查发现，尽管采用率较高，但模型在情境理解、文化敏感性和复杂场景理解方面存在不足，特别是对依赖其进行视觉解释的用户。研究提出了五项以用户为中心的图像/视频任务(包括新型光学盲文识别)，并系统评估了12个MLLMs，发现其在文化背景、多语言支持、盲文理解、辅助物体识别和幻觉问题等方面仍需改进。研究强调需要开发更具包容性、鲁棒性和可信赖的视觉辅助技术，为无障碍AI发展提供了重要方向。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-31T18:01:41Z
- **目录日期**: 2025-03-31
