# Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users

**URL**: http://arxiv.org/abs/2503.22610v1

## 原始摘要

This paper explores the effectiveness of Multimodal Large Language models
(MLLMs) as assistive technologies for visually impaired individuals. We conduct
a user survey to identify adoption patterns and key challenges users face with
such technologies. Despite a high adoption rate of these models, our findings
highlight concerns related to contextual understanding, cultural sensitivity,
and complex scene understanding, particularly for individuals who may rely
solely on them for visual interpretation. Informed by these results, we collate
five user-centred tasks with image and video inputs, including a novel task on
Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals
that further advancements are necessary to overcome limitations related to
cultural context, multilingual support, Braille reading comprehension,
assistive object recognition, and hallucinations. This work provides critical
insights into the future direction of multimodal AI for accessibility,
underscoring the need for more inclusive, robust, and trustworthy visual
assistance technologies.


## AI 摘要

该研究探讨了多模态大语言模型(MLLMs)作为视障人士辅助技术的效果。用户调查显示，尽管采用率较高，但存在语境理解、文化敏感性和复杂场景理解等关键问题。研究人员设计了5项以用户为中心的图像/视频任务(包括创新的光学盲文识别)，系统评估12个MLLM后发现：这些模型在文化语境、多语言支持、盲文识别、辅助物品认知和幻觉问题等方面仍需改进。研究为无障碍多模态AI的发展方向提供了重要见解，强调需要开发更具包容性、鲁棒性和可信赖的视觉辅助技术。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-31T21:01:31Z
- **目录日期**: 2025-03-31
