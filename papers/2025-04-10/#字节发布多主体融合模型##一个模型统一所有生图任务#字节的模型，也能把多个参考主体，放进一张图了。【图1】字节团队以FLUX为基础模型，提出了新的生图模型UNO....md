# #字节发布多主体融合模型##一个模型统一所有生图任务#字节的模型，也能把多个参考主体，放进一张图了。【图1】字节团队以FLUX为基础模型，提出了新的生图模型UNO...

**URL**: https://weibo.com/6105753431/PmCMWnzMx

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%AD%97%E8%8A%82%E5%8F%91%E5%B8%83%E5%A4%9A%E4%B8%BB%E4%BD%93%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23%E5%AD%97%E8%8A%82%E5%8F%91%E5%B8%83%E5%A4%9A%E4%B8%BB%E4%BD%93%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#字节发布多主体融合模型#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%BB%9F%E4%B8%80%E6%89%80%E6%9C%89%E7%94%9F%E5%9B%BE%E4%BB%BB%E5%8A%A1%23&amp;extparam=%23%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9E%8B%E7%BB%9F%E4%B8%80%E6%89%80%E6%9C%89%E7%94%9F%E5%9B%BE%E4%BB%BB%E5%8A%A1%23" data-hide=""><span class="surl-text">#一个模型统一所有生图任务#</span></a><br><br>字节的模型，也能把多个参考主体，放进一张图了。【图1】<br><br>字节团队以FLUX为基础模型，提出了新的生图模型UNO，统一了图像生成任务中不同输入条件的处理。<br><br>无论是单主体进行风格变换，还是不同物体的融合，UNO都能直接搞定。【图2】<br><br>除了直接的文生图之外，它可以把多张参考图当中的物体进行组合。【图3】<br><br>当然三个物体也照样能很好地组合，官方提供的在线Demo当中最多可以上传四张参考图。【图4】<br><br>也可以对参考主体中的人物特征进行保持，生成不同场景的人物图像。【图5】<br><br>同时对于人物而言，也可以在保留基本特征的条件下进行风格转换，包括被GPT-4o带火的吉卜力风也能拿捏。【图6】<br><br>应用场景方面，官方给出了虚拟试穿和产品设计这两组示例。【图7】<br><br>技术方面，UNO采用了这一种“模型-数据共同进化”的新范式，核心思想是用较弱的模型生成训练数据，训练更强的模型。<br><br>在模型架构方面，UNO以开源模型FLUX.1 dev为基础，继承了其文生图基础能力和多模态注意力机制，采用了通用定制化模型框架。<br><br>具体来说，该框架采用渐进式跨模态对齐策略，将训练过程分为两个连续阶段——<br><br>- 首先使用单主体数据对预训练的文生图（T2I）模型进行微调，使其获得基本的主体到图像转换（S2I）能力；<br><br>- 随后引入多主体数据继续训练，增强模型处理复杂场景的能力。【图8】<br><br>此外研究团队提出了通用旋转位置嵌入（UnoPE）技术，通过为文本和图像标记分配特定的位置索引，来调控多模态标记之间的交互。<br><br>UnoPE采用从噪声图像标记最大维度开始的对角线位置编码方式，并通过调整位置索引范围来防止生成图像过度依赖参考图像的空间结构，有效缓解了在扩展视觉主体控制时容易出现的属性混淆问题。【图9】<br><br>论文地址：  <a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fabs%2F2504.02160" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a>  <br>项目主页：  <a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fbytedance.github.io%2FUNO%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>原文：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2Fh4fkShoOG_Fi66fksXaJGw" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0bt085vt4j30zk0dx48j.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i0bt08edv2j30zk0g4n62.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0bt07xp8vj30mm0k0ncs.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0bt08encoj30zk0ghgqr.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0bt077vd9j30zk08eadh.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i0bt0886akj30pj0k0129.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i0bt08fz2bj30zk0glqas.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i0bt08h4bpj30zk0hnq9s.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i0bt0a5skzj30pa0hc468.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

字节跳动团队基于FLUX模型提出新型图像生成模型UNO，可统一处理多种生图任务。该模型支持多主体融合（最多4张参考图组合）、风格转换（包括吉卜力风格）、人物特征保持等功能，适用于虚拟试穿和产品设计等场景。技术亮点包括：1）采用"模型-数据共同进化"训练范式；2）两阶段渐进式训练（单主体微调→多主体增强）；3）创新UnoPE位置编码技术防止属性混淆。UNO继承了FLUX的多模态注意力机制，通过跨模态对齐实现复杂场景生成能力。论文和Demo已公开。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-10T20:03:24Z
- **目录日期**: 2025-04-10
