# #Kimi发布开源视觉推理模型##16B多模态模型媲美旗舰模型#沉寂了一个月，Kimi憋了个大的——开源轻量级视觉语言模型Kimi-VL及其推理版Kimi-VL-Thinking，多模态和...

**URL**: https://weibo.com/6105753431/PmCYw6vPs

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23Kimi%E5%8F%91%E5%B8%83%E5%BC%80%E6%BA%90%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23Kimi%E5%8F%91%E5%B8%83%E5%BC%80%E6%BA%90%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#Kimi发布开源视觉推理模型#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%2316B%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AA%B2%E7%BE%8E%E6%97%97%E8%88%B0%E6%A8%A1%E5%9E%8B%23&amp;extparam=%2316B%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AA%B2%E7%BE%8E%E6%97%97%E8%88%B0%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#16B多模态模型媲美旗舰模型#</span></a><br><br>沉寂了一个月，Kimi憋了个大的——<br><br>开源轻量级视觉语言模型Kimi-VL及其推理版Kimi-VL-Thinking，多模态和推理双双拿捏。【图1】<br><br>按照Kimi官方的说法，其关键亮点如下：<br><br>- 都是基于MoE架构，总参数为16B，但推理时仅激活2.8B；<br>- 具备强大的多模态推理能力（媲美参数大10倍的模型）和Agent能力；<br>- 支持128K上下文窗口；<br>- 采用相对较为宽松的MIT许可证。<br><br>话不多说，我们直接看Kimi新模型的具体玩法和效果。<br><br>首先，作为一款通用的VLM模型，Kimi-VL具备强大的视觉理解和推理能力。<br><br>给它一份手稿，要求它通过逐步推理来确认手稿属于谁，以及所记录的内容。<br><br>可以看到，Kimi-VL通过分析手稿的笔迹、内容、语言等特征，推断出手稿可能属于爱因斯坦，理由是这些内容与引力场方程有关，这与爱因斯坦对广义相对论的贡献有关。【图2】<br><br>又或者只提供一张图片，让Kimi-VL来判断城市地标建筑、识别游戏场景等。<br><br>比如第2个例子中，它成功识别出图片中的穹顶建筑为多伦多的罗杰斯中心（Rogers Centre），同时描述了其特征和用途。【图3】<br><br>除此之外，Kimi-VL也能被用来解答高难度几何数学题。<br><br>还是仅需一个上传图片的动作，它就能将复杂数学公式转换为LaTeX代码，并以正确格式输出。【图4】<br><br>当然，Kimi-VL对多模态数据的正确理解还离不开一项关键能力——OCR字符识别。<br><br>在OCRBench基准测试中，其得分为867，属于SOTA水平。<br><br>除了识别数学公式，它还能识别金融表格（以Markdown表格格式输出）和手写作文。【图5】<br><br>甚至还能从长达一小时的视频课程中捕捉和理解关键细节。【图6】<br><br>比如提供视频中的某句话“授人以鱼不如授人以渔”，要求它找到出处并进一步解读。<br><br>那么接下来的问题是，怎么做到的？<br><br>来看Kimi此次公开的技术报告。【图7】<br><br>首先，在模型架构上，Kimi-VL和Kimi-VL-Thinking主要由三大部分构成：【图8】<br><br>- MoE专家混合语言模型（之前发布的Moonlight-16B-A3B）*；<br>- 原生分辨率视觉编码器（MoonViT，基于SigLIP-SO-400M微调）；<br>- 一个多层感知机（MLP）投影器。<br><br>训练环节，团队构建了三大类别数据集：<br><br>1、预训练数据。精选来自六个类别的高质量数据，包括字幕数据、图像文本交织数据、OCR数据、知识数据、视频数据和智能体数据。通过过滤、合成和去重等操作，控制数据质量。<br><br>2、指令数据。用于增强模型的对话和指令遵循能力。对于非推理任务，通过人工标注构建种子数据集，训练种子模型后生成并筛选多轮响应；对于推理任务，利用拒绝采样的方式扩展数据集，确保数据多样性和准确性。<br><br>3、推理数据。通过类似拒绝采样和提示工程的方法，收集和合成高质量的长思维链数据。<br><br>然后开始预训练，这一阶段共消耗4.4T tokens，主要目标是提高模型的多模态理解能力。<br><br>概括而言，这一过程包含4个步骤：先独立进行ViT训练，以建立原生分辨率视觉编码器；随后进行三个联合训练阶段（预训练、冷却、长上下文激活）。<br><br>接着进行后训练，通过在32K和128K上下文中进行的两个阶段的联合监督微调、长思维链监督微调及强化学习，团队进一步提升了模型的长期思考能力。【图9】<br><br>更多细节感兴趣可以查阅原论文：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fgithub.com%2FMoonshotAI%2FKimi-VL%2Fblob%2Fmain%2FKimi-VL.pdf" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>模型开源地址：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fhuggingface.co%2Fcollections%2Fmoonshotai%2Fkimi-vl-a3b-67f67b6ac91d3b03d382dd85" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0btu2sewdj30n60zke13.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0btu3rwrzj30zk0kenfs.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i0btu4vo6pj30zk0u6ngv.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0btu6705oj30zk0mcae8.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i0btu87aa0j30zk0n8akk.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0btu9b1rmj30zk0rv4bf.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i0btuajvsdj30zk0sg15k.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0btubnrgbj30zk0mudtp.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0btucpn9gj31dk0rodwc.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

Kimi近日开源了轻量级视觉语言模型Kimi-VL及其推理版Kimi-VL-Thinking，采用MoE架构（总参数16B，推理时仅激活2.8B），具备媲美更大模型的多模态推理能力。该模型支持128K上下文窗口，在OCR识别（如手稿分析、数学公式转换）、图像理解（地标识别）和视频关键信息提取等任务中表现优异。技术核心包括MoE语言模型、原生分辨率视觉编码器和MLP投影器，通过多阶段训练（预训练4.4T tokens、联合微调）优化性能。模型采用MIT许可证开源，相关资源已发布在GitHub和Hugging Face平台。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-10T21:03:18Z
- **目录日期**: 2025-04-10
