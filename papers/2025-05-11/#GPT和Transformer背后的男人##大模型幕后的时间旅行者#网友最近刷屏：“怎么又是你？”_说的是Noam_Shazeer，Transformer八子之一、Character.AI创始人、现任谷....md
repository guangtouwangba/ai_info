# #GPT和Transformer背后的男人##大模型幕后的时间旅行者#网友最近刷屏：“怎么又是你？” 说的是Noam Shazeer，Transformer八子之一、Character.AI创始人、现任谷...

**URL**: https://weibo.com/6105753431/Pr9WeEUWW

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23GPT%E5%92%8CTransformer%E8%83%8C%E5%90%8E%E7%9A%84%E7%94%B7%E4%BA%BA%23&amp;extparam=%23GPT%E5%92%8CTransformer%E8%83%8C%E5%90%8E%E7%9A%84%E7%94%B7%E4%BA%BA%23" data-hide=""><span class="surl-text">#GPT和Transformer背后的男人#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%95%E5%90%8E%E7%9A%84%E6%97%B6%E9%97%B4%E6%97%85%E8%A1%8C%E8%80%85%23&amp;extparam=%23%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B9%95%E5%90%8E%E7%9A%84%E6%97%B6%E9%97%B4%E6%97%85%E8%A1%8C%E8%80%85%23" data-hide=""><span class="surl-text">#大模型幕后的时间旅行者#</span></a><br><br>网友最近刷屏：“怎么又是你？”  <br><br>说的是Noam Shazeer，Transformer八子之一、Character.AI创始人、现任谷歌Gemini技术主管——技术圈公认的大模型「时间旅行者」。<br><br>朱泽园最近发的《Physics of Language Models》项目里提到的3-token因果卷积，网友一扒，沙哥三年前就做过类似研究了。<br><br>这还不是第一次。<br><br>大语言模型的发展史里，沙哥的名字简直像“常驻NPC”：<br><br>- Transformer的成名作《Attention is All You Need》他是第八位作者，被公认贡献最大<br>    <br>- MoE架构的早期推进者，从提出Sparsely-Gated MoE、到GShard、Switch Transformer、再到ST-MoE，都有他一手参与<br>    <br>- 为了解决大模型训练和推理的现实问题，他还提出了Adafactor优化器、Multi Query Attention、Gated Linear Unit等方案<br>    <br>几乎每个关键节点，沙哥都站在前面。<br><br>他的“预判”，不是靠运气。  <br><br>从3岁自学算术、IMO竞赛满分、Putnam数学竞赛Top 10，到加入谷歌成为第200号员工，一路做到广告、搜索、AI系统的骨干。<br><br>2017年加入Transformer团队前，他已经是谷歌内部的技术大佬；创业Character.AI后又被谷歌27亿美元“买回”，继续担任Gemini联合负责人。<br><br>他曾写信说：语言模型将主导全球算力。  <br>如今的现实，正像他预言的那样在发生。<br><br>有网友说：“我们其实是生活在Noam Shazeer时代。”  <br>这句话现在听起来，越来越像实话了。<br><br>详情阅读：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FUSqd0EDbJtbU87hV7jdV5Q" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">一个「always」站在大模型技术C位的传奇男子</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1aa735orij30v00zknmy.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1aa74lik2j30zk0qttov.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1aa6y0d4ej30wo0di477.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1aa78sz3ej30u10idjux.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1aa7olbazj30to0n6qfx.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1aa81x38fj30un0dlad1.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

Noam Shazeer是Transformer架构核心贡献者、谷歌Gemini技术主管，被誉为大模型领域的"时间旅行者"。作为Transformer论文第八作者，他深度参与了MoE架构演进，提出Adafactor优化器等关键技术。从IMO满分到谷歌第200号员工，再到创立Character.AI后被谷歌27亿美元收购，他始终处于AI技术前沿。其"语言模型将主导全球算力"的预言正在应验，网友称"我们生活在Noam Shazeer时代"。他持续引领大模型发展的关键节点，展现了非凡的技术预见力和执行力。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-11T06:03:29Z
- **目录日期**: 2025-05-11
