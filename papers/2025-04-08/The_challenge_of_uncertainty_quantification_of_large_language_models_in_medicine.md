# The challenge of uncertainty quantification of large language models in medicine

**URL**: http://arxiv.org/abs/2504.05278v1

## 原始摘要

This study investigates uncertainty quantification in large language models
(LLMs) for medical applications, emphasizing both technical innovations and
philosophical implications. As LLMs become integral to clinical
decision-making, accurately communicating uncertainty is crucial for ensuring
reliable, safe, and ethical AI-assisted healthcare. Our research frames
uncertainty not as a barrier but as an essential part of knowledge that invites
a dynamic and reflective approach to AI design. By integrating advanced
probabilistic methods such as Bayesian inference, deep ensembles, and Monte
Carlo dropout with linguistic analysis that computes predictive and semantic
entropy, we propose a comprehensive framework that manages both epistemic and
aleatoric uncertainties. The framework incorporates surrogate modeling to
address limitations of proprietary APIs, multi-source data integration for
better context, and dynamic calibration via continual and meta-learning.
Explainability is embedded through uncertainty maps and confidence metrics to
support user trust and clinical interpretability. Our approach supports
transparent and ethical decision-making aligned with Responsible and Reflective
AI principles. Philosophically, we advocate accepting controlled ambiguity
instead of striving for absolute predictability, recognizing the inherent
provisionality of medical knowledge.


## AI 摘要

该研究探讨了大型语言模型(LLMs)在医疗应用中的不确定性量化问题，结合技术创新与哲学思考。随着LLMs参与临床决策，准确传达不确定性对确保AI辅助医疗的可靠性、安全性和伦理性至关重要。研究提出将不确定性视为知识的必要组成部分，而非障碍，并开发了一个综合框架：整合贝叶斯推理、深度集成等概率方法与语言分析，管理认知和随机不确定性；采用代理模型解决API限制，多源数据整合增强上下文理解，持续学习实现动态校准；通过不确定性图谱和置信度指标提升可解释性。研究主张接受可控模糊性，而非追求绝对确定性，体现了医疗知识的固有临时性特性。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-08T21:02:09Z
- **目录日期**: 2025-04-08
