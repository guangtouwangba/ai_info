# How to evaluate control measures for LLM agents? A trajectory from today to superintelligence

**URL**: http://arxiv.org/abs/2504.05259v1

## 原始摘要

As LLM agents grow more capable of causing harm autonomously, AI developers
will rely on increasingly sophisticated control measures to prevent possibly
misaligned agents from causing harm. AI developers could demonstrate that their
control measures are sufficient by running control evaluations: testing
exercises in which a red team produces agents that try to subvert control
measures. To ensure control evaluations accurately capture misalignment risks,
the affordances granted to this red team should be adapted to the capability
profiles of the agents to be deployed under control measures.
  In this paper we propose a systematic framework for adapting affordances of
red teams to advancing AI capabilities. Rather than assuming that agents will
always execute the best attack strategies known to humans, we demonstrate how
knowledge of an agents's actual capability profile can inform proportional
control evaluations, resulting in more practical and cost-effective control
measures. We illustrate our framework by considering a sequence of five
fictional models (M1-M5) with progressively advanced capabilities, defining
five distinct AI control levels (ACLs). For each ACL, we provide example rules
for control evaluation, control measures, and safety cases that could be
appropriate. Finally, we show why constructing a compelling AI control safety
case for superintelligent LLM agents will require research breakthroughs,
highlighting that we might eventually need alternative approaches to mitigating
misalignment risk.


## AI 摘要

随着AI代理能力的提升，其潜在危害风险增加，开发者需采用更复杂的控制措施。本文提出通过"控制评估"（由红队模拟对抗测试）来验证控制措施的有效性，并建立系统框架，根据AI能力动态调整红队的测试权限。研究以5个虚构模型（M1-M5）为例，对应5个AI控制等级（ACLs），为每个等级提供评估规则、控制措施和安全方案。研究指出，针对超级智能LLM代理的安全论证需要技术突破，最终可能需要新的对齐风险缓解方法。框架强调应根据AI实际能力（而非人类已知攻击方式）制定比例化评估，以提高措施的实用性和成本效益。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-08T08:01:43Z
- **目录日期**: 2025-04-08
