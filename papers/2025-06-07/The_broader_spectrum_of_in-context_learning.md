# The broader spectrum of in-context learning

**URL**: http://arxiv.org/abs/2412.03782v3

## 原始摘要

The ability of language models to learn a task from a few examples in context
has generated substantial interest. Here, we provide a perspective that
situates this type of supervised few-shot learning within a much broader
spectrum of meta-learned in-context learning. Indeed, we suggest that any
distribution of sequences in which context non-trivially decreases loss on
subsequent predictions can be interpreted as eliciting a kind of in-context
learning. We suggest that this perspective helps to unify the broad set of
in-context abilities that language models exhibit -- such as adapting to tasks
from instructions or role play, or extrapolating time series. This perspective
also sheds light on potential roots of in-context learning in lower-level
processing of linguistic dependencies (e.g. coreference or parallel
structures). Finally, taking this perspective highlights the importance of
generalization, which we suggest can be studied along several dimensions: not
only the ability to learn something novel, but also flexibility in learning
from different presentations, and in applying what is learned. We discuss
broader connections to past literature in meta-learning and goal-conditioned
agents, and other perspectives on learning and adaptation. We close by
suggesting that research on in-context learning should consider this broader
spectrum of in-context capabilities and types of generalization.


## AI 摘要

本文提出了一种新视角，将语言模型的少样本学习能力视为更广泛的"元学习上下文学习"范畴的一部分。作者认为，任何能通过上下文显著降低后续预测损失的序列分布，都可视为引发某种上下文学习。这一观点有助于统一语言模型展现的各种上下文能力，如根据指令适应任务、角色扮演或时间序列外推等，并揭示了上下文学习可能源于语言依赖关系（如共指或平行结构）的低层次处理。研究强调了泛化能力的重要性，建议从学习新颖内容、适应不同呈现方式及应用所学等多个维度进行研究。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-07T16:02:30Z
- **目录日期**: 2025-06-07
