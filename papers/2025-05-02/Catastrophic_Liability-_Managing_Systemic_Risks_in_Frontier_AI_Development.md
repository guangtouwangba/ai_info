# Catastrophic Liability: Managing Systemic Risks in Frontier AI Development

**URL**: http://arxiv.org/abs/2505.00616v1

## 原始摘要

As artificial intelligence systems grow more capable and autonomous, frontier
AI development poses potential systemic risks that could affect society at a
massive scale. Current practices at many AI labs developing these systems lack
sufficient transparency around safety measures, testing procedures, and
governance structures. This opacity makes it challenging to verify safety
claims or establish appropriate liability when harm occurs. Drawing on
liability frameworks from nuclear energy, aviation software, and healthcare, we
propose a comprehensive approach to safety documentation and accountability in
frontier AI development.


## AI 摘要

随着前沿AI系统能力增强和自主性提高，其发展可能带来大规模社会风险。当前许多AI实验室在安全措施、测试流程和治理结构方面缺乏透明度，导致难以验证安全性或在损害发生时明确责任。借鉴核能、航空软件和医疗领域的责任框架，本文提出了一套全面的安全文档和问责机制方案，以应对前沿AI开发中的系统性风险。该方案旨在通过规范化的安全记录和明确的责任划分，增强AI开发的透明度和可追溯性。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-02T08:01:41Z
- **目录日期**: 2025-05-02
