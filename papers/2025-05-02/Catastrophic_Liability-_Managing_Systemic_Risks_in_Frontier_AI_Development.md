# Catastrophic Liability: Managing Systemic Risks in Frontier AI Development

**URL**: http://arxiv.org/abs/2505.00616v1

## 原始摘要

As artificial intelligence systems grow more capable and autonomous, frontier
AI development poses potential systemic risks that could affect society at a
massive scale. Current practices at many AI labs developing these systems lack
sufficient transparency around safety measures, testing procedures, and
governance structures. This opacity makes it challenging to verify safety
claims or establish appropriate liability when harm occurs. Drawing on
liability frameworks from nuclear energy, aviation software, and healthcare, we
propose a comprehensive approach to safety documentation and accountability in
frontier AI development.


## AI 摘要

随着人工智能系统变得更强大和自主，前沿AI的发展可能带来大规模的社会系统性风险。目前许多AI实验室在开发过程中缺乏足够的安全措施、测试流程和治理结构的透明度，导致难以验证安全声明或明确责任归属。借鉴核能、航空软件和医疗等领域的责任框架，研究者提出了一套针对前沿AI开发的安全文档和问责机制的综合方案，旨在加强AI系统的安全性和责任追溯能力。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-02T16:01:59Z
- **目录日期**: 2025-05-02
