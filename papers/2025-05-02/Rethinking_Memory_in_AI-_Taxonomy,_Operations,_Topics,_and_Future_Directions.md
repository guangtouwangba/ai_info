# Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions

**URL**: http://arxiv.org/abs/2505.00675v1

## 原始摘要

Memory is a fundamental component of AI systems, underpinning large language
models (LLMs) based agents. While prior surveys have focused on memory
applications with LLMs, they often overlook the atomic operations that underlie
memory dynamics. In this survey, we first categorize memory representations
into parametric, contextual structured, and contextual unstructured and then
introduce six fundamental memory operations: Consolidation, Updating, Indexing,
Forgetting, Retrieval, and Compression. We systematically map these operations
to the most relevant research topics across long-term, long-context, parametric
modification, and multi-source memory. By reframing memory systems through the
lens of atomic operations and representation types, this survey provides a
structured and dynamic perspective on research, benchmark datasets, and tools
related to memory in AI, clarifying the functional interplay in LLMs based
agents while outlining promising directions for future research\footnote{The
paper list, datasets, methods and tools are available at
\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.


## AI 摘要

这篇综述探讨了AI系统中的记忆机制，重点关注基于大语言模型（LLM）的智能体。研究首先将记忆表征分为参数化、上下文结构化和上下文非结构化三类，并提出了六种基础记忆操作：巩固、更新、索引、遗忘、检索和压缩。通过将这些操作映射到长期记忆、长上下文、参数修改和多源记忆等研究领域，该研究为AI记忆系统提供了结构化视角，同时梳理了相关研究、基准数据集和工具，阐明了LLM智能体中记忆功能的相互作用，并为未来研究方向提供了建议。相关资源已在GitHub开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-02T21:01:23Z
- **目录日期**: 2025-05-02
