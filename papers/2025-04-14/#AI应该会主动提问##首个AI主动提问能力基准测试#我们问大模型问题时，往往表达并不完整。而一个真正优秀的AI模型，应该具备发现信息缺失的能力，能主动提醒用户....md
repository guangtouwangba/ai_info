# #AI应该会主动提问##首个AI主动提问能力基准测试#我们问大模型问题时，往往表达并不完整。而一个真正优秀的AI模型，应该具备发现信息缺失的能力，能主动提醒用户...

**URL**: https://weibo.com/6105753431/Pncfz38SR

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23AI%E5%BA%94%E8%AF%A5%E4%BC%9A%E4%B8%BB%E5%8A%A8%E6%8F%90%E9%97%AE%23&amp;extparam=%23AI%E5%BA%94%E8%AF%A5%E4%BC%9A%E4%B8%BB%E5%8A%A8%E6%8F%90%E9%97%AE%23" data-hide=""><span class="surl-text">#AI应该会主动提问#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E9%A6%96%E4%B8%AAAI%E4%B8%BB%E5%8A%A8%E6%8F%90%E9%97%AE%E8%83%BD%E5%8A%9B%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%23&amp;extparam=%23%E9%A6%96%E4%B8%AAAI%E4%B8%BB%E5%8A%A8%E6%8F%90%E9%97%AE%E8%83%BD%E5%8A%9B%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%23" data-hide=""><span class="surl-text">#首个AI主动提问能力基准测试#</span></a><br><br>我们问大模型问题时，往往表达并不完整。而一个真正优秀的AI模型，应该具备发现信息缺失的能力，能主动提醒用户：“你是不是还没告诉我某个关键信息？”<br><br>这其实就是“会提问”的能力，在实际应用中类似这样：<br><br>- 面试机器人能否追问候选人模糊的描述；<br>- 法律助手是否能察觉合同条款中遗漏的要素；<br>- 项目协作模型是否能发现需求不全，并提醒用户补充。<br><br>过去的评测基准大多侧重答题能力，却忽视了提问能力。对此，谷歌DeepMind提出了一项新研究——QuestBench，首次量化评估模型在“识别信息缺失并主动提问”方面的表现，填补了这一空白。<br><br>具体来说，研究人员提出了一个新概念：1-sufficient Constraint Satisfaction Problem（CSP），即只要知道一个额外变量的值，问题就能被解决。<br><br>他们构建了一个名为QuestBench的基准测试集，涵盖四类任务：<br><br>1. Logic-Q：命题逻辑类问题，需要判断真假；<br>2. Planning-Q：规划类任务，例如堆叠积木等；<br>3. GSM-Q：小学数学类题目，用自然语言描述；<br>4. GSME-Q：数学等式问题，更偏向结构化表达。<br><br>每道题都故意遗漏了一个关键信息，然后要求模型从多个备选问题中，挑出那个最关键的问题进行补充，选对了，才有可能解出原题。<br><br>因此，模型是否“意识到缺了什么”，就成为评估的重点。<br><br>测试发现，即使是GPT-4o和Claude 3.5 Sonnet，表现也参差不齐：<br><br>- 数学题表现非常好：在GSM-Q和GSME-Q上，准确率能达到 80%~98%，说明这些题型里模型很容易知道缺的是什么；<br>- 逻辑和规划类题差很多：在Logic-Q和Planning-Q上，准确率经常只有40%~50%，很多时候模型根本没发现缺了关键信息；<br>- 明明不知道，也不敢说：在Planning-Q任务中，正确答案是“not sure”选项的情况大概有42%，但模型很少选这个选项，反而会胡乱猜一个答案；<br>- 推理能力 ≠ 提问能力：信息完整的情况下，模型便能把题解出来，但信息缺失时，却提不出正确的问题，这说明提问和解题并不是同一种能力；<br>- 不同任务，策略不同：在Logic-Q中，模型表现和“推理深度”高度相关，表现像是在进行思考；但在Planning-Q中就看不出什么规律，更像在蒙。<br><br>QuestBench告诉我们，真正实用的智能系统，不仅要能解答问题，更要能识别用户表达中的模糊与缺漏，主动提问、引导澄清，才能真正提供帮助。<br><br>论文原文地址：arxiv.org/abs/2503.22674<img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i0g5kn3gt2j30zk0uf1c1.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

谷歌DeepMind最新研究提出首个AI主动提问能力基准测试QuestBench，通过四类任务（逻辑、规划、数学题和数学等式）评估模型识别信息缺失的能力。研究发现，GPT-4o和Claude 3.5在数学类任务表现优异（80%-98%准确率），但在逻辑和规划类任务表现较差（40%-50%），且常回避"不确定"选项。研究指出提问能力与解题能力不同，实用AI系统需主动发现并补充缺失信息。论文地址：arxiv.org/abs/2503.22674

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-14T07:03:21Z
- **目录日期**: 2025-04-14
