# SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents

**URL**: http://arxiv.org/abs/2504.08703v1

## 原始摘要

Coding agents powered by large language models have shown impressive
capabilities in software engineering tasks, but evaluating their performance
across diverse programming languages and real-world scenarios remains
challenging. We introduce SWE-PolyBench, a new multi-language benchmark for
repository-level, execution-based evaluation of coding agents. SWE-PolyBench
contains 2110 instances from 21 repositories and includes tasks in Java (165),
JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes,
feature additions, and code refactoring. We provide a task and
repository-stratified subsample (SWE-PolyBench500) and release an evaluation
harness allowing for fully automated evaluation. To enable a more comprehensive
comparison of coding agents, this work also presents a novel set of metrics
rooted in syntax tree analysis. We evaluate leading open source coding agents
on SWE-PolyBench, revealing their strengths and limitations across languages,
task types, and complexity classes. Our experiments show that current agents
exhibit uneven performances across languages and struggle with complex problems
while showing higher performance on simpler tasks. SWE-PolyBench aims to drive
progress in developing more versatile and robust AI coding assistants for
real-world software engineering. Our datasets and code are available at:
https://github.com/amazon-science/SWE-PolyBench


## AI 摘要

研究人员推出了SWE-PolyBench，一个多语言基准测试工具，用于评估基于大语言模型的编码代理在真实场景中的表现。该工具包含2110个任务实例，涵盖Java、JavaScript、TypeScript和Python四种语言，涉及错误修复、功能添加和代码重构等任务。研究还开发了一套基于语法树分析的新评估指标，并测试了主流开源编码代理的表现。结果显示，当前代理在不同语言和任务复杂度上表现不均，处理复杂问题时仍有困难。该基准旨在推动开发更强大、通用的AI编程助手。数据集和代码已开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-14T23:01:13Z
- **目录日期**: 2025-04-14
