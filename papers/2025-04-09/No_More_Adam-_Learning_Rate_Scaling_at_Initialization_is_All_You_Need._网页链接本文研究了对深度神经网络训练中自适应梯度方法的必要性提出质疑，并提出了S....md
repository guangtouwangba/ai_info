# No More Adam: Learning Rate Scaling at Initialization is All You Need. 网页链接本文研究了对深度神经网络训练中自适应梯度方法的必要性提出质疑，并提出了S...

**URL**: https://weibo.com/1870858943/Pk06MwyNe

## 原始摘要

No More Adam: Learning Rate Scaling at Initialization is All You Need. <a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fwww.aminer.cn%2Fpub%2F6762315eae8580e7ff8ed69e%2Fno-more-adam-learning-rate-scaling-at-initialization-is-all-you-need" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>本文研究了对深度神经网络训练中自适应梯度方法的必要性提出质疑，并提出了SGD-SaI方法。该方法是对带动量的随机梯度下降（SGDM）的简单而有效的改进。SGD-SaI通过对不同参数组在初始化时进行学习率调整 ...<img style="" src="https://tvax4.sinaimg.cn/large/6f830abfly1hzrut5jebij21na0z04qp.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

这篇论文质疑了深度神经网络训练中自适应梯度方法(如Adam)的必要性，提出了一种名为SGD-SaI的改进方法。该方法基于带动量的随机梯度下降(SGDM)，通过在不同参数组初始化时进行学习率调整来优化训练过程。研究表明，这种简单的学习率缩放方法可以替代复杂的自适应优化器，在保持训练效率的同时简化优化流程。SGD-SaI方法展示了在神经网络训练中，适当的学习率初始化策略可能比复杂的自适应算法更为关键和有效。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-09T09:04:40Z
- **目录日期**: 2025-04-09
