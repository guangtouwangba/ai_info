# #14B开源模型比肩o3mini##开源模型DeepCoder超越o1#一个14B的开源模型，超过了OpenAI的o1，打平了o3-mini【图1】！它就是Agentica x Together AI联手打造的DeepC...

**URL**: https://weibo.com/6105753431/PmryiBUZ2

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%2314B%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%82%A9o3mini%23&amp;extparam=%2314B%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%82%A9o3mini%23" data-hide=""><span class="surl-text">#14B开源模型比肩o3mini#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8BDeepCoder%E8%B6%85%E8%B6%8Ao1%23&amp;extparam=%23%E5%BC%80%E6%BA%90%E6%A8%A1%E5%9E%8BDeepCoder%E8%B6%85%E8%B6%8Ao1%23" data-hide=""><span class="surl-text">#开源模型DeepCoder超越o1#</span></a><br><br>一个14B的开源模型，超过了OpenAI的o1，打平了o3-mini【图1】！<br><br>它就是Agentica x Together AI联手打造的DeepCoder-14B-Preview，让我们来速览一下DeepCoder的要点：<br><br>- 训练自Deepseek-R1-Distilled-Qwen-14B，并且通过分布式强化学习进行微调。<br>- 它是一个完全开源的模型，官方把数据集、代码、训练日志和系统优化都全部开放，让大家自由查看。<br>- 在 LiveCodeBench（LCB）上的Pass@ 1准确率达到了60.6%，几乎与o3-mini-2025-01-031的表现持平。<br><br>那么，DeepCoder训练时都用到了哪些“黑科技”呢？<br><br>1、高质量验证数据<br><br>深度学习里，“数据”其实就是养分。在数学习题上，还比较容易得到高质量、可验证的奖励数据，但在代码领域，很多代码题的测试用例不全、或者题目本身就有问题，这都让模型的奖励信号变得“不准”。<br><br>为了给模型提供准确的学习环境，DeepCoder-14B-Preview 筛选了以下数据来源：<br><br>1. TACO Verified：里面的题目都带有完备的测试用例；<br>2. PrimeIntellect’s SYNTHETIC-1：同样有经过官方验证的题目；<br>3. LiveCodeBench（2023/05/01 - 2024/07/31 期间）：包含一批高质量、有完善测试用例的编码任务。<br><br>并且还做了一系列严格的过滤：<br><br>- 自动验证（Programmatic Verification）<br>- 测试用例筛选（至少有5个测试用例才保留）<br>- 数据去重<br><br>最终他们只留下了 2.4 万道优质题，专门用来做强化学习训练。<br><br>2、独特的代码沙盒<br><br>在训练的强化学习环节，需要对模型“写”出来的代码进行单元测试，这一步尤为耗时，因为每道题可能包含好几组测试用例。为此，官方使用了两种沙盒环境：<br><br>- Together Code Interpreter：一个付费但快速的云端沙盒，支持并行跑代码。<br>- 本地沙盒：本地隔离环境，可用官方的LiveCodeBench评测脚本来跑测试，保证评测一致性。<br><br>奖励函数很简单：<br><br>- 1 分：如果模型写的代码通过了抽样到的所有测试；<br>- 0 分：只要有任意一个测试点没通过，或者格式写错，就不给分。<br><br>3、训练策略<br><br>官方采用了GRPO+ 和迭代式上下文扩展的训练策略，具体来说：<br><br>1. No Entropy Loss：去掉了熵损失，避免熵无限增大导致训练崩溃。<br>2. No KL Loss（from DAPO）：不再约束模型回到原先的监督微调分布，也让训练更高效。<br>3. Overlong Filtering：对于超出上下文长度的部分不计入损失，让模型可以“放飞思路”，有助于在 64K 时也能发挥良好。<br>4. Clip High：调高 PPO/GRPO 中的策略更新上限，鼓励多样化探索，稳定训练过程。<br><br>在迭代式上下文扩展方面，他们先以16K的上下文窗口进行训练，后来提升到32K，并在推理时可以扩展到 64K！【图2】<br><br>- 当用64K推理时，模型在LiveCodeBench上能到60.6%（Pass@ 1），而原先在16K可能只有 54%～58%。<br>- “越用越长，越长越强”，模型可以处理更多上下文、写更复杂的代码。<br><br>此外，由于做长上下文的RL训练，每一步都要进行大规模采样，动辄上万乃至几万 token。官方还特意进行了系统层面的加速。<br><br>他们在原先的verl（RLHF 库）基础上做了扩展，名叫verl-pipeline，能让训练流程 加速 2 倍 以上：<br><br>1. One-Off Pipelining，在“采样—计算奖励—训练”这三个环节之间做并行流水线，让采样和训练在不同步的时候也能先后交替进行，节省大量时间。<br>2. 减少熵损失、KL 限制等，减少不必要的计算。<br>3. 奖励并行计算。编码测试环节，必须多线程并发跑测试。<br><br>这样做的结果是，同样8块A100来跑，两周多时间就把DeepCoder-14B-Preview训练完毕，效率惊人。<br><br>此外，官方为了让更多人都能入门“RL 训练大模型”，他们也在此基础上做了1.5B 版本（DeepCoder-1.5B-Preview），帮助开发者轻松上手。<br><br>DeepCoder的出现再次证明，模型大小不是绝对门槛，只要策略和数据到位，14B也能写出堪比更大模型的优雅代码。<br><br>感兴趣的小伙伴可以点击：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fpretty-radio-b75.notion.site%2FDeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i0afeutbddj30zk0otgql.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0afewa3a2j31kw0u3n9r.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

DeepCoder-14B-Preview是由Agentica和Together AI联合开发的开源14B参数代码生成模型，性能媲美OpenAI的o3-mini。该模型基于Deepseek-R1-Distilled-Qwen-14B，通过分布式强化学习微调，在LiveCodeBench上达到60.6%的Pass@1准确率。其核心创新包括：1)严格筛选2.4万道高质量编程题作为训练数据；2)采用代码沙盒进行高效测试；3)运用GRPO+训练策略和迭代式上下文扩展(16K→32K→64K)。通过系统优化verl-pipeline，训练效率提升2倍，8块A100仅需两周完成。项目完全开源，还提供1.5B版本供开发者入门。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-09T08:03:39Z
- **目录日期**: 2025-04-09
