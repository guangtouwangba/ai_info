# Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention 网页链接该论文提出了一种名为NSA的稀疏注意力机制，旨在解决传统注意力...

**URL**: https://weibo.com/1870858943/PjqOTBJ4E

## 原始摘要

Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention <a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fwww.aminer.cn%2Fpub%2F67b3fbf2ae8580e7ff49bfcc%2Fnative-sparse-attention-hardware-aligned-and-natively-trainable-sparse-attention" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>该论文提出了一种名为NSA的稀疏注意力机制，旨在解决传统注意力机制在处理长文本上下文时的高计算成本问题。NSA通过结合算法创新与硬件优化，实现了高效的长文本上下文建模。它采用动态分层 ...<img style="" src="https://tvax3.sinaimg.cn/large/6f830abfly1hznizqy0akj21o00z37wh.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

该论文提出了一种名为NSA（Native Sparse Attention）的新型稀疏注意力机制，旨在降低传统注意力机制处理长文本时的高计算成本。NSA通过动态分层稀疏模式与硬件优化相结合，在保持模型性能的同时显著提升了计算效率。其创新点在于实现了算法与硬件的高度对齐，使稀疏注意力能够原生训练，从而更高效地建模长文本上下文。该方法为处理大规模语言任务提供了更优化的解决方案，平衡了计算资源消耗与模型表现。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-09T09:04:57Z
- **目录日期**: 2025-04-09
