# Modeling Challenging Patient Interactions: LLMs for Medical Communication Training

**URL**: http://arxiv.org/abs/2503.22250v2

## 原始摘要

Effective patient communication is pivotal in healthcare, yet traditional
medical training often lacks exposure to diverse, challenging interpersonal
dynamics. To bridge this gap, this study proposes the use of Large Language
Models (LLMs) to simulate authentic patient communication styles, specifically
the "accuser" and "rationalizer" personas derived from the Satir model, while
also ensuring multilingual applicability to accommodate diverse cultural
contexts and enhance accessibility for medical professionals. Leveraging
advanced prompt engineering, including behavioral prompts, author's notes, and
stubbornness mechanisms, we developed virtual patients (VPs) that embody
nuanced emotional and conversational traits. Medical professionals evaluated
these VPs, rating their authenticity (accuser: $3.8 \pm 1.0$; rationalizer:
$3.7 \pm 0.8$ on a 5-point Likert scale (from one to five)) and correctly
identifying their styles. Emotion analysis revealed distinct profiles: the
accuser exhibited pain, anger, and distress, while the rationalizer displayed
contemplation and calmness, aligning with predefined, detailed patient
description including medical history. Sentiment scores (on a scale from zero
to nine) further validated these differences in the communication styles, with
the accuser adopting negative ($3.1 \pm 0.6$) and the rationalizer more neutral
($4.0 \pm 0.4$) tone. These results underscore LLMs' capability to replicate
complex communication styles, offering transformative potential for medical
education. This approach equips trainees to navigate challenging clinical
scenarios by providing realistic, adaptable patient interactions, enhancing
empathy and diagnostic acumen. Our findings advocate for AI-driven tools as
scalable, cost-effective solutions to cultivate nuanced communication skills,
setting a foundation for future innovations in healthcare training.


## AI 摘要

本研究利用大语言模型(LLMs)开发了基于Satir模型的"指责型"和"合理化型"虚拟患者(VPs)，用于医疗沟通培训。通过高级提示工程(包括行为提示、作者注释和固执机制)，VPs能准确呈现不同沟通风格。医疗专业人员评估显示，两种VPs的真实性评分较高(指责型3.8±1.0；合理化型3.7±0.8，5分制)，且情感特征鲜明：指责型表现痛苦、愤怒(情感得分3.1±0.6)，合理化型呈现沉思、冷静(4.0±0.4)。该研究证实LLMs能有效模拟复杂医患沟通，为培养医务人员共情能力和诊断技巧提供可扩展、经济高效的AI培训方案，具有推动医学教育创新的潜力。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-09T15:02:43Z
- **目录日期**: 2025-04-09
