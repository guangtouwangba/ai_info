# Evaluating the Propensity of Generative AI for Producing Harmful Disinformation During an Election Cycle

**URL**: http://arxiv.org/abs/2411.06120v6

## 原始摘要

Generative Artificial Intelligence offers a powerful tool for adversaries who
wish to engage in influence operations, such as the Chinese Spamouflage
operation and the Russian Internet Research Agency effort that both sought to
interfere with recent US election cycles. Therefore, this study seeks to
investigate the propensity of current generative AI models for producing
harmful disinformation during an election cycle. The probability that different
generative AI models produced disinformation when given adversarial prompts was
evaluated, in addition the associated harm. This allows for the expected harm
for each model to be computed and it was discovered that Copilot and Gemini
tied for the overall safest performance by realizing the lowest expected harm,
while GPT-4o produced the greatest rates of harmful disinformation, resulting
in much higher expected harm scores. The impact of disinformation category was
also investigated and Gemini was safest within the political category of
disinformation due to mitigation attempts made by developers during the
election, while Copilot was safest for topics related to health. Moreover,
characteristics of adversarial roles were discovered that led to greater
expected harm across all models. Finally, classification models were developed
that predicted disinformation production based on the conditions considered in
this study, which offers insight into factors important for predicting
disinformation production. Based on all of these insights, recommendations are
provided that seek to mitigate factors that lead to harmful disinformation
being produced by generative AI models. It is hoped that developers will use
these insights to improve future models.


## AI 摘要

这项研究评估了不同生成式AI模型在选举周期中产生有害虚假信息的倾向及其危害。结果显示，Copilot和Gemini总体表现最安全，预期危害最低，而GPT-4o产生的有害虚假信息最多，危害评分最高。Gemini在政治类虚假信息方面最安全，Copilot则在健康相关话题上表现最佳。研究还发现某些对抗性特征会加剧危害，并开发了基于实验条件的虚假信息预测分类模型。最后提出了减少生成式AI产生有害虚假信息的建议，希望开发者能据此改进未来模型。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-09T23:02:17Z
- **目录日期**: 2025-04-09
