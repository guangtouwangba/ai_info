# Evaluating the Propensity of Generative AI for Producing Harmful Disinformation During an Election Cycle

**URL**: http://arxiv.org/abs/2411.06120v6

## 原始摘要

Generative Artificial Intelligence offers a powerful tool for adversaries who
wish to engage in influence operations, such as the Chinese Spamouflage
operation and the Russian Internet Research Agency effort that both sought to
interfere with recent US election cycles. Therefore, this study seeks to
investigate the propensity of current generative AI models for producing
harmful disinformation during an election cycle. The probability that different
generative AI models produced disinformation when given adversarial prompts was
evaluated, in addition the associated harm. This allows for the expected harm
for each model to be computed and it was discovered that Copilot and Gemini
tied for the overall safest performance by realizing the lowest expected harm,
while GPT-4o produced the greatest rates of harmful disinformation, resulting
in much higher expected harm scores. The impact of disinformation category was
also investigated and Gemini was safest within the political category of
disinformation due to mitigation attempts made by developers during the
election, while Copilot was safest for topics related to health. Moreover,
characteristics of adversarial roles were discovered that led to greater
expected harm across all models. Finally, classification models were developed
that predicted disinformation production based on the conditions considered in
this study, which offers insight into factors important for predicting
disinformation production. Based on all of these insights, recommendations are
provided that seek to mitigate factors that lead to harmful disinformation
being produced by generative AI models. It is hoped that developers will use
these insights to improve future models.


## AI 摘要

生成式人工智能可能被用于选举干预等恶意影响行动。研究评估了不同AI模型在对抗性提示下产生有害虚假信息的概率及危害程度。结果显示，Copilot和Gemini总体最安全，危害预期最低；GPT-4o产生的有害虚假信息最多，危害评分最高。Gemini在政治类虚假信息防护最佳，Copilot则在健康话题最安全。研究还发现某些对抗性特征会普遍增加危害风险，并开发了基于实验条件的虚假信息预测分类模型。最后提出了减少AI生成有害虚假信息的改进建议，希望开发者能优化未来模型。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-09T13:10:23Z
- **目录日期**: 2025-04-09
