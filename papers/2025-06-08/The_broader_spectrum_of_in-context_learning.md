# The broader spectrum of in-context learning

**URL**: http://arxiv.org/abs/2412.03782v3

## 原始摘要

The ability of language models to learn a task from a few examples in context
has generated substantial interest. Here, we provide a perspective that
situates this type of supervised few-shot learning within a much broader
spectrum of meta-learned in-context learning. Indeed, we suggest that any
distribution of sequences in which context non-trivially decreases loss on
subsequent predictions can be interpreted as eliciting a kind of in-context
learning. We suggest that this perspective helps to unify the broad set of
in-context abilities that language models exhibit -- such as adapting to tasks
from instructions or role play, or extrapolating time series. This perspective
also sheds light on potential roots of in-context learning in lower-level
processing of linguistic dependencies (e.g. coreference or parallel
structures). Finally, taking this perspective highlights the importance of
generalization, which we suggest can be studied along several dimensions: not
only the ability to learn something novel, but also flexibility in learning
from different presentations, and in applying what is learned. We discuss
broader connections to past literature in meta-learning and goal-conditioned
agents, and other perspectives on learning and adaptation. We close by
suggesting that research on in-context learning should consider this broader
spectrum of in-context capabilities and types of generalization.


## AI 摘要

这篇论文提出了一种新视角，将语言模型的少样本学习能力置于更广泛的元学习框架下。作者认为，任何能通过上下文显著降低后续预测损失的序列分布，都可视为引发某种上下文学习。这一观点有助于统一语言模型展现的各种上下文能力，如任务适应、角色扮演和时间序列外推等。研究还揭示了上下文学习可能源于语言依赖关系（如共指或平行结构）的低层次处理。作者强调泛化能力的重要性，包括学习新事物、灵活适应不同呈现方式及应用所学知识。最后，建议未来研究应关注更广泛的上下文学习能力和泛化类型。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-08T00:02:31Z
- **目录日期**: 2025-06-08
