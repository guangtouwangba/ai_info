# SpatialScore: Towards Unified Evaluation for Multimodal Spatial Understanding

**URL**: http://arxiv.org/abs/2505.17012v1

## 原始摘要

Multimodal large language models (MLLMs) have achieved impressive success in
question-answering tasks, yet their capabilities for spatial understanding are
less explored. This work investigates a critical question: do existing MLLMs
possess 3D spatial perception and understanding abilities? Concretely, we make
the following contributions in this paper: (i) we introduce VGBench, a
benchmark specifically designed to assess MLLMs for visual geometry perception,
e.g., camera pose and motion estimation; (ii) we propose SpatialScore, the most
comprehensive and diverse multimodal spatial understanding benchmark to date,
integrating VGBench with relevant data from the other 11 existing datasets.
This benchmark comprises 28K samples across various spatial understanding
tasks, modalities, and QA formats, along with a carefully curated challenging
subset, SpatialScore-Hard; (iii) we develop SpatialAgent, a novel multi-agent
system incorporating 9 specialized tools for spatial understanding, supporting
both Plan-Execute and ReAct reasoning paradigms; (iv) we conduct extensive
evaluations to reveal persistent challenges in spatial reasoning while
demonstrating the effectiveness of SpatialAgent. We believe SpatialScore will
offer valuable insights and serve as a rigorous benchmark for the next
evolution of MLLMs.


## AI 摘要

这篇论文研究了多模态大语言模型(MLLM)在3D空间感知和理解方面的能力，提出了VGBench基准来评估视觉几何感知(如相机位姿和运动估计)，并整合11个现有数据集创建了迄今为止最全面的多模态空间理解基准SpatialScore(包含28K样本)。研究还开发了支持多种推理范式的多智能体系统SpatialAgent(包含9种专用工具)。通过广泛评估，研究揭示了MLLM在空间推理方面的持续挑战，同时证明了SpatialAgent的有效性。该基准将为MLLM的下一代发展提供有价值的见解和严格标准。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-25T15:01:23Z
- **目录日期**: 2025-05-25
