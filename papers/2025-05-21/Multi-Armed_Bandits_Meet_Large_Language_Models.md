# Multi-Armed Bandits Meet Large Language Models

**URL**: http://arxiv.org/abs/2505.13355v1

## 原始摘要

Bandit algorithms and Large Language Models (LLMs) have emerged as powerful
tools in artificial intelligence, each addressing distinct yet complementary
challenges in decision-making and natural language processing. This survey
explores the synergistic potential between these two fields, highlighting how
bandit algorithms can enhance the performance of LLMs and how LLMs, in turn,
can provide novel insights for improving bandit-based decision-making. We first
examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt
engineering, and adaptive response generation, focusing on their ability to
balance exploration and exploitation in large-scale learning tasks.
Subsequently, we explore how LLMs can augment bandit algorithms through
advanced contextual understanding, dynamic adaptation, and improved policy
selection using natural language reasoning. By providing a comprehensive review
of existing research and identifying key challenges and opportunities, this
survey aims to bridge the gap between bandit algorithms and LLMs, paving the
way for innovative applications and interdisciplinary research in AI.


## AI 摘要

这篇综述探讨了Bandit算法与大型语言模型(LLMs)的协同潜力。Bandit算法能优化LLMs的微调、提示工程和自适应响应生成，通过平衡探索与利用提升大规模学习任务表现；而LLMs则可为Bandit算法提供增强的上下文理解、动态适应能力，并通过自然语言推理改进策略选择。研究分析了现有成果，指出关键挑战与机遇，旨在搭建两类方法的桥梁，推动人工智能领域的创新应用与跨学科研究。这种双向赋能关系为决策优化和自然语言处理的融合开辟了新路径。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-21T01:29:26Z
- **目录日期**: 2025-05-21
