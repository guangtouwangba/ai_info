# Evaluating Mathematical Reasoning Across Large Language Models: A Fine-Grained Approach

**URL**: http://arxiv.org/abs/2503.10573v2

## 原始摘要

With the rapid advancement of Artificial Intelligence (AI), Large Language
Models (LLMs) have significantly impacted a wide array of domains, including
healthcare, engineering, science, education, and mathematical reasoning. Among
these, mathematical reasoning remains a particularly challenging capability,
often requiring multi-step logic and abstract generalization. While prior work
has explored LLM performance on reasoning tasks, comprehensive evaluations that
span both depth and breadth across model families remain limited. In this
study, we present a systematic evaluation of mathematical reasoning abilities
across eight leading LLMs, including two recent DeepSeek models, using three
independent benchmark datasets. Our analyses reveal several key findings: (1)
DeepSeek-R1 performs competitively with o1 across most domains and achieves the
highest accuracy on the MMLU Formal Logic benchmark; (2) distilled variants,
such as DeepSeek-1.5B, exhibit substantial performance degradation; and (3)
Gemini 2.0 Flash achieves the lowest response latency. Beyond quantitative
metrics, we explore how architectural choices, training paradigms, and
optimization strategies contribute to variation in reasoning performance. These
findings provide new insights into the capabilities and limitations of current
LLMs in mathematical domains, and offer guidance for the development of future
models better aligned with rigorous reasoning demands.


## AI 摘要

本研究系统评估了8种主流大语言模型（LLMs）的数学推理能力，包括两个DeepSeek模型。关键发现：1）DeepSeek-R1在多数领域表现优异，在MMLU形式逻辑基准上准确率最高；2）蒸馏变体（如DeepSeek-1.5B）性能显著下降；3）Gemini 2.0 Flash响应延迟最低。研究还探讨了架构选择、训练方法和优化策略对推理性能的影响，为开发更符合严格推理需求的未来模型提供了指导。这些发现揭示了当前LLMs在数学领域的能力与局限。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-21T00:02:24Z
- **目录日期**: 2025-05-21
