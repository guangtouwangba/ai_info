# BlendRL: A Framework for Merging Symbolic and Neural Policy Learning

**URL**: http://arxiv.org/abs/2410.11689v2

## 原始摘要

Humans can leverage both symbolic reasoning and intuitive reactions. In
contrast, reinforcement learning policies are typically encoded in either
opaque systems like neural networks or symbolic systems that rely on predefined
symbols and rules. This disjointed approach severely limits the agents'
capabilities, as they often lack either the flexible low-level reaction
characteristic of neural agents or the interpretable reasoning of symbolic
agents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL
framework that harmoniously integrates both paradigms within RL agents that use
mixtures of both logic and neural policies. We empirically demonstrate that
BlendRL agents outperform both neural and symbolic baselines in standard Atari
environments, and showcase their robustness to environmental changes.
Additionally, we analyze the interaction between neural and symbolic policies,
illustrating how their hybrid use helps agents overcome each other's
limitations.


## AI 摘要

本文介绍了BlendRL，一种融合神经与符号范式的强化学习框架。传统RL智能体通常仅依赖神经网络（不透明但灵活）或符号系统（可解释但僵化），而BlendRL通过混合逻辑策略与神经策略，使智能体兼具快速反应与可解释推理能力。实验表明，在Atari游戏中，BlendRL智能体性能优于纯神经或符号基线，且对环境变化更具鲁棒性。研究还揭示了神经与符号策略的互补机制：二者协同可有效弥补各自缺陷。该框架为构建兼具人类式直觉与推理能力的AI提供了新思路。（99字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-22T18:02:24Z
- **目录日期**: 2025-04-22
