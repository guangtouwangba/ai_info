# BlendRL: A Framework for Merging Symbolic and Neural Policy Learning

**URL**: http://arxiv.org/abs/2410.11689v2

## 原始摘要

Humans can leverage both symbolic reasoning and intuitive reactions. In
contrast, reinforcement learning policies are typically encoded in either
opaque systems like neural networks or symbolic systems that rely on predefined
symbols and rules. This disjointed approach severely limits the agents'
capabilities, as they often lack either the flexible low-level reaction
characteristic of neural agents or the interpretable reasoning of symbolic
agents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL
framework that harmoniously integrates both paradigms within RL agents that use
mixtures of both logic and neural policies. We empirically demonstrate that
BlendRL agents outperform both neural and symbolic baselines in standard Atari
environments, and showcase their robustness to environmental changes.
Additionally, we analyze the interaction between neural and symbolic policies,
illustrating how their hybrid use helps agents overcome each other's
limitations.


## AI 摘要

BlendRL是一个新型的神经符号强化学习框架，将神经网络与符号推理策略相结合，以克服传统方法的局限性。传统强化学习通常依赖不透明的神经网络或预定义的符号系统，缺乏灵活性或可解释性。BlendRL通过混合逻辑策略和神经策略，使智能体既能灵活反应又能进行可解释的推理。实验证明，在Atari游戏中，BlendRL智能体的表现优于纯神经或符号基线，并展现出更强的环境适应能力。研究还分析了神经与符号策略的互补性，表明它们的结合能有效弥补彼此的不足。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-22T05:02:30Z
- **目录日期**: 2025-04-22
