# BlendRL: A Framework for Merging Symbolic and Neural Policy Learning

**URL**: http://arxiv.org/abs/2410.11689v2

## 原始摘要

Humans can leverage both symbolic reasoning and intuitive reactions. In
contrast, reinforcement learning policies are typically encoded in either
opaque systems like neural networks or symbolic systems that rely on predefined
symbols and rules. This disjointed approach severely limits the agents'
capabilities, as they often lack either the flexible low-level reaction
characteristic of neural agents or the interpretable reasoning of symbolic
agents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL
framework that harmoniously integrates both paradigms within RL agents that use
mixtures of both logic and neural policies. We empirically demonstrate that
BlendRL agents outperform both neural and symbolic baselines in standard Atari
environments, and showcase their robustness to environmental changes.
Additionally, we analyze the interaction between neural and symbolic policies,
illustrating how their hybrid use helps agents overcome each other's
limitations.


## AI 摘要

BlendRL是一个融合神经与符号策略的强化学习框架，旨在结合神经网络的灵活反应与符号系统的可解释推理能力。相比单一神经或符号基线，BlendRL在Atari游戏中表现更优，并展现出更强的环境适应力。该框架通过混合策略的协同作用，使智能体能互补克服各自局限：神经网络处理低层感知，符号系统提供高层推理。实验证明这种混合方法不仅提升性能，还增强了鲁棒性，为构建兼具反应速度与决策透明性的智能体提供了新思路。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-22T09:02:29Z
- **目录日期**: 2025-04-22
