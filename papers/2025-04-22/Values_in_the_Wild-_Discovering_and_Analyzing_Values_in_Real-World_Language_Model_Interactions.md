# Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions

**URL**: http://arxiv.org/abs/2504.15236v1

## 原始摘要

AI assistants can impart value judgments that shape people's decisions and
worldviews, yet little is known empirically about what values these systems
rely on in practice. To address this, we develop a bottom-up,
privacy-preserving method to extract the values (normative considerations
stated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit
in hundreds of thousands of real-world interactions. We empirically discover
and taxonomize 3,307 AI values and study how they vary by context. We find that
Claude expresses many practical and epistemic values, and typically supports
prosocial human values while resisting values like "moral nihilism". While some
values appear consistently across contexts (e.g. "transparency"), many are more
specialized and context-dependent, reflecting the diversity of human
interlocutors and their varied contexts. For example, "harm prevention" emerges
when Claude resists users, "historical accuracy" when responding to queries
about controversial events, "healthy boundaries" when asked for relationship
advice, and "human agency" in technology ethics discussions. By providing the
first large-scale empirical mapping of AI values in deployment, our work
creates a foundation for more grounded evaluation and design of values in AI
systems.


## AI 摘要

这项研究开发了一种自下而上、保护隐私的方法，从Claude 3和3.5模型的实际互动中提取了3,307种AI价值观。研究发现，Claude主要表达实用性和认知性价值观，通常支持亲社会的人类价值观，同时抵制"道德虚无主义"等观念。AI价值观既包含跨情境的一致性（如"透明度"），也表现出显著的语境依赖性：在拒绝用户时体现"伤害预防"，处理争议事件时强调"历史准确性"，提供关系建议时注重"健康边界"，技术伦理讨论中重视"人类能动性"。该研究为AI价值观的评估和设计提供了首个大规模实证基础。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-22T04:01:33Z
- **目录日期**: 2025-04-22
