# Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems

**URL**: http://arxiv.org/abs/2505.18139v1

## 原始摘要

This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.


## AI 摘要

这篇立场论文主张，负责任人工智能（RAI）指标间的理论不一致性（如公平性定义冲突或准确性与隐私的权衡）应被视为优势而非缺陷。作者提出，通过将指标视为多元目标，这种不一致性可带来三大益处：(1) 保持道德立场多样性；(2) 更完整捕捉多维度伦理概念；(3) 通过多目标优化提升模型泛化能力。相反，强行统一指标会削弱价值多样性、简化伦理深度并损害模型性能。论文呼吁RAI领域转向研究"可接受的不一致性阈值"，而非追求理论一致性，以在实践中实现稳健的近似平衡。（100字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-26T19:01:13Z
- **目录日期**: 2025-05-26
