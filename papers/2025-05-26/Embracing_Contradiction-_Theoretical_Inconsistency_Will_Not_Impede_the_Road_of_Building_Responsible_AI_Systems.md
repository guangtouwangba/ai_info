# Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems

**URL**: http://arxiv.org/abs/2505.18139v1

## 原始摘要

This position paper argues that the theoretical inconsistency often observed
among Responsible AI (RAI) metrics, such as differing fairness definitions or
tradeoffs between accuracy and privacy, should be embraced as a valuable
feature rather than a flaw to be eliminated. We contend that navigating these
inconsistencies, by treating metrics as divergent objectives, yields three key
benefits: (1) Normative Pluralism: Maintaining a full suite of potentially
contradictory metrics ensures that the diverse moral stances and stakeholder
values inherent in RAI are adequately represented. (2) Epistemological
Completeness: The use of multiple, sometimes conflicting, metrics allows for a
more comprehensive capture of multifaceted ethical concepts, thereby preserving
greater informational fidelity about these concepts than any single, simplified
definition. (3) Implicit Regularization: Jointly optimizing for theoretically
conflicting objectives discourages overfitting to one specific metric, steering
models towards solutions with enhanced generalization and robustness under
real-world complexities. In contrast, efforts to enforce theoretical
consistency by simplifying or pruning metrics risk narrowing this value
diversity, losing conceptual depth, and degrading model performance. We
therefore advocate for a shift in RAI theory and practice: from getting trapped
in inconsistency to characterizing acceptable inconsistency thresholds and
elucidating the mechanisms that permit robust, approximated consistency in
practice.


## AI 摘要

这篇立场论文主张，负责任人工智能（RAI）指标间的理论不一致性（如公平性定义的差异或准确性与隐私的权衡）应被视为优势而非缺陷。作者提出，通过将指标视为多元目标来处理这些矛盾能带来三大益处：(1) 保持道德立场和利益相关者价值的多样性；(2) 通过多维度指标更完整地捕捉复杂伦理概念；(3) 联合优化冲突目标可防止模型过拟合，增强泛化能力。相反，强行统一指标会削弱价值多样性、简化伦理深度并损害模型性能。论文呼吁RAI领域应转向研究可接受的不一致性阈值，而非追求理论一致性。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-26T05:01:29Z
- **目录日期**: 2025-05-26
