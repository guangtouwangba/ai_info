# Do Two AI Scientists Agree?

**URL**: http://arxiv.org/abs/2504.02822v1

## 原始摘要

When two AI models are trained on the same scientific task, do they learn the
same theory or two different theories? Throughout history of science, we have
witnessed the rise and fall of theories driven by experimental validation or
falsification: many theories may co-exist when experimental data is lacking,
but the space of survived theories become more constrained with more
experimental data becoming available. We show the same story is true for AI
scientists. With increasingly more systems provided in training data, AI
scientists tend to converge in the theories they learned, although sometimes
they form distinct groups corresponding to different theories. To
mechanistically interpret what theories AI scientists learn and quantify their
agreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI
Scientists, trained on standard problems in physics, aggregating training
results across many seeds simulating the different configurations of AI
scientists. Our findings suggests for AI scientists switch from learning a
Hamiltonian theory in simple setups to a Lagrangian formulation when more
complex systems are introduced. We also observe strong seed dependence of the
training dynamics and final learned weights, controlling the rise and fall of
relevant theories. We finally demonstrate that not only can our neural networks
aid interpretability, it can also be applied to higher dimensional problems.


## AI 摘要

研究表明，当两个AI模型在相同科学任务上训练时，它们可能学习不同理论，但随着训练数据增加，理论会趋于收敛。研究者提出MASS框架（哈密顿-拉格朗日神经网络），通过多组随机种子实验模拟不同AI科学家的理论形成过程。结果显示：在简单系统中AI倾向于学习哈密顿理论，而复杂系统则转向拉格朗日表述。训练动态和最终权重表现出强烈的随机种子依赖性，这决定了不同理论的兴衰。该方法不仅能增强模型可解释性，还可应用于高维问题，揭示了AI科学家的理论演化与人类科学发展规律的相似性。（100字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-05T15:01:12Z
- **目录日期**: 2025-04-05
