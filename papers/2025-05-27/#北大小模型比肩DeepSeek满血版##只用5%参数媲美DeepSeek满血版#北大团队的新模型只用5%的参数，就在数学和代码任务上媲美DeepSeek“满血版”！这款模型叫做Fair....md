# #北大小模型比肩DeepSeek满血版##只用5%参数媲美DeepSeek满血版#北大团队的新模型只用5%的参数，就在数学和代码任务上媲美DeepSeek“满血版”！这款模型叫做Fair...

**URL**: https://weibo.com/6105753431/PtJEWcR8r

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%8C%97%E5%A4%A7%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%82%A9DeepSeek%E6%BB%A1%E8%A1%80%E7%89%88%23&amp;extparam=%23%E5%8C%97%E5%A4%A7%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%AF%94%E8%82%A9DeepSeek%E6%BB%A1%E8%A1%80%E7%89%88%23" data-hide=""><span class="surl-text">#北大小模型比肩DeepSeek满血版#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%8F%AA%E7%94%A85%25%E5%8F%82%E6%95%B0%E5%AA%B2%E7%BE%8EDeepSeek%E6%BB%A1%E8%A1%80%E7%89%88%23&amp;extparam=%23%E5%8F%AA%E7%94%A85%25%E5%8F%82%E6%95%B0%E5%AA%B2%E7%BE%8EDeepSeek%E6%BB%A1%E8%A1%80%E7%89%88%23" data-hide=""><span class="surl-text">#只用5%参数媲美DeepSeek满血版#</span></a><br><br>北大团队的新模型只用5%的参数，就在数学和代码任务上媲美DeepSeek“满血版”！<br><br>这款模型叫做FairyR1-32B，由北京大学杨仝教授团队打造。它是在DeepSeek-R1-Distill-Qwen-32B基础上微调而来，结合了“分合蒸馏”的策略，重构数据流程，并通过多个教师模型训练提升表现。<br><br>团队特别优化了蒸馏数据流程：<br><br>- 数学和代码题来自知名数据集；<br>- 由多个大模型生成答案；<br>- 筛选答案正确性、调整结构并优化思维链；<br>- 严格控制数据长度，保留高质量内容；<br>- 最终提炼出6.6k数学题与3.8k代码题用于训练。<br><br>他们还训练了两个专业小模型（分别擅长数学和代码），再利用AcreeFusion合并，大幅提升效率同时节省资源。<br><br>在多个公开基准测试中，FairyR1展现出了在低参数量下的竞争力表现。以下为FairyR1与DeepSeek-R1-671B及DeepSeek-R1-Distill-Qwen-32B在部分基准上的得分对比：【图2】<br><br>杨仝教授团队表示：“FairyR1-32B模型是我们探索高效大型语言模型技术路线的阶段性成果。通过对蒸馏和合并方法的改进，我们初步验证了在有限资源下实现高性能模型的可行性。”<br><br>团队成员：李旺、周俊廷、刘文睿、姚一伦、王融乐、杨仝<img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1tvd0wu5rj30hs0ce0yn.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1tvd1wjkkj30zk0en77p.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

北京大学杨仝团队推出高效小模型FairyR1-32B，仅用5%参数量（32B）即媲美DeepSeek-R1-671B"满血版"性能。该模型基于分合蒸馏策略优化：1）精选数学/代码数据，经多模型生成并严格筛选（6.6k数学+3.8k代码题）；2）训练专业子模型后融合提升效率。在多项基准测试中，其表现接近671B参数大模型，验证了资源受限场景下的高性能可行性。核心创新在于重构蒸馏流程与多教师协同训练，为轻量化模型开发提供新思路。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-27T06:03:36Z
- **目录日期**: 2025-05-27
