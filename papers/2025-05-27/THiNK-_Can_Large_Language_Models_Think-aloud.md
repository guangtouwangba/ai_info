# THiNK: Can Large Language Models Think-aloud?

**URL**: http://arxiv.org/abs/2505.20184v1

## 原始摘要

Assessing higher-order thinking skills in large language models (LLMs)
remains a fundamental challenge, especially in tasks that go beyond
surface-level accuracy. In this work, we propose THiNK (Testing Higher-order
Notion of Knowledge), a multi-agent, feedback-driven evaluation framework
grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative
task of problem generation, critique, and revision, encouraging LLMs to
think-aloud through step-by-step reflection and refinement. This enables a
systematic evaluation of both lower-order (e.g., remember, understand) and
higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven
state-of-the-art LLMs and perform a detailed cognitive analysis of their
outputs. Results reveal that while models reliably perform lower-order
categories well, they struggle with applying knowledge in realistic contexts
and exhibit limited abstraction. Structured feedback loops significantly
improve reasoning performance, particularly in higher-order thinking.
Qualitative evaluations further confirm that THiNK-guided outputs better align
with domain logic and problem structure. The code of our framework provides a
scalable methodology for probing and enhancing LLM reasoning, offering new
directions for evaluation grounded in learning science, which is available at
our GitHub repository.


## AI 摘要

本文提出了THiNK框架，一个基于布鲁姆分类法的多智能体反馈驱动评估系统，用于评估大语言模型(LLMs)的高阶思维能力。THiNK通过问题生成、批判和修订的迭代过程，系统评估模型从基础记忆到创造等高阶认知能力。研究发现，当前LLMs在低阶任务表现良好，但在实际应用和抽象推理方面存在困难。反馈循环显著提升了推理能力，特别是高阶思维。THiNK为LLM评估提供了可扩展的方法论，其代码已在GitHub开源。该研究为基于学习科学的模型评估提供了新方向。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-27T23:01:44Z
- **目录日期**: 2025-05-27
