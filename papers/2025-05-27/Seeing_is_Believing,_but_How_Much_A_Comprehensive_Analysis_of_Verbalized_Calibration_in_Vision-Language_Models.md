# Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models

**URL**: http://arxiv.org/abs/2505.20236v1

## 原始摘要

Uncertainty quantification is essential for assessing the reliability and
trustworthiness of modern AI systems. Among existing approaches, verbalized
uncertainty, where models express their confidence through natural language,
has emerged as a lightweight and interpretable solution in large language
models (LLMs). However, its effectiveness in vision-language models (VLMs)
remains insufficiently studied. In this work, we conduct a comprehensive
evaluation of verbalized confidence in VLMs, spanning three model categories,
four task domains, and three evaluation scenarios. Our results show that
current VLMs often display notable miscalibration across diverse tasks and
settings. Notably, visual reasoning models (i.e., thinking with images)
consistently exhibit better calibration, suggesting that modality-specific
reasoning is critical for reliable uncertainty estimation. To further address
calibration challenges, we introduce Visual Confidence-Aware Prompting, a
two-stage prompting strategy that improves confidence alignment in multimodal
settings. Overall, our study highlights the inherent miscalibration in VLMs
across modalities. More broadly, our findings underscore the fundamental
importance of modality alignment and model faithfulness in advancing reliable
multimodal systems.


## AI 摘要

当前研究评估了视觉语言模型（VLMs）中语言化不确定性的校准效果，发现现有模型在多任务和场景中存在明显校准不足。视觉推理模型表现更优，表明模态特定推理对可靠的不确定性估计至关重要。为此，研究提出"视觉置信感知提示"（Visual Confidence-Aware Prompting）的两阶段提示策略，以改善多模态设置下的置信校准。研究揭示了VLMs跨模态的固有校准问题，强调模态对齐和模型忠实性对发展可靠多模态系统的关键作用。该成果为提升AI系统可信度提供了重要见解，涵盖3类模型、4个任务域和3种评估场景。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-27T20:01:11Z
- **目录日期**: 2025-05-27
