# #多模态模型统一图像生成和编辑##北大团队多模态新突破#北京大学的VARGPT团队，推出了VARGPT模型的v1.1版本，能在不改动架构的情况下，根据你给出的指令编辑生成...

**URL**: https://weibo.com/6105753431/Pnn7ygPSP

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%9F%E4%B8%80%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E5%92%8C%E7%BC%96%E8%BE%91%23&amp;extparam=%23%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E7%BB%9F%E4%B8%80%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E5%92%8C%E7%BC%96%E8%BE%91%23" data-hide=""><span class="surl-text">#多模态模型统一图像生成和编辑#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%8C%97%E5%A4%A7%E5%9B%A2%E9%98%9F%E5%A4%9A%E6%A8%A1%E6%80%81%E6%96%B0%E7%AA%81%E7%A0%B4%23&amp;extparam=%23%E5%8C%97%E5%A4%A7%E5%9B%A2%E9%98%9F%E5%A4%9A%E6%A8%A1%E6%80%81%E6%96%B0%E7%AA%81%E7%A0%B4%23" data-hide=""><span class="surl-text">#北大团队多模态新突破#</span></a><br><br>北京大学的VARGPT团队，推出了VARGPT模型的v1.1版本，能在不改动架构的情况下，根据你给出的指令编辑生成图像。<br><br>比如，用户可以要求模型将某个图像转变成“抽象风”、“艺术风”、“未来风”、“漫画风”。VARGPT-v1.1便会自动进行颜色搭配、构图方式甚至细节上的风格调整。<br><br>VARGPT-v1.1的亮点包括：<br><br>1. 迭代视觉指令微调与强化学习结合的训练策略：&nbsp;通过交替进行监督微调（SFT）与基于偏好直接优化（DPO）的强化学习，有效提高了模型的图像生成质量。模型逐步提升图像生成分辨率，从256×256扩展至512×512像素，图像细节与真实性显著增强。<br><br>2. 更大规模的视觉生成训练数据集：&nbsp;VARGPT-v1.1采用了多达830万条视觉生成指令数据，包括真实世界的LAION-COCO数据集以及由Midjourney与Flux模型生成的合成数据。大规模数据的使用显著扩大了模型对不同类型图像生成的泛化能力。<br><br>3. 升级语言模型主干至Qwen2：&nbsp;引入最新的Qwen2-7B语言模型主干，利用其高效的注意力机制与更好的token化策略，有效提升了模型的视觉理解能力。<br><br>4. 无架构修改的图像编辑能力：&nbsp;VARGPT-v1.1在不改动模型架构的基础上，通过专门构建的图像编辑数据集，实现了图像编辑功能。这使得模型不仅可以理解和生成图像，还能根据用户指令对图像进行编辑。<br><br>VARGPT-v1.1的训练策略采用了三阶段方法，首先，VARGPT-v1.1在视觉理解、图像生成和图像编辑等方面都进行了针对性训练。<br><br>其次，团队结合视觉指令微调和强化学习，让模型能够通过迭代的方式不断优化生成效果，以提升图像生成的分辨率和质量。<br><br>最后，经过有监督微调（SFT）和直接偏好优化（DPO）的多轮训练，VARGPT-v1.1不仅能生成图像，还能在不改变原架构的基础上，对现有图像进行精细化调整，甚至可以根据用户提供的指令对图像进行各种编辑操作。<br><br>实验结果显示，VARGPT-v1.1在一些视觉理解、视觉问答等任务上，全面超越了现有的多模态大语言模型。它不仅能够生成高质量的图像，还能通过输入文本指令和图像指令来产生混合模态的输出，真正做到了图文并茂。<br><br>VARGPT-v1.1所有的训练数据、推理代码、模型都已经开源，感兴趣的小伙伴可以点击——<br><br>细节解读：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F96yyriyCmwnrGk4_M9_ZUg" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>project:&nbsp;<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fvargpt1-1.github.io%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a>  <br>code:&nbsp;<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fgithub.com%2FVARGPT-family%2FVARGPT-v1.1" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a>  <br>arxiv:&nbsp;<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fabs%2F2504.02949" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3ly1i0hhk79htgj31hc0u0abk.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i0hhjzhvfvj30u00y17wh.jpg" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax3.sinaimg.cn/orj480/006Fd7o3ly1i0hhk7uwz8j31hc0u0abk.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/sCY5OUqJlx08nuubRB7G010412007Ayz0E010.mp4?label=mp4_720p&amp;template=1280x720.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1744714859&amp;ssig=rmRRs1deCD&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/o3vdIwrhlx08nuubIbPW0104120049ka0E010.mp4?label=mp4_hd&amp;template=852x480.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1744714859&amp;ssig=KNgF7XMenC&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/amP9tCE3lx08nuubLkPS010412002HTB0E010.mp4?label=mp4_ld&amp;template=640x360.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1744714859&amp;ssig=UAyKpikAIG&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5155704122572849" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

北京大学VARGPT团队推出v1.1多模态模型，通过三阶段训练策略（视觉指令微调+强化学习）实现512×512高分辨率图像生成与编辑。模型特点包括：采用830万条视觉指令数据增强泛化能力，升级Qwen2-7B语言模型提升理解力，无需架构修改即可完成风格转换（如抽象/漫画风）等图像编辑。实验显示其在视觉问答等任务超越现有模型，支持混合模态输出。所有训练数据、代码和模型均已开源。关键突破在于结合监督微调(SFT)与直接偏好优化(DPO)的迭代优化方法。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-15T10:02:30Z
- **目录日期**: 2025-04-15
