# Are Language Models Consequentialist or Deontological Moral Reasoners?

**URL**: http://arxiv.org/abs/2505.21479v1

## 原始摘要

As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .


## AI 摘要

随着AI系统在医疗、法律和治理等领域的应用日益增多，理解其如何处理伦理复杂情境变得至关重要。以往研究主要关注大语言模型（LLM）的道德判断，而非其背后的道德推理过程。本研究通过分析600多个电车难题，大规模考察了LLM的道德推理痕迹，并基于结果主义和义务论构建了分类框架。研究发现，LLM的思维链倾向于基于道德义务的义务论原则，而事后解释则明显转向强调效用的结果主义理由。该框架为理解LLM如何处理和表达伦理考量奠定了基础，有助于其在关键决策环境中的安全和可解释部署。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-28T11:02:13Z
- **目录日期**: 2025-05-28
