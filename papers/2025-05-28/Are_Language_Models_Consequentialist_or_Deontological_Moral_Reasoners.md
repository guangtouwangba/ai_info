# Are Language Models Consequentialist or Deontological Moral Reasoners?

**URL**: http://arxiv.org/abs/2505.21479v1

## 原始摘要

As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .


## AI 摘要

随着AI系统在医疗、法律和治理领域的应用增多，理解其处理伦理复杂场景的方式变得至关重要。本研究不同于以往仅关注大语言模型（LLM）道德判断的工作，而是大规模分析了LLM提供的道德推理过程。通过600多个电车难题作为测试案例，研究揭示了不同LLM的推理模式，并基于结果主义和义务论构建了道德理由分类体系。研究发现：LLM的思维链倾向于基于道德义务的义务论原则，而事后解释则明显转向强调效用的结果主义理由。该框架为理解LLM如何处理伦理问题奠定了基础，有助于高风险决策中LLM的安全部署。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-28T03:21:08Z
- **目录日期**: 2025-05-28
