# Are Language Models Consequentialist or Deontological Moral Reasoners?

**URL**: http://arxiv.org/abs/2505.21479v1

## 原始摘要

As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .


## AI 摘要

随着AI系统在医疗、法律和治理等领域的应用增多，理解其处理伦理复杂场景的方式变得至关重要。本研究不同于以往仅关注大型语言模型(LLMs)道德判断的工作，而是大规模分析其道德推理过程。研究者利用600多个电车难题作为测试案例，提出了基于结果论和义务论的道德推理分类法。分析发现：LLMs的思维链倾向于基于道德义务的义务论原则，而事后解释则明显转向强调效用的结果论理由。该框架为理解LLMs处理伦理问题提供了基础，是高风险决策中安全部署LLMs的重要一步。代码已开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-28T21:02:05Z
- **目录日期**: 2025-05-28
