# Are Language Models Consequentialist or Deontological Moral Reasoners?

**URL**: http://arxiv.org/abs/2505.21479v1

## 原始摘要

As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .


## AI 摘要

随着AI系统在医疗、法律和治理等领域的应用日益增多，理解其如何处理伦理复杂场景变得至关重要。本研究不同于以往仅关注大语言模型（LLM）道德判断的工作，而是聚焦于大规模分析LLM提供的道德推理过程。通过600多个电车难题作为测试案例，研究引入并验证了一套道德推理分类法，基于结果主义和义务论两大伦理理论对推理痕迹进行系统分类。分析发现：LLM的思维链倾向于基于道德义务的义务论原则，而事后解释则明显转向强调效用的结果主义理由。该框架为理解LLM处理伦理问题的方式奠定了基础，有助于高风险决策环境中LLM的安全和可解释部署。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-28T20:02:08Z
- **目录日期**: 2025-05-28
