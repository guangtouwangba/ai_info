# Are Language Models Consequentialist or Deontological Moral Reasoners?

**URL**: http://arxiv.org/abs/2505.21479v1

## 原始摘要

As AI systems increasingly navigate applications in healthcare, law, and
governance, understanding how they handle ethically complex scenarios becomes
critical. Previous work has mainly examined the moral judgments in large
language models (LLMs), rather than their underlying moral reasoning process.
In contrast, we focus on a large-scale analysis of the moral reasoning traces
provided by LLMs. Furthermore, unlike prior work that attempted to draw
inferences from only a handful of moral dilemmas, our study leverages over 600
distinct trolley problems as probes for revealing the reasoning patterns that
emerge within different LLMs. We introduce and test a taxonomy of moral
rationales to systematically classify reasoning traces according to two main
normative ethical theories: consequentialism and deontology. Our analysis
reveals that LLM chains-of-thought tend to favor deontological principles based
on moral obligations, while post-hoc explanations shift notably toward
consequentialist rationales that emphasize utility. Our framework provides a
foundation for understanding how LLMs process and articulate ethical
considerations, an important step toward safe and interpretable deployment of
LLMs in high-stakes decision-making environments. Our code is available at
https://github.com/keenansamway/moral-lens .


## AI 摘要

随着AI系统在医疗、法律和治理等领域的应用增多，理解其如何处理伦理复杂场景变得至关重要。现有研究主要关注大语言模型（LLMs）的道德判断，而非其内在推理过程。本研究通过分析600多个电车难题案例，系统考察了LLMs的道德推理轨迹，并基于结果主义和义务论两大伦理理论建立了分类框架。研究发现：LLMs的思维链更倾向于基于道德义务的义务论原则，而事后解释则明显转向强调效用的结果主义理由。该框架为理解LLMs的伦理处理机制奠定了基础，有助于其在高风险决策中的安全部署。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-28T05:02:02Z
- **目录日期**: 2025-05-28
