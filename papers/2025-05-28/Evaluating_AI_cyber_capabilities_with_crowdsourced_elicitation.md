# Evaluating AI cyber capabilities with crowdsourced elicitation

**URL**: http://arxiv.org/abs/2505.19915v2

## 原始摘要

As AI systems become increasingly capable, understanding their offensive
cyber potential is critical for informed governance and responsible deployment.
However, it's hard to accurately bound their capabilities, and some prior
evaluations dramatically underestimated them. The art of extracting maximum
task-specific performance from AIs is called "AI elicitation", and today's
safety organizations typically conduct it in-house. In this paper, we explore
crowdsourcing elicitation efforts as an alternative to in-house elicitation
work.
  We host open-access AI tracks at two Capture The Flag (CTF) competitions: AI
vs. Humans (400 teams) and Cyber Apocalypse (8000 teams). The AI teams achieve
outstanding performance at both events, ranking top-5% and top-10% respectively
for a total of \$7500 in bounties. This impressive performance suggests that
open-market elicitation may offer an effective complement to in-house
elicitation. We propose elicitation bounties as a practical mechanism for
maintaining timely, cost-effective situational awareness of emerging AI
capabilities.
  Another advantage of open elicitations is the option to collect human
performance data at scale. Applying METR's methodology, we found that AI agents
can reliably solve cyber challenges requiring one hour or less of effort from a
median human CTF participant.


## AI 摘要

随着AI系统能力提升，评估其网络攻击潜力对治理和部署至关重要。传统内部评估常低估AI能力，而本研究通过众包方式探索替代方案。研究团队在两场网络安全夺旗赛（400队和8000队规模）中设立公开AI赛道，参赛AI分别取得前5%和前10%的成绩，赢得7500美元奖金，证明众包激发比内部评估更有效。研究者建议设立"能力激发奖金"机制，以经济高效地追踪AI新兴能力。研究还发现，AI能可靠解决中位人类选手1小时内完成的网络安全挑战。众包方式可大规模收集人类表现数据，为AI能力评估提供新途径。（99字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-28T09:02:40Z
- **目录日期**: 2025-05-28
