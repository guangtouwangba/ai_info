# #DeepSeekR2最新爆料##DeepSeekR2成本下降97.3%#DeepSeek R2最新爆料来了，单位成本居然下降97.3%，而且彻底摆脱了对英伟达芯片的依赖！- 单位成本暴降97.3%：训...

**URL**: https://weibo.com/6105753431/PpawAzyoj

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23DeepSeekR2%E6%9C%80%E6%96%B0%E7%88%86%E6%96%99%23&amp;extparam=%23DeepSeekR2%E6%9C%80%E6%96%B0%E7%88%86%E6%96%99%23" data-hide=""><span class="surl-text">#DeepSeekR2最新爆料#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23DeepSeekR2%E6%88%90%E6%9C%AC%E4%B8%8B%E9%99%8D97.3%25%23&amp;extparam=%23DeepSeekR2%E6%88%90%E6%9C%AC%E4%B8%8B%E9%99%8D97.3%25%23" data-hide=""><span class="surl-text">#DeepSeekR2成本下降97.3%#</span></a><br><br>DeepSeek R2最新爆料来了，单位成本居然下降97.3%，而且彻底摆脱了对英伟达芯片的依赖！<br><br>- 单位成本暴降97.3%：训练和推理的单位成本，比GPT-4o下降了惊人的97.3%。具体来看，传言是输入每百万tokens只要0.07美元，输出是0.27美元。这意味着，用同样的钱，别人训练一次模型，DeepSeek能训练30次以上。<br><br>-参数量和训练数据：R2模型参数量达1.2万亿，是R1的两倍。其中活跃参数为78亿，走的是混合专家模型（MoE）路线。另外，R2的训练数据量达5.2PB，覆盖范围极广。<br>    <br>- 摆脱英伟达芯片：DeepSeek R2整个训练完全没用英伟达显卡，而是全部基于华为的Ascend 910B集群。团队自主搭建了分布式训练框架，在浮点16位（FP16）精度下，实测集群算力达到512 PetaFLOPS，集群利用率高达82%，整体性能约为英伟达A100集群的91%。<br>    <br>- 性能方面：在最新的C-Eval 2.0评测中，R2取得了89.7%的得分，在COCO图像测试中则实现了92.4%的准确率，展现出强大的综合性能。<br>    <br>- 量化压缩技术：DeepSeek R2在8bit精度下，模型体积压缩了83%，而精度损失不到2%，大幅提升了端侧部署的可行性。<br><br>值得注意的是，目前上述信息还没有获得内部人士证实，请大家理性围观。<br><br>如果上述消息属实，DeepSeek R2在开源社区上线之日，很可能会引发资本市场的又一轮震动。<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23DeepSeekR2%23&amp;extparam=%23DeepSeekR2%23" data-hide=""><span class="surl-text">#DeepSeekR2#</span></a><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i0v5dqzxjzj30zk0gfafu.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i0v5dtj2i2j30xc0k8h16.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i0v5du37nsj315e10qtj7.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

DeepSeek R2最新爆料显示，其训练和推理成本比GPT-4o降低97.3%，输入每百万tokens仅0.07美元，输出0.27美元。模型参数量达1.2万亿（活跃参数78亿），采用混合专家架构（MoE），训练数据5.2PB。完全基于华为Ascend 910B集群训练，算力达512 PetaFLOPS，利用率82%，性能接近A100的91%。在C-Eval 2.0评测中得分89.7%，COCO图像测试准确率92.4%。8bit量化压缩83%，精度损失<2%，提升端侧部署可行性。若属实，可能对开源社区和资本市场产生重大影响。（注：信息尚未官方证实）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-27T08:03:27Z
- **目录日期**: 2025-04-27
