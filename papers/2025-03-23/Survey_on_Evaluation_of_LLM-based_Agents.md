# Survey on Evaluation of LLM-based Agents

**URL**: http://arxiv.org/abs/2503.16416v1

## 原始摘要

The emergence of LLM-based agents represents a paradigm shift in AI, enabling
autonomous systems to plan, reason, use tools, and maintain memory while
interacting with dynamic environments. This paper provides the first
comprehensive survey of evaluation methodologies for these increasingly capable
agents. We systematically analyze evaluation benchmarks and frameworks across
four critical dimensions: (1) fundamental agent capabilities, including
planning, tool use, self-reflection, and memory; (2) application-specific
benchmarks for web, software engineering, scientific, and conversational
agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating
agents. Our analysis reveals emerging trends, including a shift toward more
realistic, challenging evaluations with continuously updated benchmarks. We
also identify critical gaps that future research must address-particularly in
assessing cost-efficiency, safety, and robustness, and in developing
fine-grained, and scalable evaluation methods. This survey maps the rapidly
evolving landscape of agent evaluation, reveals the emerging trends in the
field, identifies current limitations, and proposes directions for future
research.


## AI 摘要

本文首次全面调查了基于大型语言模型（LLM）的智能体评估方法，系统分析了四个关键维度的评估基准和框架：基本能力（如规划、工具使用、自我反思和记忆）、特定应用基准（如网络、软件工程、科学和对话代理）、通用代理基准以及评估框架。研究发现，评估趋势正转向更现实和具挑战性的持续更新基准。同时，指出了未来研究需解决的关键问题，包括成本效率、安全性和鲁棒性评估，以及开发细粒度和可扩展的评估方法。本文为智能体评估领域的快速发展提供了方向。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-23T10:01:18Z
- **目录日期**: 2025-03-23
