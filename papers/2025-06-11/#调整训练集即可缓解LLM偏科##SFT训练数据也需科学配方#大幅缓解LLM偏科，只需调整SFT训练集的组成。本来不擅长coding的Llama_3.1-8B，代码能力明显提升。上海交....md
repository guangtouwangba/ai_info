# #调整训练集即可缓解LLM偏科##SFT训练数据也需科学配方#大幅缓解LLM偏科，只需调整SFT训练集的组成。本来不擅长coding的Llama 3.1-8B，代码能力明显提升。上海交...

**URL**: https://weibo.com/6105753431/Pw3AYqCyk

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%B0%83%E6%95%B4%E8%AE%AD%E7%BB%83%E9%9B%86%E5%8D%B3%E5%8F%AF%E7%BC%93%E8%A7%A3LLM%E5%81%8F%E7%A7%91%23&amp;extparam=%23%E8%B0%83%E6%95%B4%E8%AE%AD%E7%BB%83%E9%9B%86%E5%8D%B3%E5%8F%AF%E7%BC%93%E8%A7%A3LLM%E5%81%8F%E7%A7%91%23" data-hide=""><span class="surl-text">#调整训练集即可缓解LLM偏科#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23SFT%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B9%9F%E9%9C%80%E7%A7%91%E5%AD%A6%E9%85%8D%E6%96%B9%23&amp;extparam=%23SFT%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B9%9F%E9%9C%80%E7%A7%91%E5%AD%A6%E9%85%8D%E6%96%B9%23" data-hide=""><span class="surl-text">#SFT训练数据也需科学配方#</span></a><br><br>大幅缓解LLM偏科，只需调整SFT训练集的组成。<br><br>本来不擅长coding的Llama 3.1-8B，代码能力明显提升。<br><br>上海交大&amp;上海AI Lab联合团队提出创新方法IDEAL，可显著提升LLM在多种不同领域上的综合性能。<br><br>此外，研究还有一些重要发现，比如：<br><br>- SFT阶段训练数据的数量不是关键<br>- 配比不合适，训练数据越多，反而会加剧模型“偏科”<br><br>具体来看👉<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F3b4afz0xD6_drY10kosUrA" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i2bit433ccj30kg0hojwp.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

上海交大与上海AI Lab联合团队提出IDEAL方法，通过调整SFT训练数据的组成配比，显著缓解了大语言模型(LLM)的"偏科"问题。研究表明，训练数据数量并非关键，不合理的配比反而会加剧模型能力失衡。该方法使原本不擅长编程的Llama 3.1-8B模型代码能力明显提升，有效提高了LLM在多个领域的综合表现。这一发现为优化SFT训练提供了新思路，强调数据配比的重要性。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-11T17:02:01Z
- **目录日期**: 2025-06-11
