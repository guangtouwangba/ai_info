# ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering

**URL**: http://arxiv.org/abs/2506.09050v1

## 原始摘要

How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.


## AI 摘要

ALE-Bench是一个新的AI基准测试，用于评估AI系统在基于分数的算法编程竞赛中的表现。该基准借鉴了AtCoder启发式竞赛的真实任务，涉及计算复杂度高的优化问题（如物流调度、生产规划等），且尚无精确解。相比短期、通过/失败的编程测试，ALE-Bench支持长期迭代优化，并提供测试反馈和可视化工具。评估显示，前沿大语言模型（LLM）在特定问题上表现优异，但在跨问题一致性和长期问题解决能力上仍显著落后于人类。该基准旨在推动AI在算法工程领域的进步。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-11T13:10:58Z
- **目录日期**: 2025-06-11
