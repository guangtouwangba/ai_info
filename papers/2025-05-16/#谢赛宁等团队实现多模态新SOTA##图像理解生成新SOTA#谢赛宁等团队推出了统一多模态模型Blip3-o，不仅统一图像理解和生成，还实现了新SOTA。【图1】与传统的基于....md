# #谢赛宁等团队实现多模态新SOTA##图像理解生成新SOTA#谢赛宁等团队推出了统一多模态模型Blip3-o，不仅统一图像理解和生成，还实现了新SOTA。【图1】与传统的基于...

**URL**: https://weibo.com/6105753431/Ps66trwSM

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%B0%A2%E8%B5%9B%E5%AE%81%E7%AD%89%E5%9B%A2%E9%98%9F%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%A8%A1%E6%80%81%E6%96%B0SOTA%23&amp;extparam=%23%E8%B0%A2%E8%B5%9B%E5%AE%81%E7%AD%89%E5%9B%A2%E9%98%9F%E5%AE%9E%E7%8E%B0%E5%A4%9A%E6%A8%A1%E6%80%81%E6%96%B0SOTA%23" data-hide=""><span class="surl-text">#谢赛宁等团队实现多模态新SOTA#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E7%94%9F%E6%88%90%E6%96%B0SOTA%23&amp;extparam=%23%E5%9B%BE%E5%83%8F%E7%90%86%E8%A7%A3%E7%94%9F%E6%88%90%E6%96%B0SOTA%23" data-hide=""><span class="surl-text">#图像理解生成新SOTA#</span></a><br><br>谢赛宁等团队推出了统一多模态模型Blip3-o，不仅统一图像理解和生成，还实现了新SOTA。【图1】<br><br>与传统的基于VAE的表征不同，他们提出了一种新方法，使用扩散Transformer来生成语义丰富的CLIP图像特征。这种设计提高了训练效率，又提升了生成质量。<br><br>此外，他们还证明，先进行图像理解训练，再进行图像生成训练的统一模型顺序预训练策略，具有实用优势，既能保持图像理解能力，又能培养强大的图像生成能力。<br><br>Blip3-o采用的新的统一架构，包括两部分。【图2】<br><br>图像理解部分，他们使用CLIP对图像进行编码，并计算目标文本标记与预测文本标记之间的交叉熵损失。<br><br>在图像生成部分，自回归模型首先生成一系列中间视觉特征，然后将其作为扩散Transformer的条件输入，生成CLIP图像特征，以逼近地面真实的CLIP特征。<br><br>通过使用CLIP编码器，图像理解和图像生成共享同一个语义空间，从而有效地统一了这两项任务。<br><br>具体来说，他们在图像生成部分，尝试了三种设计方案——所有设计都使用了自回归+扩散框架，但图像生成组件各不相同。<br><br>对于流匹配损失，他们冻结了自回归模型，只对图像生成模块进行了微调，以保留模型的语言能力。【图3】<br><br>结果显示，CLIP+Flow Matching在GenEval和DPG-Bench上都获得了最佳的提示对齐得分，而VAE+Flow Matching产生的FID最低（最佳），表明美学质量上乘。<br><br>不过，FID有其固有的局限性：它量化的是与目标图像分布的风格偏差，往往忽略了真正的生成质量和即时配准。他们在MJHQ-30k数据集上对GPT-4o进行的FID评估得出的分数约为30.0，这说明FID在图像生成评估中可能会产生误导。【图4】<br><br>最终，他们确定CLIP+Flow Matching是最有效的设计选择。<br><br>将图像生成整合到统一模型中时，自回归模型比像素级表征（VAE）更有效地学习语义级特征（CLIP）；采用流匹配作为训练目标能更好地捕捉底层图像分布，从而提高样本多样性和视觉质量。<br><br>随后，他们开始研究训练策略： &gt;到底是联合训练还是顺序训练？【图5】<br><br>在联合训练设置中，尽管之前研究证明图像理解和生成任务可能互惠互利，但有两个关键因素会影响它们的协同效应：<br><br>(i)总数据量和(ii)图像理解和生成数据之间的数据比例。<br><br>相比之下，顺序训练具有更大的灵活性：它允许冻结自回归骨干，并保持图像理解能力。这样一来，可以将所有训练能力用于图像生成，避免联合训练中的任何任务间效应。同样受LMFusion和MetaQuery的启发，我们决定选择顺序训练来构建统一的多模态模型，并将联合训练留待未来工作中使用。<br><br>最终在图像理解和生成任务的大多数热门基准测试中均取得了卓越的性能。【图6、图7】<br><br>总之，团队首次系统地探讨了用于统一多模态建模的混合自回归和扩散体系结构，评估了三个关键方面：图像表征（CLIP与VAE特征）、训练目标（流量匹配与MSE）和训练策略（联合与顺序）<br><br>实验证明，CLIP嵌入与流匹配loss搭配使用，可以提高训练效率和输出质量。基于这些见解，他们推出了BLIP3-o，这是一个最先进的统一模型系列，使用60k指令调整数据集BLIP3o-60k进行了增强，大大提高了提示对齐和视觉美感。<br><br>为了方便未来的研究，他们完全开源了模型，包括代码、模型权重、训练脚本以及预训练和指导调整数据集。<br><br>目前正在积极开发统一模型的应用，包括迭代图像编辑、视觉对话和逐步视觉推理。<br><br>网页端可以免费体验Demo：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fblip3o.salesforceresearch.ai%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a> <br>更多详细内容，欢迎点击链接查看：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2F0Xcr4mQvJOVsWhGdOgrUbg" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">谢赛宁等推出统一多模态模型！替代VAE实现图像理解/生成双SOTA，代码权重数据集全开源</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1hf0ctkbij30po0yce81.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1hf0ackfuj30qk0ak427.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1hf0cfcjxj30nu0b4juf.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1hf0eb1usj30ma08677b.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1hf0ghj6rj30mw08m0vk.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1hf0jczx8j30mi09w77g.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1hf0lgoekj30n807ugo1.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

谢赛宁团队推出统一多模态模型Blip3-o，创新性地结合自回归和扩散Transformer架构，实现图像理解与生成双SOTA。该模型采用CLIP特征替代传统VAE，通过流匹配（Flow Matching）提升训练效率和生成质量。实验表明，CLIP+流匹配方案在提示对齐和视觉质量上表现最优。团队采用顺序训练策略（先理解后生成），保留语言能力的同时优化生成效果。模型在多个基准测试中领先，并开源代码、权重及60k指令数据集。当前正开发图像编辑、视觉对话等应用，Demo已开放体验。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-16T12:04:13Z
- **目录日期**: 2025-05-16
