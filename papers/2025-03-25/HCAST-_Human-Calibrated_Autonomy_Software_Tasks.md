# HCAST: Human-Calibrated Autonomy Software Tasks

**URL**: http://arxiv.org/abs/2503.17354v1

## 原始摘要

To understand and predict the societal impacts of highly autonomous AI
systems, we need benchmarks with grounding, i.e., metrics that directly connect
AI performance to real-world effects we care about. We present HCAST
(Human-Calibrated Autonomy Software Tasks), a benchmark of 189 machine learning
engineering, cybersecurity, software engineering, and general reasoning tasks.
We collect 563 human baselines (totaling over 1500 hours) from people skilled
in these domains, working under identical conditions as AI agents, which lets
us estimate that HCAST tasks take humans between one minute and 8+ hours.
Measuring the time tasks take for humans provides an intuitive metric for
evaluating AI capabilities, helping answer the question "can an agent be
trusted to complete a task that would take a human X hours?" We evaluate the
success rates of AI agents built on frontier foundation models, and we find
that current agents succeed 70-80% of the time on tasks that take humans less
than one hour, and less than 20% of the time on tasks that take humans more
than 4 hours.


## AI 摘要

研究人员开发了名为HCAST的基准测试，包含189项机器学习、网络安全、软件工程和通用推理任务，并收集了563名专业人士（总计1500+小时）的人类表现基线。通过比较AI与人类完成相同任务的时间，该研究量化了AI的可信度：当前前沿模型在人类1小时内完成的任务上成功率70-80%，但在人类需4小时以上的任务上成功率不足20%。这一基于时间指标的评估方法，为衡量AI自主系统的实际社会影响提供了直观标准，有助于回答"AI能否胜任人类需要X小时完成的任务"这一关键问题。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-25T00:01:28Z
- **目录日期**: 2025-03-25
