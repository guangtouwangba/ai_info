# Safety at Scale: A Comprehensive Survey of Large Model Safety

**URL**: http://arxiv.org/abs/2502.05206v4

## 原始摘要

The rapid advancement of large models, driven by their exceptional abilities
in learning and generalization through large-scale pre-training, has reshaped
the landscape of Artificial Intelligence (AI). These models are now
foundational to a wide range of applications, including conversational AI,
recommendation systems, autonomous driving, content generation, medical
diagnostics, and scientific discovery. However, their widespread deployment
also exposes them to significant safety risks, raising concerns about
robustness, reliability, and ethical implications. This survey provides a
systematic review of current safety research on large models, covering Vision
Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language
Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models
(DMs), and large-model-based Agents. Our contributions are summarized as
follows: (1) We present a comprehensive taxonomy of safety threats to these
models, including adversarial attacks, data poisoning, backdoor attacks,
jailbreak and prompt injection attacks, energy-latency attacks, data and model
extraction attacks, and emerging agent-specific threats. (2) We review defense
strategies proposed for each type of attacks if available and summarize the
commonly used datasets and benchmarks for safety research. (3) Building on
this, we identify and discuss the open challenges in large model safety,
emphasizing the need for comprehensive safety evaluations, scalable and
effective defense mechanisms, and sustainable data practices. More importantly,
we highlight the necessity of collective efforts from the research community
and international collaboration. Our work can serve as a useful reference for
researchers and practitioners, fostering the ongoing development of
comprehensive defense systems and platforms to safeguard AI models.


## AI 摘要

大型模型（如视觉基础模型、大语言模型、扩散模型等）通过大规模预训练展现出强大的学习和泛化能力，推动了AI领域的变革，广泛应用于对话、推荐、医疗等领域。然而，其广泛应用也带来了安全风险，包括对抗攻击、数据投毒、越狱攻击、能耗延迟攻击等威胁。本文系统梳理了各类模型的安全威胁、防御策略及常用数据集，并指出当前挑战：需建立全面的安全评估体系、可扩展的防御机制和可持续的数据实践。研究强调，保障AI安全需学术界和国际社会的共同努力。本文为相关研究与实践提供了重要参考。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T20:02:18Z
- **目录日期**: 2025-06-03
