# Neuroplastic Expansion in Deep Reinforcement Learning

**URL**: http://arxiv.org/abs/2410.07994v3

## 原始摘要

The loss of plasticity in learning agents, analogous to the solidification of
neural pathways in biological brains, significantly impedes learning and
adaptation in reinforcement learning due to its non-stationary nature. To
address this fundamental challenge, we propose a novel approach, {\it
Neuroplastic Expansion} (NE), inspired by cortical expansion in cognitive
science. NE maintains learnability and adaptability throughout the entire
training process by dynamically growing the network from a smaller initial size
to its full dimension. Our method is designed with three key components:
(\textit{1}) elastic topology generation based on potential gradients,
(\textit{2}) dormant neuron pruning to optimize network expressivity, and
(\textit{3}) neuron consolidation via experience review to strike a balance in
the plasticity-stability dilemma. Extensive experiments demonstrate that NE
effectively mitigates plasticity loss and outperforms state-of-the-art methods
across various tasks in MuJoCo and DeepMind Control Suite environments. NE
enables more adaptive learning in complex, dynamic environments, which
represents a crucial step towards transitioning deep reinforcement learning
from static, one-time training paradigms to more flexible, continually adapting
models.


## AI 摘要

本文提出了一种名为"神经可塑性扩展"(NE)的新方法，旨在解决强化学习中因神经通路固化导致的可塑性丧失问题。NE受认知科学中皮层扩展启发，通过动态扩展网络规模来保持学习适应性，包含三个关键组件：(1)基于潜在梯度的弹性拓扑生成，(2)休眠神经元剪枝优化网络表达能力，(3)通过经验回顾实现神经元整合以平衡可塑性-稳定性困境。实验表明，NE能有效缓解可塑性丧失，在MuJoCo和DeepMind Control Suite环境中优于现有方法，使深度强化学习从静态训练转向更灵活的持续适应模型迈出关键一步。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T10:02:34Z
- **目录日期**: 2025-06-03
