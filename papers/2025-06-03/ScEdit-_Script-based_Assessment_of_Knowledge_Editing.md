# ScEdit: Script-based Assessment of Knowledge Editing

**URL**: http://arxiv.org/abs/2505.23291v2

## 原始摘要

Knowledge Editing (KE) has gained increasing attention, yet current KE tasks
remain relatively simple. Under current evaluation frameworks, many editing
methods achieve exceptionally high scores, sometimes nearing perfection.
However, few studies integrate KE into real-world application scenarios (e.g.,
recent interest in LLM-as-agent). To support our analysis, we introduce a novel
script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --
which encompasses both counterfactual and temporal edits. We integrate
token-level and text-level evaluation methods, comprehensively analyzing
existing KE techniques. The benchmark extends traditional fact-based
("What"-type question) evaluation to action-based ("How"-type question)
evaluation. We observe that all KE methods exhibit a drop in performance on
established metrics and face challenges on text-level metrics, indicating a
challenging task. Our benchmark is available at
https://github.com/asdfo123/ScEdit.


## AI 摘要

知识编辑(KE)研究虽受关注，但现有评估任务过于简单，导致多数方法得分虚高。为验证KE在实际应用(如LLM智能体)中的效果，研究者提出基于脚本的评估基准ScEdit，涵盖反事实和时序编辑。该基准结合标记级和文本级评估，将传统事实性("What"类)问题扩展为行动导向("How"类)评估。实验表明，所有KE方法在现有指标上均表现下滑，文本级评估面临挑战，揭示了知识编辑的实际难度。基准已开源：https://github.com/asdfo123/ScEdit。（98字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T07:01:00Z
- **目录日期**: 2025-06-03
