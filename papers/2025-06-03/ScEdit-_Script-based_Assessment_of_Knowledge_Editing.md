# ScEdit: Script-based Assessment of Knowledge Editing

**URL**: http://arxiv.org/abs/2505.23291v2

## 原始摘要

Knowledge Editing (KE) has gained increasing attention, yet current KE tasks
remain relatively simple. Under current evaluation frameworks, many editing
methods achieve exceptionally high scores, sometimes nearing perfection.
However, few studies integrate KE into real-world application scenarios (e.g.,
recent interest in LLM-as-agent). To support our analysis, we introduce a novel
script-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --
which encompasses both counterfactual and temporal edits. We integrate
token-level and text-level evaluation methods, comprehensively analyzing
existing KE techniques. The benchmark extends traditional fact-based
("What"-type question) evaluation to action-based ("How"-type question)
evaluation. We observe that all KE methods exhibit a drop in performance on
established metrics and face challenges on text-level metrics, indicating a
challenging task. Our benchmark is available at
https://github.com/asdfo123/ScEdit.


## AI 摘要

该研究指出当前知识编辑（KE）任务过于简单，许多方法在评估中近乎完美，但缺乏实际应用验证。为此，作者提出了基于脚本的基准测试ScEdit，涵盖反事实和时间性编辑，结合词级和文本级评估方法，将传统事实型（"是什么"）问题扩展到行动型（"如何做"）问题。实验发现所有KE方法在现有指标上表现下降，文本级评估面临挑战，表明该任务具有较高难度。该基准测试已开源，旨在推动KE研究向更实用的方向发展。（99字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T09:01:19Z
- **目录日期**: 2025-06-03
