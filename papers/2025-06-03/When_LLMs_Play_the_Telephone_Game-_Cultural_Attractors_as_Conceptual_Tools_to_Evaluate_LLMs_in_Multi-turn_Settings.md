# When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings

**URL**: http://arxiv.org/abs/2407.04503v3

## 原始摘要

As large language models (LLMs) start interacting with each other and
generating an increasing amount of text online, it becomes crucial to better
understand how information is transformed as it passes from one LLM to the
next. While significant research has examined individual LLM behaviors,
existing studies have largely overlooked the collective behaviors and
information distortions arising from iterated LLM interactions. Small biases,
negligible at the single output level, risk being amplified in iterated
interactions, potentially leading the content to evolve towards attractor
states. In a series of telephone game experiments, we apply a transmission
chain design borrowed from the human cultural evolution literature: LLM agents
iteratively receive, produce, and transmit texts from the previous to the next
agent in the chain. By tracking the evolution of text toxicity, positivity,
difficulty, and length across transmission chains, we uncover the existence of
biases and attractors, and study their dependence on the initial text, the
instructions, language model, and model size. For instance, we find that more
open-ended instructions lead to stronger attraction effects compared to more
constrained tasks. We also find that different text properties display
different sensitivity to attraction effects, with toxicity leading to stronger
attractors than length. These findings highlight the importance of accounting
for multi-step transmission dynamics and represent a first step towards a more
comprehensive understanding of LLM cultural dynamics.


## AI 摘要

这篇论文研究了多个大语言模型(LLM)在信息传递链中的集体行为和信息失真现象。通过"电话游戏"实验设计，研究人员追踪文本在模型间传递时毒性、积极性、难度和长度的演变，发现初始文本、指令、模型类型和规模都会影响信息失真。研究发现开放性指令比约束性任务更容易产生吸引效应，且不同文本属性对吸引效应的敏感度不同(如毒性比长度更容易被放大)。这些发现揭示了多步信息传递动态的重要性，为理解LLM文化动态迈出了第一步。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T03:23:03Z
- **目录日期**: 2025-06-03
