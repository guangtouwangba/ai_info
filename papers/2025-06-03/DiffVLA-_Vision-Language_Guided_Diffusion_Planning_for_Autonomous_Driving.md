# DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving

**URL**: http://arxiv.org/abs/2505.19381v3

## 原始摘要

Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.


## AI 摘要

Diff-VLA是一种新型的混合稀疏-稠密扩散策略，结合视觉语言模型(VLM)来解决端到端自动驾驶中的挑战。该方法通过稀疏扩散表示实现高效多模态驾驶行为，并利用VLM输出的深度交互改进轨迹生成指导。相比现有方法，Diff-VLA避免了昂贵的鸟瞰图计算，提高了复杂场景下的决策能力。在2025年自动驾驶挑战赛中，该方法在真实和合成反应场景中表现优异，达到了45.0 PDMS的评分。这项研究展示了VLM在提升自动驾驶决策和轨迹规划方面的潜力，为端到端自动驾驶系统提供了新思路。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T12:01:21Z
- **目录日期**: 2025-06-03
