# DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving

**URL**: http://arxiv.org/abs/2505.19381v3

## 原始摘要

Research interest in end-to-end autonomous driving has surged owing to its
fully differentiable design integrating modular tasks, i.e. perception,
prediction and planing, which enables optimization in pursuit of the ultimate
goal. Despite the great potential of the end-to-end paradigm, existing methods
suffer from several aspects including expensive BEV (bird's eye view)
computation, action diversity, and sub-optimal decision in complex real-world
scenarios. To address these challenges, we propose a novel hybrid sparse-dense
diffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.
We explore the sparse diffusion representation for efficient multi-modal
driving behavior. Moreover, we rethink the effectiveness of VLM driving
decision and improve the trajectory generation guidance through deep
interaction across agent, map instances and VLM output. Our method shows
superior performance in Autonomous Grand Challenge 2025 which contains
challenging real and reactive synthetic scenarios. Our methods achieves 45.0
PDMS.


## AI 摘要

Diff-VLA是一种新型端到端自动驾驶方法，采用混合稀疏-密集扩散策略并结合视觉语言模型(VLM)，解决了现有方法计算成本高、行为多样性不足和复杂场景决策欠佳等问题。该方法利用稀疏扩散表示实现高效多模态驾驶行为，并通过智能体、地图实例与VLM输出的深度交互优化轨迹生成。在包含真实场景和反应式合成场景的Autonomous Grand Challenge 2025中表现优异，达到45.0 PDMS指标。该研究通过整合感知、预测和规划模块，展示了端到端范式在自动驾驶领域的巨大潜力。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T08:01:10Z
- **目录日期**: 2025-06-03
