# Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation

**URL**: http://arxiv.org/abs/2502.08826v3

## 原始摘要

Large Language Models (LLMs) suffer from hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information for improved factual grounding. With advances in multimodal
learning, Multimodal RAG extends this approach by incorporating multiple
modalities such as text, images, audio, and video to enhance the generated
outputs. However, cross-modal alignment and reasoning introduce unique
challenges beyond those in unimodal RAG. This survey offers a structured and
comprehensive analysis of Multimodal RAG systems, covering datasets,
benchmarks, metrics, evaluation, methodologies, and innovations in retrieval,
fusion, augmentation, and generation. We review training strategies, robustness
enhancements, loss functions, and agent-based approaches, while also exploring
the diverse Multimodal RAG scenarios. In addition, we outline open challenges
and future directions to guide research in this evolving field. This survey
lays the foundation for developing more capable and reliable AI systems that
effectively leverage multimodal dynamic external knowledge bases. All resources
are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.


## AI 摘要

这篇综述系统分析了多模态检索增强生成(Multimodal RAG)技术，该技术通过整合文本、图像、音频和视频等外部动态信息，解决大语言模型(LLMs)存在的幻觉和知识陈旧问题。文章全面梳理了多模态RAG的数据集、基准、评估指标、方法创新等关键要素，重点探讨了跨模态对齐、信息融合、训练策略、鲁棒性增强等核心挑战。相比单模态RAG，多模态RAG面临更复杂的跨模态推理难题。研究为开发更可靠的多模态AI系统奠定了基础，并指出了未来研究方向。相关资源已在GitHub开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T03:22:03Z
- **目录日期**: 2025-06-03
