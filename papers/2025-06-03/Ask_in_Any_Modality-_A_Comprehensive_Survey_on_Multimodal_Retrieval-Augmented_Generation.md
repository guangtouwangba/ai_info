# Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation

**URL**: http://arxiv.org/abs/2502.08826v3

## 原始摘要

Large Language Models (LLMs) suffer from hallucinations and outdated
knowledge due to their reliance on static training data. Retrieval-Augmented
Generation (RAG) mitigates these issues by integrating external dynamic
information for improved factual grounding. With advances in multimodal
learning, Multimodal RAG extends this approach by incorporating multiple
modalities such as text, images, audio, and video to enhance the generated
outputs. However, cross-modal alignment and reasoning introduce unique
challenges beyond those in unimodal RAG. This survey offers a structured and
comprehensive analysis of Multimodal RAG systems, covering datasets,
benchmarks, metrics, evaluation, methodologies, and innovations in retrieval,
fusion, augmentation, and generation. We review training strategies, robustness
enhancements, loss functions, and agent-based approaches, while also exploring
the diverse Multimodal RAG scenarios. In addition, we outline open challenges
and future directions to guide research in this evolving field. This survey
lays the foundation for developing more capable and reliable AI systems that
effectively leverage multimodal dynamic external knowledge bases. All resources
are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.


## AI 摘要

大型语言模型（LLMs）因依赖静态训练数据而存在幻觉和知识过时问题，检索增强生成（RAG）通过整合外部动态信息缓解了这些问题。随着多模态学习的进步，多模态RAG进一步融合文本、图像、音频和视频等多种模态以提升生成质量，但也面临跨模态对齐和推理的新挑战。本文综述了多模态RAG系统的数据集、基准、评估方法、技术框架（检索/融合/增强/生成）及训练策略，探讨了鲁棒性优化、损失函数和基于智能体的方法，并展望了未来研究方向，旨在推动更可靠的多模态动态知识库AI系统发展。资源已开源。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-03T04:05:14Z
- **目录日期**: 2025-06-03
