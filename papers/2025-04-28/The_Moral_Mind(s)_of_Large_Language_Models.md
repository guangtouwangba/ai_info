# The Moral Mind(s) of Large Language Models

**URL**: http://arxiv.org/abs/2412.04476v3

## 原始摘要

As large language models (LLMs) increasingly participate in tasks with
ethical and societal stakes, a critical question arises: do they exhibit an
emergent "moral mind" - a consistent structure of moral preferences guiding
their decisions - and to what extent is this structure shared across models? To
investigate this, we applied tools from revealed preference theory to nearly 40
leading LLMs, presenting each with many structured moral dilemmas spanning five
foundational dimensions of ethical reasoning. Using a probabilistic rationality
test, we found that at least one model from each major provider exhibited
behavior consistent with approximately stable moral preferences, acting as if
guided by an underlying utility function. We then estimated these utility
functions and found that most models cluster around neutral moral stances. To
further characterize heterogeneity, we employed a non-parametric permutation
approach, constructing a probabilistic similarity network based on revealed
preference patterns. The results reveal a shared core in LLMs' moral reasoning,
but also meaningful variation: some models show flexible reasoning across
perspectives, while others adhere to more rigid ethical profiles. These
findings provide a new empirical lens for evaluating moral consistency in LLMs
and offer a framework for benchmarking ethical alignment across AI systems.


## AI 摘要

该研究探讨了大语言模型（LLM）是否表现出稳定的"道德思维"，通过揭示偏好理论分析了近40个主流LLM在道德困境中的决策。研究发现，每个主要提供商的至少一个模型表现出近似稳定的道德偏好，其行为似乎由潜在效用函数驱动。多数模型倾向于中立道德立场，但也存在显著差异：部分模型能灵活切换推理视角，而另一些则坚持更刚性的伦理框架。研究揭示了LLM道德推理的共同核心及差异性，为评估AI系统的道德一致性和伦理对齐提供了新框架。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-28T17:02:18Z
- **目录日期**: 2025-04-28
