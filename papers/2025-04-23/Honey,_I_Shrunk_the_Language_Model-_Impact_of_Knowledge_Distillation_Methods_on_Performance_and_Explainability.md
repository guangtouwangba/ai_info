# Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability

**URL**: http://arxiv.org/abs/2504.16056v1

## 原始摘要

Artificial Intelligence (AI) has increasingly influenced modern society,
recently in particular through significant advancements in Large Language
Models (LLMs). However, high computational and storage demands of LLMs still
limit their deployment in resource-constrained environments. Knowledge
distillation addresses this challenge by training a small student model from a
larger teacher model. Previous research has introduced several distillation
methods for both generating training data and for training the student model.
Despite their relevance, the effects of state-of-the-art distillation methods
on model performance and explainability have not been thoroughly investigated
and compared. In this work, we enlarge the set of available methods by applying
critique-revision prompting to distillation for data generation and by
synthesizing existing methods for training. For these methods, we provide a
systematic comparison based on the widely used Commonsense Question-Answering
(CQA) dataset. While we measure performance via student model accuracy, we
employ a human-grounded study to evaluate explainability. We contribute new
distillation methods and their comparison in terms of both performance and
explainability. This should further advance the distillation of small language
models and, thus, contribute to broader applicability and faster diffusion of
LLM technology.


## AI 摘要

大型语言模型(LLMs)虽发展迅速，但其高计算和存储需求限制了在资源受限环境中的部署。知识蒸馏通过训练小型学生模型来应对这一挑战。本研究扩展了现有蒸馏方法，将批判性修正提示应用于训练数据生成，并综合现有方法进行模型训练。基于常识问答(CQA)数据集，我们系统比较了这些方法在模型性能和可解释性方面的表现：通过准确率评估性能，通过人类研究评估可解释性。研究贡献了新蒸馏方法及其比较结果，有助于推动小型语言模型的蒸馏技术，促进LLM技术的更广泛应用和快速扩散。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-23T09:02:11Z
- **目录日期**: 2025-04-23
