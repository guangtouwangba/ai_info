# Interpretable Locomotion Prediction in Construction Using a Memory-Driven LLM Agent With Chain-of-Thought Reasoning

**URL**: http://arxiv.org/abs/2504.15263v1

## 原始摘要

Construction tasks are inherently unpredictable, with dynamic environments
and safety-critical demands posing significant risks to workers. Exoskeletons
offer potential assistance but falter without accurate intent recognition
across diverse locomotion modes. This paper presents a locomotion prediction
agent leveraging Large Language Models (LLMs) augmented with memory systems,
aimed at improving exoskeleton assistance in such settings. Using multimodal
inputs - spoken commands and visual data from smart glasses - the agent
integrates a Perception Module, Short-Term Memory (STM), Long-Term Memory
(LTM), and Refinement Module to predict locomotion modes effectively.
Evaluation reveals a baseline weighted F1-score of 0.73 without memory, rising
to 0.81 with STM, and reaching 0.90 with both STM and LTM, excelling with vague
and safety-critical commands. Calibration metrics, including a Brier Score drop
from 0.244 to 0.090 and ECE from 0.222 to 0.044, affirm improved reliability.
This framework supports safer, high-level human-exoskeleton collaboration, with
promise for adaptive assistive systems in dynamic industries.


## AI 摘要

建筑作业环境动态且危险，工人面临高风险。外骨骼可提供辅助，但需准确识别不同运动模式。本文提出一种基于大语言模型（LLM）的运动预测智能体，结合记忆系统，通过语音指令和智能眼镜的视觉数据，整合感知模块、短期记忆（STM）、长期记忆（LTM）和优化模块来预测运动模式。实验显示，无记忆时加权F1分数为0.73，加入STM后提升至0.81，结合LTM后达0.90，尤其在模糊和安全关键指令中表现优异。校准指标（Brier分数从0.244降至0.090，ECE从0.222降至0.044）证实了其可靠性，为动态行业中的自适应辅助系统提供了安全高效的人机协作方案。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-23T01:28:37Z
- **目录日期**: 2025-04-23
