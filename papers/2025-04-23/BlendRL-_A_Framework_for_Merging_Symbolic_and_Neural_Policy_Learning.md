# BlendRL: A Framework for Merging Symbolic and Neural Policy Learning

**URL**: http://arxiv.org/abs/2410.11689v2

## 原始摘要

Humans can leverage both symbolic reasoning and intuitive reactions. In
contrast, reinforcement learning policies are typically encoded in either
opaque systems like neural networks or symbolic systems that rely on predefined
symbols and rules. This disjointed approach severely limits the agents'
capabilities, as they often lack either the flexible low-level reaction
characteristic of neural agents or the interpretable reasoning of symbolic
agents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL
framework that harmoniously integrates both paradigms within RL agents that use
mixtures of both logic and neural policies. We empirically demonstrate that
BlendRL agents outperform both neural and symbolic baselines in standard Atari
environments, and showcase their robustness to environmental changes.
Additionally, we analyze the interaction between neural and symbolic policies,
illustrating how their hybrid use helps agents overcome each other's
limitations.


## AI 摘要

BlendRL是一个新型的神经符号强化学习框架，将神经网络与符号逻辑策略有机结合，克服了传统方法中神经代理缺乏可解释性或符号代理缺乏灵活性的局限。实验表明，BlendRL在Atari游戏中表现优于纯神经或纯符号基线模型，且对环境变化更具鲁棒性。该框架通过混合使用两种策略，使智能体能相互弥补不足：神经网络提供快速反应能力，而符号系统赋予可解释的推理能力。这种协同作用展示了神经符号方法在强化学习中的潜力，为开发更强大、适应性更强的AI代理提供了新方向。（99字）

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-23T02:30:07Z
- **目录日期**: 2025-04-23
