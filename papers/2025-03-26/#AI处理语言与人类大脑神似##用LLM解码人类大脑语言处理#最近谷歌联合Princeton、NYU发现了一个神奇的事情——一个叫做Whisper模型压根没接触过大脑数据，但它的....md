# #AI处理语言与人类大脑神似##用LLM解码人类大脑语言处理#最近谷歌联合Princeton、NYU发现了一个神奇的事情——一个叫做Whisper模型压根没接触过大脑数据，但它的...

**URL**: https://weibo.com/6105753431/PklhScpip

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23AI%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%A4%A7%E8%84%91%E7%A5%9E%E4%BC%BC%23&amp;extparam=%23AI%E5%A4%84%E7%90%86%E8%AF%AD%E8%A8%80%E4%B8%8E%E4%BA%BA%E7%B1%BB%E5%A4%A7%E8%84%91%E7%A5%9E%E4%BC%BC%23" data-hide=""><span class="surl-text">#AI处理语言与人类大脑神似#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E7%94%A8LLM%E8%A7%A3%E7%A0%81%E4%BA%BA%E7%B1%BB%E5%A4%A7%E8%84%91%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%23&amp;extparam=%23%E7%94%A8LLM%E8%A7%A3%E7%A0%81%E4%BA%BA%E7%B1%BB%E5%A4%A7%E8%84%91%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%23" data-hide=""><span class="surl-text">#用LLM解码人类大脑语言处理#</span></a><br><br>最近谷歌联合Princeton、NYU发现了一个神奇的事情——<br><br>一个叫做Whisper模型压根没接触过大脑数据，但它的内部结构与大脑处理语音的路径惊人一致，仿佛天生就“懂人脑”！<br><br>以下是研究人员对齐实验的详细拆解，让我们看看为什么它们能神奇地对应上：<br><br>图1：我们听别人说话，比如一句“how are you doing”，大脑第一反应是声音区域（STG）先亮了，Whisper的语音embedding正好能精准预测这个阶段的大脑活动，说明模型对“声音信号”的捕捉和大脑是一致的。<br><br>图2：几百毫秒后，大脑进入理解语义的阶段，也就是IFG开始激活——这个区域主要处理词义、语法这些。而这个时候，Whisper的语言embedding也刚好上场了，负责建模词义、语法等结构信息。<br><br>图3：轮到自己开口，比如说“feeling fantastic”，大脑不是立刻动嘴，而是先在IFG里打腹稿（说啥先想好）。Whisper的语言embedding也在这一步生成，完全对得上。<br><br>图4：想好说啥之后，才轮到怎么说。大脑调动运动皮层（MC）准备发音动作，语音embedding跟上节奏。这一步就像把“内心语言”翻译成嘴巴能发出的指令。<br><br>图5：说完话还没完，大脑还会监听自己说出来的声音，确认有没有说错。听觉区STG又被激活了，进入“自我监听”模式，模型的语音embedding也继续跟进。<br><br>整体来看，大脑在说话时是IFG→MC→STG，听话时是STG→IFG，刚好一个顺着来，一个反着走。【图6】<br><br>更妙的是，Whisper的embedding在每一步都能准确预测大脑不同区域的活动变化，连时间点的激活顺序都对应上了。这种对齐程度，不能说“像”，是真的照着神经元画出来的。<br><br>研究人员都说，这不是巧合，而是AI和人脑在底层机制上有某种共通性，例如预测下一个词、用embedding表示上下文等处理方式，本质上都指向类似的计算逻辑。<br><br>这项发现可能会成为未来脑机接口，认知神经科学等领域的关键突破口。AI或许真的能成为人类思维的“翻译官”。<br><br>感兴趣的小伙伴可以点击：<a href="https://research.google/blog/deciphering-language-processing-in-the-human-brain-through-llm-representations/" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span> <span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1hzugbaxw5ej30zk0lb79m.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1hzugbd7vvdj31he0v2gv0.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1hzugbfcsj4j30zk0lcdkn.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1hzugbi23bmj30zk0kf0xs.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1hzugbipn53j30zk0kqwjs.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1hzugbm5jvyj30yq0jj79f.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

谷歌与普林斯顿、NYU联合研究发现，OpenAI的Whisper语音模型虽未经大脑数据训练，其处理语音的神经表征却与人类大脑高度吻合。研究表明：1）模型语音嵌入层能准确预测听觉皮层（STG）对声音信号的反应；2）语言嵌入层与语义理解区（IFG）的激活时序完全同步；3）语言生成时模型与大脑均遵循"概念构思→运动编码→听觉反馈"的相同流程。这种跨模态对齐暗示AI与大脑可能共享底层计算逻辑，为脑机接口和认知科学研究提供了新思路。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-26T09:04:55Z
- **目录日期**: 2025-03-26
