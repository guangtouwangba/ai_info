# A proposal for an incident regime that tracks and counters threats to national security posed by AI systems

**URL**: http://arxiv.org/abs/2503.19887v1

## 原始摘要

Recent progress in AI capabilities has heightened concerns that AI systems
could pose a threat to national security, for example, by making it easier for
malicious actors to perform cyberattacks on critical national infrastructure,
or through loss of control of autonomous AI systems. In parallel, federal
legislators in the US have proposed nascent 'AI incident regimes' to identify
and counter similar threats. In this paper, we consolidate these two trends and
present a proposal for a legally mandated post-deployment AI incident regie
that aims to counter potential national security threats from AI systems. We
start the paper by introducing the concept of 'security-critical' to describe
doctors that pose extreme risks to national security, before arguing that
'security-critical' describes civilian nuclear power, aviation, life science
dual-use research of concern, and frontier AI development. We then present in
detail our AI incident regime proposal,, justifying each component of the
proposal by demonstrating its similarity to US domestic incident regimes in
other 'security-critical' sectors. Finally, we sketch a hypothetical scenario
where our proposed AI incident regime deals with an AI cyber incident. Our
proposed AI incident regime is split into three phases. The first phase
revolves around a novel operationalization of what counts as an 'AI incident'
and we suggest that AI providers must create a 'national security case' before
deploying a frontier AI system. The second and third phases spell out that AI
providers should notify a government agency about incidents, and that the
government agency should be involved in amending AI providers' security and
safety procedures, in order to counter future threats to national security. Our
proposal is timely, given ongoing policy interest in the potential national
security threats posed by AI systems.


## AI 摘要

本文提出针对AI系统国家安全威胁的强制性事后监管框架。研究将核能、航空等"安全关键"领域概念扩展到前沿AI开发，建议AI部署前需提交"国家安全案例"，并建立三阶段监管机制：1)明确定义AI事件标准；2)要求企业向政府报告事件；3)政府参与修正企业安全规程。该提案借鉴美国现有行业监管经验，旨在应对AI可能导致的网络攻击或系统失控风险。作者通过假设性网络事件案例说明监管流程，强调当前政策环境下该提案的时效性，为平衡AI发展与国家安全提供制度解决方案。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-03-26T12:01:56Z
- **目录日期**: 2025-03-26
