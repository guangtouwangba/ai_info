# #Meta最强视觉感知编码器##Meta视觉编码器可识别海底魟鱼#在AI飞速发展的今天，如何打造出符合高阶智能期待的视觉编码器呢？Meta FAIR团队在上个月放了个大招，P...

**URL**: https://weibo.com/6105753431/PqJ3Df6PO

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23Meta%E6%9C%80%E5%BC%BA%E8%A7%86%E8%A7%89%E6%84%9F%E7%9F%A5%E7%BC%96%E7%A0%81%E5%99%A8%23&amp;extparam=%23Meta%E6%9C%80%E5%BC%BA%E8%A7%86%E8%A7%89%E6%84%9F%E7%9F%A5%E7%BC%96%E7%A0%81%E5%99%A8%23" data-hide=""><span class="surl-text">#Meta最强视觉感知编码器#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23Meta%E8%A7%86%E8%A7%89%E7%BC%96%E7%A0%81%E5%99%A8%E5%8F%AF%E8%AF%86%E5%88%AB%E6%B5%B7%E5%BA%95%E9%AD%9F%E9%B1%BC%23&amp;extparam=%23Meta%E8%A7%86%E8%A7%89%E7%BC%96%E7%A0%81%E5%99%A8%E5%8F%AF%E8%AF%86%E5%88%AB%E6%B5%B7%E5%BA%95%E9%AD%9F%E9%B1%BC%23" data-hide=""><span class="surl-text">#Meta视觉编码器可识别海底魟鱼#</span></a><br><br>在AI飞速发展的今天，如何打造出符合高阶智能期待的视觉编码器呢？<br><br>Meta FAIR团队在上个月放了个大招，Perception Encoder仅仅通过单一的对比学习目标，就成功在多个视觉任务中达成了出色表现，包括：<br><br>- 识别埋藏海底的魟鱼<br>- 判断物体间的空间遮挡关系<br>- 识别摄像机环绕物体的运动方向<br><br>要知道，以往的传统视觉编码器通常依赖多种预训练目标，每个任务都要专门训练，而PE仅用单一对比学习就生成了通用特征！<br><br>究竟是怎么做到的呢？在进行对比学习的过程中，PE进行了三个步骤的训练：<br><br>1、图像预训练：<br>  - 使用大规模的图像-文本对数据进行对比学习，训练视觉编码器和文本编码器。<br>  - 通过调整超参数（如学习率、批量大小、数据增强等），优化对比学习目标。<br>2、视频数据引擎：【图3】<br>- 使用预训练的视觉编码器作为帧编码器，生成高质量的合成视频描述文本。<br>- 通过对比学习，将视频帧的特征与合成文本的特征对齐。<br>3、视频微调：<br>  - 在图像预训练的基础上，使用合成视频描述文本对模型进行微调，进一步提升模型在视频任务上的性能。<br><br>不过，FAIR团队也发现，尽管对比学习能够产生强大的特征，但这些特征往往隐藏在模型的中间层，为了能直接将这些特征用于下游任务，团队使用了两种对齐方法：<br><br>- 语言对齐：通过一个中间阶段的训练，将PE的特征与预训练的语言模型连接起来<br>- 空间对齐：通过自对齐和利用Segment Anything Model (SAM) 的空间对应信息，将 PE的特征调整为更适合空间任务的形式。<br><br>PE的方法展示出了对比学习在多任务适应性方面的潜力，随着它逐步集成至各类创新应用中，智能视觉的未来值得期待。<br><br>项目主页：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fai.meta.com%2Fblog%2Fmeta-fair-updates-perception-localization-reasoning%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>开源数仓：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fgithub.com%2Ffacebookresearch%2Fperception_models" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>论文地址：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fai.meta.com%2Fresearch%2Fpublications%2Fperception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network%2F" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3ly1i16zjoyoonj30zk0k0gls.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i16zid1iaej30zk07q0z5.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i16zif6p7zj30k80c4dkf.jpg" referrerpolicy="no-referrer"><br><br><br clear="both"><div style="clear: both"></div><video controls="controls" poster="https://tvax3.sinaimg.cn/orj480/006Fd7o3ly1i16zjp1kzbj30zk0k0gls.jpg" style="width: 100%"><source src="https://f.video.weibocdn.com/o0/2DHu0JlIlx08o3FotxEY01041200a2sV0E010.mp4?label=mp4_720p&amp;template=1280x720.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1746671414&amp;ssig=qOD2BrV6AR&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/cpG66ld1lx08o3FnSbxe010412004CXC0E010.mp4?label=mp4_hd&amp;template=852x480.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1746671414&amp;ssig=f%2FDf4OBT2o&amp;KID=unistore,video"><source src="https://f.video.weibocdn.com/o0/ykSmwx7llx08o3FnRPa8010412002TZj0E010.mp4?label=mp4_ld&amp;template=640x360.25.0&amp;ori=0&amp;ps=1CwnkDw1GXwCQx&amp;Expires=1746671414&amp;ssig=YKkdc2L%2B%2F1&amp;KID=unistore,video"><p>视频无法显示，请前往<a href="https://video.weibo.com/show?fid=1034%3A5163696867967025" target="_blank" rel="noopener noreferrer">微博视频</a>观看。</p></video>

## AI 摘要

Meta FAIR团队开发的Perception Encoder（PE）通过单一对比学习目标实现了多任务视觉感知突破。该模型仅用对比学习预训练就能完成海底魟鱼识别、空间遮挡关系判断和环绕物体运动方向检测等复杂任务，无需传统方法的多目标训练。关键技术包括：图像-文本对比预训练、视频数据引擎生成合成文本对齐特征、视频微调三阶段训练，并通过语言/空间对齐优化特征可用性。研究证明对比学习能产生强大的中间层特征，为通用视觉编码器提供了新思路。项目已开源，相关论文和代码可通过Meta官网获取。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-08T01:31:29Z
- **目录日期**: 2025-05-08
