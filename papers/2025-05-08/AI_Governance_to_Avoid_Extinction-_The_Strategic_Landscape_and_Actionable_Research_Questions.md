# AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions

**URL**: http://arxiv.org/abs/2505.04592v1

## 原始摘要

Humanity appears to be on course to soon develop AI systems that
substantially outperform human experts in all cognitive domains and activities.
We believe the default trajectory has a high likelihood of catastrophe,
including human extinction. Risks come from failure to control powerful AI
systems, misuse of AI by malicious rogue actors, war between great powers, and
authoritarian lock-in. This research agenda has two aims: to describe the
strategic landscape of AI development and to catalog important governance
research questions. These questions, if answered, would provide important
insight on how to successfully reduce catastrophic risks.
  We describe four high-level scenarios for the geopolitical response to
advanced AI development, cataloging the research questions most relevant to
each. Our favored scenario involves building the technical, legal, and
institutional infrastructure required to internationally restrict dangerous AI
development and deployment (which we refer to as an Off Switch), which leads
into an internationally coordinated Halt on frontier AI activities at some
point in the future. The second scenario we describe is a US National Project
for AI, in which the US Government races to develop advanced AI systems and
establish unilateral control over global AI development. We also describe two
additional scenarios: a Light-Touch world similar to that of today and a Threat
of Sabotage situation where countries use sabotage and deterrence to slow AI
development.
  In our view, apart from the Off Switch and Halt scenario, all of these
trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent
action is needed from the US National Security community and AI governance
ecosystem to answer key research questions, build the capability to halt
dangerous AI activities, and prepare for international AI agreements.


## AI 摘要

这篇摘要警告，AI系统即将在所有认知领域超越人类专家，但默认发展路径可能导致灾难性风险，包括人类灭绝。风险来自失控的AI系统、恶意滥用、大国冲突和威权锁定。研究提出四种应对高级AI发展的地缘政治情景：1)国际限制危险AI的"关闭开关"方案（首选）；2)美国主导的"国家AI计划"；3)延续当前轻监管模式；4)通过破坏遏制AI发展的"破坏威胁"情景。除第一种方案外，其他路径都可能带来不可接受的风险。呼吁美国国家安全界和AI治理生态系统紧急行动，研究关键问题，为国际AI协议做准备。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-08T15:01:49Z
- **目录日期**: 2025-05-08
