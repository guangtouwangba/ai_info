# #AI竟然搞不清左右##大模型空间感有多差#杯子在我左边还是右边？这问题看起来简单，GPT-4o这样的视觉语言大模型（VLM）却经常答错。原因在于：这些模型只能从静...

**URL**: https://weibo.com/6105753431/PuEVNgKEj

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23AI%E7%AB%9F%E7%84%B6%E6%90%9E%E4%B8%8D%E6%B8%85%E5%B7%A6%E5%8F%B3%23&amp;extparam=%23AI%E7%AB%9F%E7%84%B6%E6%90%9E%E4%B8%8D%E6%B8%85%E5%B7%A6%E5%8F%B3%23" data-hide=""><span class="surl-text">#AI竟然搞不清左右#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A9%BA%E9%97%B4%E6%84%9F%E6%9C%89%E5%A4%9A%E5%B7%AE%23&amp;extparam=%23%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A9%BA%E9%97%B4%E6%84%9F%E6%9C%89%E5%A4%9A%E5%B7%AE%23" data-hide=""><span class="surl-text">#大模型空间感有多差#</span></a><br><br>杯子在我左边还是右边？这问题看起来简单，GPT-4o这样的视觉语言大模型（VLM）却经常答错。<br><br>原因在于：这些模型只能从静态图文中学习空间信息，视角单一，空间推理能力差。当问题涉及多角度判断时，它们就开始“宕机”。<br><br>为了测评这类弱点，浙江大学、电子科技大学和香港中文大学联合推出了首个评估VLM空间定位能力的基准体系——ViewSpatial-Bench。它设有五种任务，覆盖相机与人类两种视角，还有自动化的3D标注流水线，生成5700多个问答对。<br><br>测试发现，主流大模型普遍翻车。人物注视方向这种任务，准确率低到25%；即便GPT-4o，也只能勉强达到36%。不少模型还出现“人物视角表现优于相机视角”的反常现象，说明它们更擅长处理第三人称信息，却难以从镜头视角进行空间映射。 <a href="https://weibo.com/ttarticle/p/show?id=2309405173071836479545" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_article_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">5700问答对全面评估AI空间感！浙大＆成电＆港中文丨空间智能评测基准</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1i20w61uxb9j30rs0fmn1l.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

浙江大学等机构研究发现，当前视觉语言大模型(VLM)在空间定位能力上存在明显缺陷。通过新开发的ViewSpatial-Bench基准测试(包含5700多个问答对)发现，主流模型在判断左右方位等空间任务时表现不佳，如GPT-4o在人物注视方向判断上准确率仅36%。测试显示模型更擅长处理第三人称信息，而难以从相机视角进行准确空间映射。这表明现有AI系统主要依赖静态图文学习，缺乏多角度空间推理能力。该研究为评估和改进AI的空间感知能力提供了重要基准。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-06-02T19:02:45Z
- **目录日期**: 2025-06-02
