# Multi-Armed Bandits Meet Large Language Models

**URL**: http://arxiv.org/abs/2505.13355v1

## 原始摘要

Bandit algorithms and Large Language Models (LLMs) have emerged as powerful
tools in artificial intelligence, each addressing distinct yet complementary
challenges in decision-making and natural language processing. This survey
explores the synergistic potential between these two fields, highlighting how
bandit algorithms can enhance the performance of LLMs and how LLMs, in turn,
can provide novel insights for improving bandit-based decision-making. We first
examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt
engineering, and adaptive response generation, focusing on their ability to
balance exploration and exploitation in large-scale learning tasks.
Subsequently, we explore how LLMs can augment bandit algorithms through
advanced contextual understanding, dynamic adaptation, and improved policy
selection using natural language reasoning. By providing a comprehensive review
of existing research and identifying key challenges and opportunities, this
survey aims to bridge the gap between bandit algorithms and LLMs, paving the
way for innovative applications and interdisciplinary research in AI.


## AI 摘要

本文综述了多臂老虎机算法（Bandit）与大语言模型（LLM）的协同潜力。老虎机算法通过平衡探索与利用，可优化LLM的微调、提示工程和自适应响应生成；而LLM凭借自然语言理解能力，能为老虎机算法提供更优的上下文感知和策略选择。研究探讨了二者如何相互增强：老虎机提升LLM的大规模学习效率，LLM则改进老虎机的动态适应能力。通过梳理现有研究，本文旨在搭建两个领域的桥梁，为人工智能的跨学科创新应用指明方向，同时指出当前挑战与未来机遇。全文聚焦算法互补性，推动AI决策与语言处理的融合。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-20T14:01:49Z
- **目录日期**: 2025-05-20
