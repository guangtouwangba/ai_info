# #手机也能跑128K长文了##vivo新算法让长文本不再卡#在端侧设备上处理长文本常常面临计算和内存瓶颈。而vivo AI研究院推出的EdgeInfinite算法专为端侧设备设计，...

**URL**: https://weibo.com/6105753431/PsHnRqnhE

## 原始摘要

<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E6%89%8B%E6%9C%BA%E4%B9%9F%E8%83%BD%E8%B7%91128K%E9%95%BF%E6%96%87%E4%BA%86%23&amp;extparam=%23%E6%89%8B%E6%9C%BA%E4%B9%9F%E8%83%BD%E8%B7%91128K%E9%95%BF%E6%96%87%E4%BA%86%23" data-hide=""><span class="surl-text">#手机也能跑128K长文了#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23vivo%E6%96%B0%E7%AE%97%E6%B3%95%E8%AE%A9%E9%95%BF%E6%96%87%E6%9C%AC%E4%B8%8D%E5%86%8D%E5%8D%A1%23&amp;extparam=%23vivo%E6%96%B0%E7%AE%97%E6%B3%95%E8%AE%A9%E9%95%BF%E6%96%87%E6%9C%AC%E4%B8%8D%E5%86%8D%E5%8D%A1%23" data-hide=""><span class="surl-text">#vivo新算法让长文本不再卡#</span></a><br><br>在端侧设备上处理长文本常常面临计算和内存瓶颈。<br><br>而vivo AI研究院推出的EdgeInfinite算法专为端侧设备设计，让设备处理超长文本时更加高效流畅，该方法能够在不到10GB GPU内存的设备上处理长达128K tokens的输入。该研究成果已中稿ACL 2025。【图1】<br><br>以下是更多详细内容介绍。<br><br>EdgeInfinite：解决端侧设备长文本处理的高效算法<br><br>端侧LLM在实际应用中会遇到很多长文本输入的场景（例如通话摘要和个人文档总结），但由于端侧设备的资源限制，现有的LLM在部署到端侧后都无法处理很长的上下文。这是由于现在LLM都是基于Transformer架构，其计算耗时和内存占用会随着输入长度增加而显著增长，尤其当需要将Transformer类模型部署到端侧设备上时，面临的挑战会愈发突出。<br><br>为了解决这类问题，vivo AI研究院提出了一种用于端侧设备的长文本算法——EdgeInfinite，该算法通过一个可训练的门控记忆模块将记忆压缩算法集成到了Transformer架构中。本方法与原生的Transformer架构完全兼容，在训练时只需要微调一小部分参数，就可以在长文本任务上取得不错的效果，同时保持了相对高效的推理性能，非常适合在端侧设备上高效地处理长文本任务。<br><br>EdgeInfinite架构解析【图2】<br><br>EdgeInfinite的架构如上图所示，主要包括三个核心部分：结合ROPE的分块注意力模块、记忆压缩与解压缩模块和自适应的门控记忆模块。<br><br>1. 结合ROPE的分块注意力<br><br>EdgeInfinite会把输入的文本序列，按照一定长度切成小的片段。对每个片段分别计算 Q、K、V 值。同时，对每个片段会加入位置编码ROPE，让模型更好地理解小段内每个token之间的相对位置关系，这样在计算注意力的时候就更准确。<br><br>2. 记忆压缩与解压缩<br><br>EdgeInfinite引入了记忆的压缩与解压缩模块，将过去KV状态分块并存储为固定长度的记忆块，并在计算attention时候解压缩出来进行计算。由于记忆中编码了之前片段的KV对的关联，解压缩使我们能够计算当前Q状态和过去的KV状态之间的注意力。这个过程使得块状计算能够近似原始长序列的注意力计算。<br><br>3. 自适应的门控记忆模块<br><br>EdgeInfinite通过自适应的门控记忆模块将基于记忆的注意力与基于局部片段的注意力相结合，从而增强模型处理长距离依赖关系的能力。在训练时，EdgeInfinite只需要对记忆门控模块进行微调。<br><br>推理策略【图3】【图4】<br><br>EdgeInfinite在推理时采用了两个策略：<br><br>1.保留特定token的kv cache：在推理过程中的固定保留了两种特殊token的kv cache，即sink token（序列最开始的一些token）和window token（序列末尾的一些token），这些token对于保留语义和位置信息非常重要，需要将它们保留为未压缩状态来确保高质量的推理输出。<br><br>2.长短文本任务路由机制：EdgeInfinite可以灵活的和已有的基础模型相结合，来提升基础模型的长文本能力，同时也不影响基础模型的短文本能力，这是通过推理时动态的长短文本任务路由来实现的。<br><br>实验结果<br><br>研究人员使用vivo自研的 BlueLM-3B 作为基础模型，在 LongBench 这个包含多种长文本任务的数据集上测试 EdgeInfinite 的性能，同时将EdgeInfinite与三种KV Cache优化方法（SnapKV、PyramidKV和StreamingLLM）以及保留完整KV Cache的原始模型（FullKV）进行比较。【图5】<br><br>实验结果（如上图）显示，EdgeInfinite在多文档问答和少样本学习这些任务上，相比其他的方法有明显优势；在其中部分任务上还会优于原始的 FullKV 模型，整体模型效果还是很有竞争力的。【图6】<br><br>研究人员还做了消融实验来验证推理时保留特定token是很重要的。实验结果（如上图）表明，如果去掉sink token或者window token，推理效果会受到很大影响。【图7】<br><br>和原始的BlueLM-3B模型相比，EdgeInfinite在处理长文本输入时，首词出词时间更短，占用的内存也更少；即使输入文本长度增加，EdgeInfinite 的内存占用也保持在相对稳定的水平。<br><br>未来，EdgeInfinite有望在更多资源受限的设备上广泛应用，提升各类长文本处理任务的效率。比如在智能语音助手、移动办公文档处理等场景中，让用户获得更流畅的体验。<br><br>论文链接：<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Farxiv.org%2Fpdf%2F2503.22196" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax1.sinaimg.cn/large/006Fd7o3gy1i1lzhzdfocj30zk0bc0w4.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1lzhzx7h4j30zk0h1gq5.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1lzhz1ub2j30k00l3diu.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1lzhzzw1uj30k00vdtf4.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax2.sinaimg.cn/large/006Fd7o3gy1i1lzhzrt17j30zk0g50zt.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax4.sinaimg.cn/large/006Fd7o3gy1i1lzhyptyuj30mu0cmjve.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3gy1i1lzi01ywyj30zk0icn2b.jpg" referrerpolicy="no-referrer"><br><br>

## AI 摘要

vivo AI研究院提出的EdgeInfinite算法解决了端侧设备处理长文本的难题。该算法通过分块注意力、记忆压缩/解压缩和自适应门控模块，能在10GB内存设备上处理128K tokens的长文本，且兼容现有Transformer架构。实验显示，在BlueLM-3B模型上，其长文本任务性能优于主流优化方法，内存占用稳定，首词响应更快。该技术可提升手机等设备的长文本处理能力，适用于语音助手、文档总结等场景，相关论文已被ACL 2025收录。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-05-20T08:03:16Z
- **目录日期**: 2025-05-20
