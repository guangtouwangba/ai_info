# Embodied World Models Emerge from Navigational Task in Open-Ended Environments

**URL**: http://arxiv.org/abs/2504.11419v1

## 原始摘要

Understanding how artificial systems can develop spatial awareness and
reasoning has long been a challenge in AI research. Traditional models often
rely on passive observation, but embodied cognition theory suggests that deeper
understanding emerges from active interaction with the environment. This study
investigates whether neural networks can autonomously internalize spatial
concepts through interaction, focusing on planar navigation tasks. Using Gated
Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we
show that agents can learn to encode spatial properties like direction,
distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS)
to model the agent-environment interaction as a closed dynamical system,
revealing stable limit cycles that correspond to optimal navigation strategies.
Ridge Representation allows us to map navigation paths into a fixed-dimensional
behavioral space, enabling comparison with neural states. Canonical Correlation
Analysis (CCA) confirms strong alignment between these representations,
suggesting that the agent's neural states actively encode spatial knowledge.
Intervention experiments further show that specific neural dimensions are
causally linked to navigation performance. This work provides an approach to
bridging the gap between action and perception in AI, offering new insights
into building adaptive, interpretable models that can generalize across complex
environments. The causal validation of neural representations also opens new
avenues for understanding and controlling the internal mechanisms of AI
systems, pushing the boundaries of how machines learn and reason in dynamic,
real-world scenarios.


## AI 摘要

本研究探讨神经网络能否通过主动交互自主内化空间概念，聚焦平面导航任务。结合门控循环单元(GRU)和元强化学习(Meta-RL)，研究发现智能体可编码方向、距离和避障等空间属性。采用混合动力系统(HDS)建模交互过程，揭示了对应最优导航策略的稳定极限环。通过脊表示将路径映射到行为空间，典型相关分析(CCA)证实神经状态与空间表征高度一致。干预实验显示特定神经维度与导航性能存在因果关联。该研究为构建可解释的适应性模型提供了新思路，并开辟了理解AI系统内部机制的新途径。

## 元数据

- **来源**: ArXiv
- **类型**: 论文
- **保存时间**: 2025-04-16T21:02:35Z
- **目录日期**: 2025-04-16
