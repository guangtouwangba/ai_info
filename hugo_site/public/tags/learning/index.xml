<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Learning on AI研究资料库</title>
    <link>https://example.org/tags/learning/</link>
    <description>Recent content in Learning on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 13:02:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces</title>
      <link>https://example.org/papers/2025-03-20/hybrid-quantum-classical-reinforcement-learning-in/</link>
      <pubDate>Thu, 20 Mar 2025 13:02:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/hybrid-quantum-classical-reinforcement-learning-in/</guid>
      <description>量子机器学习的最新进展激发了利用量子强化学习方法解决经典控制问题的兴趣。然而，经典强化学习环境通常涉及高维问题空间，这对量子代理实现的有限且昂贵的资源构成了挑战。为解决这一维度挑战，本文提出了一种结合经典自编码器和量子代理的混合训练方法，通过联合学习压缩的观测表示来优化控制问题和量子处理单元架构的需求。通过一系列数值实验，展示了该方法在不同控制问题和光子（连续变量）及基于量子比特的代理中的性能，证明了联合训练对量子神经网络学习过程的改进效果。</description>
      <content:encoded><![CDATA[<h1 id="hybrid-quantum-classical-reinforcement-learning-in-latent-observation-spaces">Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2410.18284v3">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Recent progress in quantum machine learning has sparked interest in using
quantum methods to tackle classical control problems via quantum reinforcement
learning. However, the classical reinforcement learning environments often
scale to high dimensional problem spaces, which represents a challenge for the
limited and costly resources available for quantum agent implementations. We
propose to solve this dimensionality challenge by a classical autoencoder and a
quantum agent together, where a compressed representation of observations is
jointly learned in a hybrid training loop. The latent representation of such an
autoencoder will serve as a tailored observation space best suited for both the
control problem and the QPU architecture, aligning with the agent&rsquo;s
requirements. A series of numerical experiments are designed for a performance
analysis of the latent-space learning method. Results are presented for
different control problems and for both photonic (continuous-variable) and
qubit-based agents, to show how the QNN learning process is improved by the
joint training.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>量子机器学习的最新进展激发了利用量子强化学习方法解决经典控制问题的兴趣。然而，经典强化学习环境通常涉及高维问题空间，这对量子代理实现的有限且昂贵的资源构成了挑战。为解决这一维度挑战，本文提出了一种结合经典自编码器和量子代理的混合训练方法，通过联合学习压缩的观测表示来优化控制问题和量子处理单元架构的需求。通过一系列数值实验，展示了该方法在不同控制问题和光子（连续变量）及基于量子比特的代理中的性能，证明了联合训练对量子神经网络学习过程的改进效果。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T13:02:55+08:00</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Information Fusion in Smart Agriculture: Machine Learning Applications and Future Research Directions</title>
      <link>https://example.org/papers/2025-03-20/information-fusion-in-smart-agriculture-machine-le/</link>
      <pubDate>Thu, 20 Mar 2025 03:12:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/information-fusion-in-smart-agriculture-machine-le/</guid>
      <description>本文综述了机器学习（ML）在农业可持续发展与效率提升中的广泛应用，填补了现有研究在跨领域融合视角上的不足。研究围绕五大目标展开：分析ML在农业各阶段的应用、探讨ML与农业数据及数据融合的结合、进行文献计量与统计分析、考察AI驱动农业企业的实际案例，以及整理公开数据集以支持ML模型训练。通过多源数据融合（如遥感、物联网和气候分析），ML提升了精准农业的预测精度与决策能力。本文为研究者、行业专家及政策制定者提供了利用信息融合与ML推动精准农业发展的路线图。</description>
      <content:encoded><![CDATA[<h1 id="information-fusion-in-smart-agriculture-machine-learning-applications-and-future-research-directions">Information Fusion in Smart Agriculture: Machine Learning Applications and Future Research Directions</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2405.17465v2">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Machine learning (ML) is a rapidly evolving technology with expanding
applications across various fields. This paper presents a comprehensive survey
of recent ML applications in agriculture for sustainability and efficiency.
Existing reviews mainly focus on narrow subdomains or lack a fusion-driven
perspectives. This study provides a combined analysis of ML applications in
agriculture, structured around five key objectives: (i) Analyzing ML techniques
across pre-harvesting, harvesting, and post-harvesting phases. (ii)
Demonstrating how ML can be used with agricultural data and data fusion. (iii)
Conducting a bibliometric and statistical analysis to reveal research trends
and activity. (iv) Investigating real-world case studies of leading artificial
intelligence (AI)-driven agricultural companies that use different types of
multisensors and multisource data. (v) Compiling publicly available datasets to
support ML model training. Going beyond existing previous reviews, this review
focuses on how machine learning (ML) techniques, combined with multi-source
data fusion (integrating remote sensing, IoT, and climate analytics), enhance
precision agriculture by improving predictive accuracy and decision-making.
Case studies and statistical insights illustrate the evolving landscape of AI
driven smart farming, while future research directions also discusses
challenges associated with data fusion for heterogeneous datasets. This review
bridges the gap between AI research and agricultural applications, offering a
roadmap for researchers, industry professionals, and policymakers to harness
information fusion and ML for advancing precision agriculture.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文综述了机器学习（ML）在农业可持续发展与效率提升中的广泛应用，填补了现有研究在跨领域融合视角上的不足。研究围绕五大目标展开：分析ML在农业各阶段的应用、探讨ML与农业数据及数据融合的结合、进行文献计量与统计分析、考察AI驱动农业企业的实际案例，以及整理公开数据集以支持ML模型训练。通过多源数据融合（如遥感、物联网和气候分析），ML提升了精准农业的预测精度与决策能力。本文为研究者、行业专家及政策制定者提供了利用信息融合与ML推动精准农业发展的路线图。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:12:05Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Don&#39;t lie to your friends: Learning what you know from collaborative self-play</title>
      <link>https://example.org/papers/2025-03-20/dont-lie-to-your-friends-learning-what-you-know-fr/</link>
      <pubDate>Thu, 20 Mar 2025 03:10:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/dont-lie-to-your-friends-learning-what-you-know-fr/</guid>
      <description>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出对自身知识的元认知。该方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单个代理独立部署时也能改善工具使用和选择性预测的策略。</description>
      <content:encoded><![CDATA[<h1 id="dont-lie-to-your-friends-learning-what-you-know-from-collaborative-self-play">Don&rsquo;t lie to your friends: Learning what you know from collaborative self-play</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14481v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>To be helpful assistants, AI agents must be aware of their own capabilities
and limitations. This includes knowing when to answer from parametric knowledge
versus using tools, when to trust tool outputs, and when to abstain or hedge.
Such capabilities are hard to teach through supervised fine-tuning because they
require constructing examples that reflect the agent&rsquo;s specific capabilities.
We therefore propose a radically new approach to teaching agents what they
know: \emph{collaborative self-play}. We construct multi-agent collaborations
in which the group is rewarded for collectively arriving at correct answers.
The desired meta-knowledge emerges from the incentives built into the structure
of the interaction. We focus on small societies of agents that have access to
heterogeneous tools (corpus-specific retrieval), and therefore must collaborate
to maximize their success while minimizing their effort. Experiments show that
group-level rewards for multi-agent communities can induce policies that
\emph{transfer} to improve tool use and selective prediction in settings where
individual agents are deployed in isolation.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出对自身知识的元认知。该方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单个代理独立部署时也能改善工具使用和选择性预测的策略。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:10:46Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Don&#39;t lie to your friends: Learning what you know from collaborative self-play</title>
      <link>https://example.org/papers/2025-03-19/dont-lie-to-your-friends-learning-what-you-know-fr/</link>
      <pubDate>Wed, 19 Mar 2025 23:01:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-19/dont-lie-to-your-friends-learning-what-you-know-fr/</guid>
      <description>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出必要的元知识。这种方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单独部署时也能改善工具使用和选择性预测的策略。这种方法避免了监督微调中难以构建反映代理特定能力的示例的问题。</description>
      <content:encoded><![CDATA[<h1 id="dont-lie-to-your-friends-learning-what-you-know-from-collaborative-self-play">Don&rsquo;t lie to your friends: Learning what you know from collaborative self-play</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14481v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>To be helpful assistants, AI agents must be aware of their own capabilities
and limitations. This includes knowing when to answer from parametric knowledge
versus using tools, when to trust tool outputs, and when to abstain or hedge.
Such capabilities are hard to teach through supervised fine-tuning because they
require constructing examples that reflect the agent&rsquo;s specific capabilities.
We therefore propose a radically new approach to teaching agents what they
know: \emph{collaborative self-play}. We construct multi-agent collaborations
in which the group is rewarded for collectively arriving at correct answers.
The desired meta-knowledge emerges from the incentives built into the structure
of the interaction. We focus on small societies of agents that have access to
heterogeneous tools (corpus-specific retrieval), and therefore must collaborate
to maximize their success while minimizing their effort. Experiments show that
group-level rewards for multi-agent communities can induce policies that
\emph{transfer} to improve tool use and selective prediction in settings where
individual agents are deployed in isolation.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出必要的元知识。这种方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单独部署时也能改善工具使用和选择性预测的策略。这种方法避免了监督微调中难以构建反映代理特定能力的示例的问题。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-19T23:01:52Z</li>
<li><strong>目录日期</strong>: 2025-03-19</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
