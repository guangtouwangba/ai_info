<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Don&#39;t on AI研究资料库</title>
    <link>https://example.org/tags/dont/</link>
    <description>Recent content in Don&#39;t on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 03:10:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/dont/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Don&#39;t lie to your friends: Learning what you know from collaborative self-play</title>
      <link>https://example.org/papers/2025-03-20/dont-lie-to-your-friends-learning-what-you-know-fr/</link>
      <pubDate>Thu, 20 Mar 2025 03:10:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/dont-lie-to-your-friends-learning-what-you-know-fr/</guid>
      <description>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出对自身知识的元认知。该方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单个代理独立部署时也能改善工具使用和选择性预测的策略。</description>
      <content:encoded><![CDATA[<h1 id="dont-lie-to-your-friends-learning-what-you-know-from-collaborative-self-play">Don&rsquo;t lie to your friends: Learning what you know from collaborative self-play</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14481v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>To be helpful assistants, AI agents must be aware of their own capabilities
and limitations. This includes knowing when to answer from parametric knowledge
versus using tools, when to trust tool outputs, and when to abstain or hedge.
Such capabilities are hard to teach through supervised fine-tuning because they
require constructing examples that reflect the agent&rsquo;s specific capabilities.
We therefore propose a radically new approach to teaching agents what they
know: \emph{collaborative self-play}. We construct multi-agent collaborations
in which the group is rewarded for collectively arriving at correct answers.
The desired meta-knowledge emerges from the incentives built into the structure
of the interaction. We focus on small societies of agents that have access to
heterogeneous tools (corpus-specific retrieval), and therefore must collaborate
to maximize their success while minimizing their effort. Experiments show that
group-level rewards for multi-agent communities can induce policies that
\emph{transfer} to improve tool use and selective prediction in settings where
individual agents are deployed in isolation.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出对自身知识的元认知。该方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单个代理独立部署时也能改善工具使用和选择性预测的策略。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:10:46Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>Don&#39;t lie to your friends: Learning what you know from collaborative self-play</title>
      <link>https://example.org/papers/2025-03-19/dont-lie-to-your-friends-learning-what-you-know-fr/</link>
      <pubDate>Wed, 19 Mar 2025 23:01:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-19/dont-lie-to-your-friends-learning-what-you-know-fr/</guid>
      <description>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出必要的元知识。这种方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单独部署时也能改善工具使用和选择性预测的策略。这种方法避免了监督微调中难以构建反映代理特定能力的示例的问题。</description>
      <content:encoded><![CDATA[<h1 id="dont-lie-to-your-friends-learning-what-you-know-from-collaborative-self-play">Don&rsquo;t lie to your friends: Learning what you know from collaborative self-play</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14481v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>To be helpful assistants, AI agents must be aware of their own capabilities
and limitations. This includes knowing when to answer from parametric knowledge
versus using tools, when to trust tool outputs, and when to abstain or hedge.
Such capabilities are hard to teach through supervised fine-tuning because they
require constructing examples that reflect the agent&rsquo;s specific capabilities.
We therefore propose a radically new approach to teaching agents what they
know: \emph{collaborative self-play}. We construct multi-agent collaborations
in which the group is rewarded for collectively arriving at correct answers.
The desired meta-knowledge emerges from the incentives built into the structure
of the interaction. We focus on small societies of agents that have access to
heterogeneous tools (corpus-specific retrieval), and therefore must collaborate
to maximize their success while minimizing their effort. Experiments show that
group-level rewards for multi-agent communities can induce policies that
\emph{transfer} to improve tool use and selective prediction in settings where
individual agents are deployed in isolation.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出必要的元知识。这种方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单独部署时也能改善工具使用和选择性预测的策略。这种方法避免了监督微调中难以构建反映代理特定能力的示例的问题。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-19T23:01:52Z</li>
<li><strong>目录日期</strong>: 2025-03-19</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
