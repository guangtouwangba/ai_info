<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>LLM - AI研究资料库</title>
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <div class="header-content">
        <h1><a href="/">AI研究资料库</a></h1>
        <nav>
            
            <a href="/papers/">论文</a>
            
            <a href="/tags/">标签</a>
            
            <a href="/categories/">分类</a>
            
            <a href="/search/">搜索</a>
            
        </nav>
        
<div class="language-switcher">
    
    <a href="https://example.org/" class="active">
        中文
    </a>
    
    <a href="https://example.org/en/" class="">
        English
    </a>
    
</div>
 
    </div>
</header> 
    
    <main>
        
<section class="taxonomy-list">
    <h1>LLM</h1>
    <div class="papers-list">
        
        <article class="paper-item">
            <h3><a href="/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/">SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks</a></h3>
            <div class="paper-meta">
                <span class="date">2025-03-20</span>
            </div>
            <div class="summary">
                大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。
            </div>
            <a href="/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/" class="read-more">阅读更多</a>
        </article>
        
        <article class="paper-item">
            <h3><a href="/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/">PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play</a></h3>
            <div class="paper-meta">
                <span class="date">2025-03-20</span>
            </div>
            <div class="summary">
                为了解决大型语言模型（LLMs）在零样本工具使用中的挑战，研究者提出了PLAY2PROMPT框架。该框架通过自动“试错”探索工具的输入输出行为，无需标注数据即可优化工具文档并生成使用示例。这些示例不仅指导LLM推理，还作为验证进一步提升工具使用效果。实验表明，PLAY2PROMPT显著提升了开放和封闭模型在零样本任务中的表现，为领域特定工具集成提供了可扩展且有效的解决方案。
            </div>
            <a href="/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/" class="read-more">阅读更多</a>
        </article>
        
    </div>
</section>

    </main>
    
    <footer>
        <p>&copy; 2025 AI研究资料库</p>
    </footer>
</body>
</html> 