<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on AI研究资料库</title>
    <link>https://example.org/tags/llm/</link>
    <description>Recent content in LLM on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 13:01:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks</title>
      <link>https://example.org/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/</link>
      <pubDate>Thu, 20 Mar 2025 13:01:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/</guid>
      <description>大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。</description>
      <content:encoded><![CDATA[<h1 id="sweet-rl-training-multi-turn-llm-agents-on-collaborative-reasoning-tasks">SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.15478v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Large language model (LLM) agents need to perform multi-turn interactions in
real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM
agents fail to perform effective credit assignment over multiple turns while
leveraging the generalization capabilities of LLMs and it remains unclear how
to develop such algorithms. To study this, we first introduce a new benchmark,
ColBench, where an LLM agent interacts with a human collaborator over multiple
turns to solve realistic tasks in backend programming and frontend design.
Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with
Step-WisE Evaluation from Training-time information), that uses a carefully
designed optimization objective to train a critic model with access to
additional training-time information. The critic provides step-level rewards
for improving the policy model. Our experiments demonstrate that SWEET-RL
achieves a 6% absolute improvement in success and win rates on ColBench
compared to other state-of-the-art multi-turn RL algorithms, enabling
Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic
collaborative content creation.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T13:01:40+08:00</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play</title>
      <link>https://example.org/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/</link>
      <pubDate>Thu, 20 Mar 2025 03:11:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/</guid>
      <description>为了解决大型语言模型（LLMs）在零样本工具使用中的挑战，研究者提出了PLAY2PROMPT框架。该框架通过自动“试错”探索工具的输入输出行为，无需标注数据即可优化工具文档并生成使用示例。这些示例不仅指导LLM推理，还作为验证进一步提升工具使用效果。实验表明，PLAY2PROMPT显著提升了开放和封闭模型在零样本任务中的表现，为领域特定工具集成提供了可扩展且有效的解决方案。</description>
      <content:encoded><![CDATA[<h1 id="play2prompt-zero-shot-tool-instruction-optimization-for-llm-agents-via-tool-play">PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14432v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Large language models (LLMs) are increasingly integrated with specialized
external tools, yet many tasks demand zero-shot tool usage with minimal or
noisy documentation. Existing solutions rely on manual rewriting or labeled
data for validation, making them inapplicable in true zero-shot settings. To
address these challenges, we propose PLAY2PROMPT, an automated framework that
systematically &ldquo;plays&rdquo; with each tool to explore its input-output behaviors.
Through this iterative trial-and-error process, PLAY2PROMPT refines tool
documentation and generates usage examples without any labeled data. These
examples not only guide LLM inference but also serve as validation to further
enhance tool utilization. Extensive experiments on real-world tasks demonstrate
that PLAY2PROMPT significantly improves zero-shot tool performance across both
open and closed models, offering a scalable and effective solution for
domain-specific tool integration.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>为了解决大型语言模型（LLMs）在零样本工具使用中的挑战，研究者提出了PLAY2PROMPT框架。该框架通过自动“试错”探索工具的输入输出行为，无需标注数据即可优化工具文档并生成使用示例。这些示例不仅指导LLM推理，还作为验证进一步提升工具使用效果。实验表明，PLAY2PROMPT显著提升了开放和封闭模型在零样本任务中的表现，为领域特定工具集成提供了可扩展且有效的解决方案。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:11:13Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
