<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Agents - AI研究资料库</title>
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <div class="header-content">
        <h1><a href="/">AI研究资料库</a></h1>
        <nav>
            
            <a href="/papers/">论文</a>
            
            <a href="/tags/">标签</a>
            
            <a href="/categories/">分类</a>
            
            <a href="/search/">搜索</a>
            
        </nav>
        
<div class="language-switcher">
    
    <a href="https://example.org/" class="active">
        中文
    </a>
    
    <a href="https://example.org/en/" class="">
        English
    </a>
    
</div>
 
    </div>
</header> 
    
    <main>
        
<section class="taxonomy-list">
    <h1>Agents</h1>
    <div class="papers-list">
        
        <article class="paper-item">
            <h3><a href="/papers/2025-03-20/a-role-of-environmental-complexity-on-representati/">A Role of Environmental Complexity on Representation Learning in Deep Reinforcement Learning Agents</a></h3>
            <div class="paper-meta">
                <span class="date">2025-03-20</span>
            </div>
            <div class="summary">
                研究开发了一个模拟环境，用于训练深度强化学习代理在捷径使用导航任务中的表现，灵感来自人类导航者的双解决方案范式测试。研究发现，代理在封闭捷径试验中一旦开始学习，便能迅速达到最佳表现。高频率接触捷径的代理在捷径开放时导航速度和捷径使用更快。分析显示，频繁呈现的导航线索在初期能更好地编码于单个节点中，但更强的线索表示是通过导航规划中的使用形成的。空间表示在训练早期发展并稳定，而计划轨迹而非当前位置在代理网络中编码，且编码在群体而非单个节点水平上表示。
            </div>
            <a href="/papers/2025-03-20/a-role-of-environmental-complexity-on-representati/" class="read-more">阅读更多</a>
        </article>
        
        <article class="paper-item">
            <h3><a href="/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/">SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks</a></h3>
            <div class="paper-meta">
                <span class="date">2025-03-20</span>
            </div>
            <div class="summary">
                大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。
            </div>
            <a href="/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/" class="read-more">阅读更多</a>
        </article>
        
    </div>
</section>

    </main>
    
    <footer>
        <p>&copy; 2025 AI研究资料库</p>
    </footer>
</body>
</html> 