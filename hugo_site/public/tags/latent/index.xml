<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Latent on AI研究资料库</title>
    <link>https://example.org/tags/latent/</link>
    <description>Recent content in Latent on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 13:02:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/latent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces</title>
      <link>https://example.org/papers/2025-03-20/hybrid-quantum-classical-reinforcement-learning-in/</link>
      <pubDate>Thu, 20 Mar 2025 13:02:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/hybrid-quantum-classical-reinforcement-learning-in/</guid>
      <description>量子机器学习的最新进展激发了利用量子强化学习方法解决经典控制问题的兴趣。然而，经典强化学习环境通常涉及高维问题空间，这对量子代理实现的有限且昂贵的资源构成了挑战。为解决这一维度挑战，本文提出了一种结合经典自编码器和量子代理的混合训练方法，通过联合学习压缩的观测表示来优化控制问题和量子处理单元架构的需求。通过一系列数值实验，展示了该方法在不同控制问题和光子（连续变量）及基于量子比特的代理中的性能，证明了联合训练对量子神经网络学习过程的改进效果。</description>
      <content:encoded><![CDATA[<h1 id="hybrid-quantum-classical-reinforcement-learning-in-latent-observation-spaces">Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2410.18284v3">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Recent progress in quantum machine learning has sparked interest in using
quantum methods to tackle classical control problems via quantum reinforcement
learning. However, the classical reinforcement learning environments often
scale to high dimensional problem spaces, which represents a challenge for the
limited and costly resources available for quantum agent implementations. We
propose to solve this dimensionality challenge by a classical autoencoder and a
quantum agent together, where a compressed representation of observations is
jointly learned in a hybrid training loop. The latent representation of such an
autoencoder will serve as a tailored observation space best suited for both the
control problem and the QPU architecture, aligning with the agent&rsquo;s
requirements. A series of numerical experiments are designed for a performance
analysis of the latent-space learning method. Results are presented for
different control problems and for both photonic (continuous-variable) and
qubit-based agents, to show how the QNN learning process is improved by the
joint training.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>量子机器学习的最新进展激发了利用量子强化学习方法解决经典控制问题的兴趣。然而，经典强化学习环境通常涉及高维问题空间，这对量子代理实现的有限且昂贵的资源构成了挑战。为解决这一维度挑战，本文提出了一种结合经典自编码器和量子代理的混合训练方法，通过联合学习压缩的观测表示来优化控制问题和量子处理单元架构的需求。通过一系列数值实验，展示了该方法在不同控制问题和光子（连续变量）及基于量子比特的代理中的性能，证明了联合训练对量子神经网络学习过程的改进效果。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T13:02:55+08:00</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision 网页链接本研究提出了一种名为LatentSync的端到端唇同...</title>
      <link>https://example.org/papers/2025-03-20/latentsync-taming-audio-conditioned-latent-diffusi/</link>
      <pubDate>Thu, 20 Mar 2025 04:03:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/latentsync-taming-audio-conditioned-latent-diffusi/</guid>
      <description>本研究提出了LatentSync，一种基于音频条件的潜在扩散模型的端到端唇同步框架，无需中间运动表示，直接建模音频与视觉的复杂关系。为解决扩散过程中时间一致性问题，提出了时间表示对齐（TREPA）方法，利用自监督视频模型增强生成帧与真实帧的时间一致性。研究还优化了SyncNet的训练框架，提升了其准确性。实验表明，该方法在HDTF和VoxCeleb2数据集上超越了现有唇同步方法，为音频驱动的人像动画提供了新的解决方案。</description>
      <content:encoded><![CDATA[<h1 id="latentsync-taming-audio-conditioned-latent-diffusion-models-for-lip-sync-with-syncnet-supervision-网页链接本研究提出了一种名为latentsync的端到端唇同">LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision 网页链接本研究提出了一种名为LatentSync的端到端唇同&hellip;</h1>
<p><strong>原始链接</strong>: <a href="https://weibo.com/1870858943/Pjot9eGs2">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision <a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fwww.aminer.cn%2Fpub%2F675ba34bae8580e7ff21df01%2Flatentsync-taming-audio-conditioned-latent-diffusion-models-for-lip-sync-with-syncnet" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>本研究提出了一种名为LatentSync的端到端唇同步框架，基于音频条件的潜在扩散模型，无需中间运动表示，与先前的基于像素空间扩散或两阶段生成的唇同步方法不同。该框架能够利用Stable Diffusion的强大能力，直接建模复杂的音频视觉相关性。研究中发现，基于扩散的唇同步方法在不同帧的扩散过程中存在时间一致性不足的问题。为此，研究提出了时间表示对齐（TREPA）方法以增强时间一致性，同时保持唇同步的准确性。TREPA利用大规模自监督视频模型提取的时间表示来对齐生成的帧与真实帧。此外，研究还观察到了SyncNet收敛问题，并进行了全面的实证研究，识别了影响SyncNet收敛的关键因素，包括模型架构、训练超参数和数据预处理方法。通过改变SyncNet的整体训练框架，显著提高了SyncNet的准确性。这些经验也可应用于其他使用SyncNet的唇同步和音频驱动的人像动画方法。基于这些创新，该方法在HDTF和VoxCeleb2数据集上的多个指标上超越了现有的唇同步方法。<a href="https://m.weibo.cn/p/index?extparam=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;containerid=100808f068f0dad74789bee210163c40a4b50d" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://n.sinaimg.cn/photo/5213b46e/20180926/timeline_card_small_super_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">人工智能</span></a><a href="https://m.weibo.cn/p/index?extparam=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;containerid=1008082dc9b4e036056e2a00e5499db67ddd30" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://n.sinaimg.cn/photo/5213b46e/20180926/timeline_card_small_super_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">大模型</span></a><a href="https://m.weibo.cn/p/index?extparam=%E7%A1%95%E5%A3%AB%E8%AE%BA%E6%96%87&amp;containerid=1008084cacf38f5903dc7b04550404d0bd3608" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://n.sinaimg.cn/photo/5213b46e/20180926/timeline_card_small_super_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">硕士论文</span></a><img style="" src="https://tvax2.sinaimg.cn/large/6f830abfly1hzn8lwp5a6j21n90z1b29.jpg" referrerpolicy="no-referrer"><br><br></p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本研究提出了LatentSync，一种基于音频条件的潜在扩散模型的端到端唇同步框架，无需中间运动表示，直接建模音频与视觉的复杂关系。为解决扩散过程中时间一致性问题，提出了时间表示对齐（TREPA）方法，利用自监督视频模型增强生成帧与真实帧的时间一致性。研究还优化了SyncNet的训练框架，提升了其准确性。实验表明，该方法在HDTF和VoxCeleb2数据集上超越了现有唇同步方法，为音频驱动的人像动画提供了新的解决方案。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T04:03:43Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
