<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Long on AI研究资料库</title>
    <link>https://example.org/tags/long/</link>
    <description>Recent content in Long on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 03:09:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/long/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Measuring AI Ability to Complete Long Tasks</title>
      <link>https://example.org/papers/2025-03-20/measuring-ai-ability-to-complete-long-tasks/</link>
      <pubDate>Thu, 20 Mar 2025 03:09:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/measuring-ai-ability-to-complete-long-tasks/</guid>
      <description>本文提出了一种新指标“50%-任务完成时间范围”，用于量化AI系统在人类能力方面的表现。该指标指人类完成AI模型能以50%成功率完成的任务所需时间。研究发现，当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间范围约为50分钟，且自2019年以来每7个月翻倍，2024年可能加速。AI能力的提升主要源于更高的可靠性、错误适应能力、逻辑推理和工具使用能力。若趋势持续，5年内AI可能自动化许多目前人类需一个月完成的软件任务，但也带来潜在危险能力增加的担忧。</description>
      <content:encoded><![CDATA[<h1 id="measuring-ai-ability-to-complete-long-tasks">Measuring AI Ability to Complete Long Tasks</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14499v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Despite rapid progress on AI benchmarks, the real-world meaning of benchmark
performance remains unclear. To quantify the capabilities of AI systems in
terms of human capabilities, we propose a new metric: 50%-task-completion time
horizon. This is the time humans typically take to complete tasks that AI
models can complete with 50% success rate. We first timed humans with relevant
domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter
tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet
have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time
horizon has been doubling approximately every seven months since 2019, though
the trend may have accelerated in 2024. The increase in AI models&rsquo; time
horizons seems to be primarily driven by greater reliability and ability to
adapt to mistakes, combined with better logical reasoning and tool use
capabilities. We discuss the limitations of our results &ndash; including their
degree of external validity &ndash; and the implications of increased autonomy for
dangerous capabilities. If these results generalize to real-world software
tasks, extrapolation of this trend predicts that within 5 years, AI systems
will be capable of automating many software tasks that currently take humans a
month.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文提出了一种新指标“50%-任务完成时间范围”，用于量化AI系统在人类能力方面的表现。该指标指人类完成AI模型能以50%成功率完成的任务所需时间。研究发现，当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间范围约为50分钟，且自2019年以来每7个月翻倍，2024年可能加速。AI能力的提升主要源于更高的可靠性、错误适应能力、逻辑推理和工具使用能力。若趋势持续，5年内AI可能自动化许多目前人类需一个月完成的软件任务，但也带来潜在危险能力增加的担忧。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:09:45Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning</title>
      <link>https://example.org/papers/2025-03-19/videomind-a-chain-of-lora-agent-for-long-video-rea/</link>
      <pubDate>Wed, 19 Mar 2025 09:16:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-19/videomind-a-chain-of-lora-agent-for-long-video-rea/</guid>
      <description>这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。</description>
      <content:encoded><![CDATA[<h1 id="videomind-a-chain-of-lora-agent-for-long-video-reasoning">VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.13444v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Videos, with their unique temporal dimension, demand precise grounded
understanding, where answers are directly linked to visual, interpretable
evidence. Despite significant breakthroughs in reasoning capabilities within
Large Language Models, multi-modal reasoning - especially for videos - remains
unexplored. In this work, we introduce VideoMind, a novel video-language agent
designed for temporal-grounded video understanding. VideoMind incorporates two
key innovations: (i) We identify essential capabilities for video temporal
reasoning and develop a role-based agentic workflow, including a planner for
coordinating different roles, a grounder for temporal localization, a verifier
to assess temporal interval accuracy, and an answerer for question-answering.
(ii) To efficiently integrate these diverse roles, we propose a novel
Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA
adaptors while avoiding the overhead of multiple models, thus balancing
efficiency and flexibility. Extensive experiments on 14 public benchmarks
demonstrate that our agent achieves state-of-the-art performance on diverse
video understanding tasks, including 3 on grounded video question-answering, 6
on video temporal grounding, and 5 on general video question-answering,
underscoring its effectiveness in advancing video agent and long-form temporal
reasoning.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-19T09:16:53+08:00</li>
<li><strong>目录日期</strong>: 2025-03-19</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
