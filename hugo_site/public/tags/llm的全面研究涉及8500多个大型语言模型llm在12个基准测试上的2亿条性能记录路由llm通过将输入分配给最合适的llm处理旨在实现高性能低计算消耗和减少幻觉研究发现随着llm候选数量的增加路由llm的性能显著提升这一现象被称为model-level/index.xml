<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-Level on AI研究资料库</title>
    <link>https://example.org/tags/llm%E7%9A%84%E5%85%A8%E9%9D%A2%E7%A0%94%E7%A9%B6%E6%B6%89%E5%8F%8A8500%E5%A4%9A%E4%B8%AA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8Bllm%E5%9C%A812%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E4%B8%8A%E7%9A%842%E4%BA%BF%E6%9D%A1%E6%80%A7%E8%83%BD%E8%AE%B0%E5%BD%95%E8%B7%AF%E7%94%B1llm%E9%80%9A%E8%BF%87%E5%B0%86%E8%BE%93%E5%85%A5%E5%88%86%E9%85%8D%E7%BB%99%E6%9C%80%E5%90%88%E9%80%82%E7%9A%84llm%E5%A4%84%E7%90%86%E6%97%A8%E5%9C%A8%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E4%BD%8E%E8%AE%A1%E7%AE%97%E6%B6%88%E8%80%97%E5%92%8C%E5%87%8F%E5%B0%91%E5%B9%BB%E8%A7%89%E7%A0%94%E7%A9%B6%E5%8F%91%E7%8E%B0%E9%9A%8F%E7%9D%80llm%E5%80%99%E9%80%89%E6%95%B0%E9%87%8F%E7%9A%84%E5%A2%9E%E5%8A%A0%E8%B7%AF%E7%94%B1llm%E7%9A%84%E6%80%A7%E8%83%BD%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%E8%BF%99%E4%B8%80%E7%8E%B0%E8%B1%A1%E8%A2%AB%E7%A7%B0%E4%B8%BAmodel-level/</link>
    <description>Recent content in LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-Level on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 19 Mar 2025 23:04:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/llm%E7%9A%84%E5%85%A8%E9%9D%A2%E7%A0%94%E7%A9%B6%E6%B6%89%E5%8F%8A8500%E5%A4%9A%E4%B8%AA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8Bllm%E5%9C%A812%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E4%B8%8A%E7%9A%842%E4%BA%BF%E6%9D%A1%E6%80%A7%E8%83%BD%E8%AE%B0%E5%BD%95%E8%B7%AF%E7%94%B1llm%E9%80%9A%E8%BF%87%E5%B0%86%E8%BE%93%E5%85%A5%E5%88%86%E9%85%8D%E7%BB%99%E6%9C%80%E5%90%88%E9%80%82%E7%9A%84llm%E5%A4%84%E7%90%86%E6%97%A8%E5%9C%A8%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E4%BD%8E%E8%AE%A1%E7%AE%97%E6%B6%88%E8%80%97%E5%92%8C%E5%87%8F%E5%B0%91%E5%B9%BB%E8%A7%89%E7%A0%94%E7%A9%B6%E5%8F%91%E7%8E%B0%E9%9A%8F%E7%9D%80llm%E5%80%99%E9%80%89%E6%95%B0%E9%87%8F%E7%9A%84%E5%A2%9E%E5%8A%A0%E8%B7%AF%E7%94%B1llm%E7%9A%84%E6%80%A7%E8%83%BD%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%E8%BF%99%E4%B8%80%E7%8E%B0%E8%B1%A1%E8%A2%AB%E7%A7%B0%E4%B8%BAmodel-level/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，...</title>
      <link>https://example.org/papers/2025-03-19/llmscaling-up-llmrouting-llm8500llm/</link>
      <pubDate>Wed, 19 Mar 2025 23:04:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-19/llmscaling-up-llmrouting-llm8500llm/</guid>
      <description>中山大学和普渡大学的研究团队进行了一项关于路由LLM（Routing LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-level Scaling Up”。此外，团队开发了RouterEval评测工具，使其他研究人员能在有限计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。</description>
      <content:encoded><![CDATA[<h1 id="路由llm最全面探索笔记本也能玩的大模型scaling-up研究-事关路由llmrouting-llm一项截至目前最全面的研究来了共计收集和整理了涉及8500个llm">#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，&hellip;</h1>
<p><strong>原始链接</strong>: <a href="https://weibo.com/6105753431/Pjgszzzde">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%B7%AF%E7%94%B1LLM%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23&amp;extparam=%23%E8%B7%AF%E7%94%B1LLM%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23" data-hide=""><span class="surl-text">#路由LLM最全面探索#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BScaling+Up%E7%A0%94%E7%A9%B6%23&amp;extparam=%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BScaling+Up%E7%A0%94%E7%A9%B6%23" data-hide=""><span class="surl-text">#笔记本也能玩的大模型Scaling Up研究#</span></a> <br><br>事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——<br><br>共计收集和整理了涉及8500+个LLM，在12个Benchmark上的共2亿条性能记录！<br><br>先来简单科普一下路由LLM。<br><br>这种方法主要是把像ChatGPT、Qwen、DeepSeek这些成型的LLM当作 “专家” ，当给一个输入的时候，有分类能力的Router（路由器）就会把这个输入分配给合适的LLM处理。<br><br>如此一来，就能实现高性能、低计算消耗、低幻觉等目标。<br><br>而来自中山大学和普渡大学的研究人员在基于上述海量的记录做了一番探索之后，发现了一个现象，叫做Model-level Scaling Up。<br><br>一言蔽之，就是一个好的Router，可以让路由LLM范式的性能随着LLM候选数量的增加迅速变强。<br><br>随后，他们通过这些数据构建了针对Router设计的评测RouterEval。<br><br>值得注意的是，其他研究人员，也可以通过RouterEval在很少的计算资源下（如笔记本、单卡GPU上）就能参与到该路由LLM的研究当中。<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FOQFUcemTEmGC0eKUO_Fuiw" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1hzm99rc631j30u0091dit.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1hzm9aggvavj311w0juwlw.jpg" referrerpolicy="no-referrer"><br><br></p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>中山大学和普渡大学的研究团队进行了一项关于路由LLM（Routing LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-level Scaling Up”。此外，团队开发了RouterEval评测工具，使其他研究人员能在有限计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-19T23:04:28Z</li>
<li><strong>目录日期</strong>: 2025-03-19</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
