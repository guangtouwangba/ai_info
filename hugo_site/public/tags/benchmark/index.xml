<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Benchmark on AI研究资料库</title>
    <link>https://example.org/tags/benchmark/</link>
    <description>Recent content in Benchmark on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 03:11:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/benchmark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>EnvBench: A Benchmark for Automated Environment Setup</title>
      <link>https://example.org/papers/2025-03-20/envbench-a-benchmark-for-automated-environment-set/</link>
      <pubDate>Thu, 20 Mar 2025 03:11:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/envbench-a-benchmark-for-automated-environment-set/</guid>
      <description>近期大型语言模型（LLMs）的进展使得研究者能够专注于软件工程领域的实际仓库级任务。本文提出EnvBench，一个全面的环境设置基准，涵盖329个Python和665个基于JVM（Java、Kotlin）的仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括一个简单的零样本基线和两个代理工作流，使用GPT-4o和GPT-4o-mini进行测试，最佳方法成功配置了6.69%的Python仓库和29.47%的JVM仓库，表明EnvBench对当前方法仍具挑战性。</description>
      <content:encoded><![CDATA[<h1 id="envbench-a-benchmark-for-automated-environment-setup">EnvBench: A Benchmark for Automated Environment Setup</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14443v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Recent advances in Large Language Models (LLMs) have enabled researchers to
focus on practical repository-level tasks in software engineering domain. In
this work, we consider a cornerstone task for automating work with software
repositories-environment setup, i.e., a task of configuring a
repository-specific development environment on a system. Existing studies on
environment setup introduce innovative agentic strategies, but their evaluation
is often based on small datasets that may not capture the full range of
configuration challenges encountered in practice. To address this gap, we
introduce a comprehensive environment setup benchmark EnvBench. It encompasses
329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on
repositories that present genuine configuration challenges, excluding projects
that can be fully configured by simple deterministic scripts. To enable further
benchmark extension and usage for model tuning, we implement two automatic
metrics: a static analysis check for missing imports in Python and a
compilation check for JVM languages. We demonstrate the applicability of our
benchmark by evaluating three environment setup approaches, including a simple
zero-shot baseline and two agentic workflows, that we test with two powerful
LLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to
successfully configure 6.69% repositories for Python and 29.47% repositories
for JVM, suggesting that EnvBench remains challenging for current approaches.
Our benchmark suite is publicly available at
<a href="https://github.com/JetBrains-Research/EnvBench">https://github.com/JetBrains-Research/EnvBench</a>. The dataset and experiment
trajectories are available at <a href="https://jb.gg/envbench">https://jb.gg/envbench</a>.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>近期大型语言模型（LLMs）的进展使得研究者能够专注于软件工程领域的实际仓库级任务。本文提出EnvBench，一个全面的环境设置基准，涵盖329个Python和665个基于JVM（Java、Kotlin）的仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括一个简单的零样本基线和两个代理工作流，使用GPT-4o和GPT-4o-mini进行测试，最佳方法成功配置了6.69%的Python仓库和29.47%的JVM仓库，表明EnvBench对当前方法仍具挑战性。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:11:01Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>EnvBench: A Benchmark for Automated Environment Setup</title>
      <link>https://example.org/papers/2025-03-19/envbench-a-benchmark-for-automated-environment-set/</link>
      <pubDate>Wed, 19 Mar 2025 23:02:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-19/envbench-a-benchmark-for-automated-environment-set/</guid>
      <description>近期大型语言模型（LLMs）的进展促使研究者关注软件工程领域的实际仓库级任务。本文聚焦于自动化软件仓库环境设置任务，提出了一个全面的环境设置基准EnvBench，涵盖329个Python和665个JVM（Java、Kotlin）仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括简单的零样本基线和两种代理工作流，使用GPT-4o和GPT-4o-mini进行测试，结果表明EnvBench对当前方法仍具挑战性。基准套件公开于https://github.com/JetBrains-Research/EnvBench。</description>
      <content:encoded><![CDATA[<h1 id="envbench-a-benchmark-for-automated-environment-setup">EnvBench: A Benchmark for Automated Environment Setup</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.14443v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Recent advances in Large Language Models (LLMs) have enabled researchers to
focus on practical repository-level tasks in software engineering domain. In
this work, we consider a cornerstone task for automating work with software
repositories-environment setup, i.e., a task of configuring a
repository-specific development environment on a system. Existing studies on
environment setup introduce innovative agentic strategies, but their evaluation
is often based on small datasets that may not capture the full range of
configuration challenges encountered in practice. To address this gap, we
introduce a comprehensive environment setup benchmark EnvBench. It encompasses
329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on
repositories that present genuine configuration challenges, excluding projects
that can be fully configured by simple deterministic scripts. To enable further
benchmark extension and usage for model tuning, we implement two automatic
metrics: a static analysis check for missing imports in Python and a
compilation check for JVM languages. We demonstrate the applicability of our
benchmark by evaluating three environment setup approaches, including a simple
zero-shot baseline and two agentic workflows, that we test with two powerful
LLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to
successfully configure 6.69% repositories for Python and 29.47% repositories
for JVM, suggesting that EnvBench remains challenging for current approaches.
Our benchmark suite is publicly available at
<a href="https://github.com/JetBrains-Research/EnvBench">https://github.com/JetBrains-Research/EnvBench</a>. The dataset and experiment
trajectories are available at <a href="https://jb.gg/envbench">https://jb.gg/envbench</a>.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>近期大型语言模型（LLMs）的进展促使研究者关注软件工程领域的实际仓库级任务。本文聚焦于自动化软件仓库环境设置任务，提出了一个全面的环境设置基准EnvBench，涵盖329个Python和665个JVM（Java、Kotlin）仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括简单的零样本基线和两种代理工作流，使用GPT-4o和GPT-4o-mini进行测试，结果表明EnvBench对当前方法仍具挑战性。基准套件公开于https://github.com/JetBrains-Research/EnvBench。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-19T23:02:03Z</li>
<li><strong>目录日期</strong>: 2025-03-19</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
