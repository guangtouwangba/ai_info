<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Feedback on AI研究资料库</title>
    <link>https://example.org/tags/feedback/</link>
    <description>Recent content in Feedback on AI研究资料库</description>
    <generator>Hugo -- 0.145.0</generator>
    <language>zh</language>
    <lastBuildDate>Thu, 20 Mar 2025 13:01:00 +0000</lastBuildDate>
    <atom:link href="https://example.org/tags/feedback/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>More Information is Not Always Better: Connections between Zero-Sum Local Nash Equilibria in Feedback and Open-Loop Information Patterns</title>
      <link>https://example.org/papers/2025-03-20/more-information-is-not-always-better-connections-/</link>
      <pubDate>Thu, 20 Mar 2025 13:01:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/more-information-is-not-always-better-connections-/</guid>
      <description>非合作动态博弈理论为多智能体在无通信情况下的序贯决策提供了建模方法。研究重点在于寻找不同信息结构下双智能体零和动态博弈的纳什均衡。在线性二次博弈中，反馈和开环信息结构下的唯一纳什均衡轨迹相同。本文扩展至非线性动态和非凸非凹目标函数，证明局部反馈纳什均衡（FBNE）满足局部开环纳什均衡（OLNE）的一阶和二阶最优性条件，且在满足反馈充分条件时构成局部OLNE。此外，在严格互补条件下，局部FBNE也满足局部OLNE的一阶最优性条件。</description>
      <content:encoded><![CDATA[<h1 id="more-information-is-not-always-better-connections-between-zero-sum-local-nash-equilibria-in-feedback-and-open-loop-information-patterns">More Information is Not Always Better: Connections between Zero-Sum Local Nash Equilibria in Feedback and Open-Loop Information Patterns</h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.15486v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>Non-cooperative dynamic game theory provides a principled approach to
modeling sequential decision-making among multiple noncommunicative agents. A
key focus has been on finding Nash equilibria in two-agent zero-sum dynamic
games under various information structures. A well-known result states that in
linear-quadratic games, unique Nash equilibria under feedback and open-loop
information structures yield identical trajectories. Motivated by two key
perspectives &ndash; (i) many real-world problems extend beyond linear-quadratic
settings and lack unique equilibria, making only local Nash equilibria
computable, and (ii) local open-loop Nash equilibria (OLNE) are easier to
compute than local feedback Nash equilibria (FBNE) &ndash; it is natural to ask
whether a similar result holds for local equilibria in zero-sum games. To this
end, we establish that for a broad class of zero-sum games with potentially
nonconvex-nonconcave objectives and nonlinear dynamics: (i) the state/control
trajectory of a local FBNE satisfies local OLNE first-order optimality
conditions, and vice versa, (ii) a local FBNE trajectory satisfies local OLNE
second-order necessary conditions, (iii) a local FBNE trajectory satisfying
feedback sufficiency conditions also constitutes a local OLNE, and (iv) with
additional hard constraints on agents&rsquo; actuations, a local FBNE where strict
complementarity holds also satisfies local OLNE first-order optimality
conditions, and vice versa.</p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>非合作动态博弈理论为多智能体在无通信情况下的序贯决策提供了建模方法。研究重点在于寻找不同信息结构下双智能体零和动态博弈的纳什均衡。在线性二次博弈中，反馈和开环信息结构下的唯一纳什均衡轨迹相同。本文扩展至非线性动态和非凸非凹目标函数，证明局部反馈纳什均衡（FBNE）满足局部开环纳什均衡（OLNE）的一阶和二阶最优性条件，且在满足反馈充分条件时构成局部OLNE。此外，在严格互补条件下，局部FBNE也满足局部OLNE的一阶最优性条件。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T13:01:17+08:00</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
    <item>
      <title>LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment网页链接本文提出了一种新的微调方法LiFT，通过利用人工反馈来优化文本到视频生成模型的匹...</title>
      <link>https://example.org/papers/2025-03-20/lift-leveraging-human-feedback-for-text-to-video-m/</link>
      <pubDate>Thu, 20 Mar 2025 04:05:00 +0000</pubDate>
      <guid>https://example.org/papers/2025-03-20/lift-leveraging-human-feedback-for-text-to-video-m/</guid>
      <description>本文介绍了一种名为LiFT的新微调方法，旨在通过利用人工反馈来优化文本到视频生成模型的匹配度。研究团队首先构建了一个包含1万个人工评分及其理由的数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，该模型能够学习奖励函数，作为人类判断的代理，衡量视频与人类期望的匹配度。最后，利用学到的奖励函数通过最大化奖励加权的似然性来调整T2V模型。实验结果显示，经过微调的模型在所有16项指标上均优于CogVideoX-5B，证明了人类反馈在提高生成视频匹配度和质量方面的潜力。</description>
      <content:encoded><![CDATA[<h1 id="lift-leveraging-human-feedback-for-text-to-video-model-alignment网页链接本文提出了一种新的微调方法lift通过利用人工反馈来优化文本到视频生成模型的匹">LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment网页链接本文提出了一种新的微调方法LiFT，通过利用人工反馈来优化文本到视频生成模型的匹&hellip;</h1>
<p><strong>原始链接</strong>: <a href="https://weibo.com/1870858943/P4kWe0L4G">查看原文</a></p>
<h2 id="原始摘要">原始摘要</h2>
<p>LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fwww.aminer.cn%2Fpub%2F675659efae8580e7ff8d68a8%2F%3Ff%3Dwb" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><br>本文提出了一种新的微调方法LiFT，通过利用人工反馈来优化文本到视频生成模型的匹配度。研究团队首先构建了一个包含大约1万个由人工评分及其理由组成的人类评分注释数据集LiFT-HRA。基于此数据集，训练了一个奖励模型LiFT-Critic，该模型可以有效地学习奖励函数，作为人类判断的代理，衡量给定视频与人类期望之间的匹配度。最后，利用学到的奖励函数通过最大化奖励加权的似然性来调整T2V模型。以CogVideoX-2B为案例，研究结果显示经过微调的模型在所有16项指标上均优于CogVideoX-5B，证明了人类反馈在提高生成视频的匹配度和质量方面的潜力。<a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#生成模型#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%23&amp;extparam=%23%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%23" data-hide=""><span class="surl-text">#奖励模型#</span></a><a href="https://m.weibo.cn/p/index?extparam=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;containerid=100808f068f0dad74789bee210163c40a4b50d" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://n.sinaimg.cn/photo/5213b46e/20180926/timeline_card_small_super_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">人工智能</span></a><a href="https://m.weibo.cn/p/index?extparam=%E7%A1%95%E5%A3%AB%E8%AE%BA%E6%96%87&amp;containerid=1008084cacf38f5903dc7b04550404d0bd3608" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://n.sinaimg.cn/photo/5213b46e/20180926/timeline_card_small_super_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">硕士论文</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E5%BC%80%E6%BA%90%23" data-hide=""><span class="surl-text">#开源#</span></a><img style="" src="https://tvax4.sinaimg.cn/large/6f830abfly1hwgrb57m70j22cl18bnpd.jpg" referrerpolicy="no-referrer"><br><br></p>
<h2 id="ai-摘要">AI 摘要</h2>
<p>本文介绍了一种名为LiFT的新微调方法，旨在通过利用人工反馈来优化文本到视频生成模型的匹配度。研究团队首先构建了一个包含1万个人工评分及其理由的数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，该模型能够学习奖励函数，作为人类判断的代理，衡量视频与人类期望的匹配度。最后，利用学到的奖励函数通过最大化奖励加权的似然性来调整T2V模型。实验结果显示，经过微调的模型在所有16项指标上均优于CogVideoX-5B，证明了人类反馈在提高生成视频匹配度和质量方面的潜力。</p>
<h2 id="元数据">元数据</h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T04:05:11Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>
]]></content:encoded>
    </item>
  </channel>
</rss>
