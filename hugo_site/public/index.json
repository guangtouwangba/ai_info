[{"content":"#小米汽车透露今年愿景#在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。卢伟冰\u0026hellip; 原始链接: 查看原文\n原始摘要 #小米汽车透露今年愿景#\n在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。\n2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。\n卢伟冰表示，要在智能驾驶领域实现“一年追三代”，并计划加大AI研发投入，力争在2025年跻身自动驾驶第一梯队。\n目前，小米的智驾系统已经实现高速NOA、端到端泊车、城区NOA全量推送。\n而今年目标则是，进一步优化端到端全场景智驾，让小米汽车能够实现真正的“车位到车位”自动驾驶体验。\n同时，在产品线扩张方面，除了持续提升SU7的产能，小米计划在2024年底推出SUV车型YU7。\nYU7这款车定位更符合中国市场偏好，预计售价区间比SU7更亲民，已经被市场提前预定为“爆款”。\n此外，小米正在加速全球化布局，计划于2027年正式启动海外销售，并在欧洲设立汽车研发中心，吸引国际顶级汽车人才加盟。\n从目前的发展节奏来看，小米汽车不仅有望在国内市场挑战特斯拉Model Y的销量霸主地位，在全球市场也可能掀起一场新的竞争风暴。\nAI 摘要 在2024年财报电话会议上，小米总裁卢伟冰透露了小米汽车的发展愿景。2024年的关键词是“增长”，而2025年则是“智能化”。小米计划在智能驾驶领域实现“一年追三代”，并加大AI研发投入，目标在2025年进入自动驾驶第一梯队。目前，小米的智驾系统已实现高速NOA、端到端泊车等功能，2024年目标是优化全场景智驾，实现“车位到车位”自动驾驶。此外，小米计划在2024年底推出SUV车型YU7，并加速全球化布局，计划2027年启动海外销售，挑战特斯拉Model Y的销量地位。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:05:20+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/20242025/","summary":"在2024年财报电话会议上，小米总裁卢伟冰透露了小米汽车的发展愿景。2024年的关键词是“增长”，而2025年则是“智能化”。小米计划在智能驾驶领域实现“一年追三代”，并加大AI研发投入，目标在2025年进入自动驾驶第一梯队。目前，小米的智驾系统已实现高速NOA、端到端泊车等功能，2024年目标是优化全场景智驾，实现“车位到车位”自动驾驶。此外，小米计划在2024年底推出SUV车型YU7，并加速全球化布局，计划2027年启动海外销售，挑战特斯拉Model Y的销量地位。","title":"#小米汽车透露今年愿景#在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。卢伟冰..."},{"content":"#小米史上最强年报#去年是小米集团成立以来，业绩最辉煌的一年。据小米年报显示，小米集团全年营收高达3659亿元，同比增长35%，第四季度更是首次突破千亿大关，\u0026hellip; 原始链接: 查看原文\n原始摘要 #小米史上最强年报#\n去年是小米集团成立以来，业绩最辉煌的一年。\n据小米年报显示，小米集团全年营收高达3659亿元，同比增长35%，第四季度更是首次突破千亿大关，达到1090亿元。\n净利润方面，小米全年调整后净利润272亿元，同比增长41.3%，其中第四季度利润同比增长近70%，远超市场预期。\n值得注意的是，小米的大手笔研发投入也是这份年报中的亮点之一：全年研发支出240.5亿元，同比增长26%，其中AI和智能驾驶成为投入重点。\n其中，智能手机业务依旧是小米的核心支柱，全年收入1918亿元，同比增长21.8%，高端化战略初见成效，手机ASP（平均售价）同比提升至1138.2元。\n此外，IoT与生活消费产品收入1041亿元，同比增长30%，互联网服务收入341亿元，同比增长13.3%。\n在多条业务线齐头并进的情况下，小米成功实现了利润率的提升，全年毛利率达到20.9%，其中汽车业务的毛利率在第四季度突破20%，这也为未来实现造车盈利奠定了基础。\n可以说，这份财报不仅展示了小米当前的强势增长，也表明其多元化布局正在进入收获期。\nAI 摘要 小米集团2022年财报显示，全年营收达3659亿元，同比增长35%，净利润272亿元，增长41.3%。智能手机业务收入1918亿元，同比增长21.8%，高端化战略初见成效。IoT与生活消费产品收入1041亿元，增长30%，互联网服务收入341亿元，增长13.3%。研发投入240.5亿元，增长26%，重点投入AI和智能驾驶。全年毛利率20.9%，汽车业务毛利率突破20%，为未来盈利奠定基础。财报显示小米多元化布局进入收获期，业绩增长强劲。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:05:08+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/365935/","summary":"小米集团2022年财报显示，全年营收达3659亿元，同比增长35%，净利润272亿元，增长41.3%。智能手机业务收入1918亿元，同比增长21.8%，高端化战略初见成效。IoT与生活消费产品收入1041亿元，增长30%，互联网服务收入341亿元，增长13.3%。研发投入240.5亿元，增长26%，重点投入AI和智能驾驶。全年毛利率20.9%，汽车业务毛利率突破20%，为未来盈利奠定基础。财报显示小米多元化布局进入收获期，业绩增长强劲。","title":"#小米史上最强年报#去年是小米集团成立以来，业绩最辉煌的一年。据小米年报显示，小米集团全年营收高达3659亿元，同比增长35%，第四季度更是首次突破千亿大关，..."},{"content":"#机器人会侧空翻了##全球首次人形机器人原地侧空翻#宇树科技的G1人形机器人，刚刚完成了全球首次原地侧空翻动作。官方发布的【视频】显示，机器人不仅顺利完成侧\u0026hellip; 原始链接: 查看原文\n原始摘要 #机器人会侧空翻了##全球首次人形机器人原地侧空翻#\n宇树科技的G1人形机器人，刚刚完成了全球首次原地侧空翻动作。\n官方发布的【视频】显示，机器人不仅顺利完成侧空翻，而且翻滚流畅，机身协调，丝毫没有多余的动作。\n宇树科技表示，该动作是团队抽空时顺便挑战的，而且开发程序和拍摄过程中，G1没有出现任何故障。\n去年，宇树科技曾用身高1.8米的H1机器人，完成全球首个电机驱动人形机器人后空翻动作。 而这次的G1侧空翻更进一步，体现了团队独创的动力学算法和机械设计实力。\n不过，评论区有网友表示，作为消费者，相比于侧空翻、回旋踢这类华丽的动作，他们更想看做家务这类实用性动作。 量子位的微博视频视频无法显示，请前往微博视频观看。\nAI 摘要 宇树科技的G1人形机器人成功完成了全球首次原地侧空翻动作，展示了其流畅的翻滚和协调的机身动作。这一成就体现了团队在动力学算法和机械设计方面的实力。去年，宇树科技曾用H1机器人完成全球首个电机驱动人形机器人后空翻。尽管这一技术突破令人印象深刻，但有网友表示，他们更希望看到机器人执行如做家务等实用性任务。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:04:32+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/g1/","summary":"宇树科技的G1人形机器人成功完成了全球首次原地侧空翻动作，展示了其流畅的翻滚和协调的机身动作。这一成就体现了团队在动力学算法和机械设计方面的实力。去年，宇树科技曾用H1机器人完成全球首个电机驱动人形机器人后空翻。尽管这一技术突破令人印象深刻，但有网友表示，他们更希望看到机器人执行如做家务等实用性任务。","title":"#机器人会侧空翻了##全球首次人形机器人原地侧空翻#宇树科技的G1人形机器人，刚刚完成了全球首次原地侧空翻动作。官方发布的【视频】显示，机器人不仅顺利完成侧..."},{"content":"#老黄带迪士尼机器人炸场GTC# 老黄带着迪士尼机器人炸场GTC，现场发布机器人物理引擎Newton、基础模型GROOT N1，他还预言：机器人将会是未来最大的行业。 量子位\u0026hellip; 原始链接: 查看原文\n原始摘要 #老黄带迪士尼机器人炸场GTC# 老黄带着迪士尼机器人炸场GTC，现场发布机器人物理引擎Newton、基础模型GROOT N1，他还预言：机器人将会是未来最大的行业。 量子位的微博视频 视频无法显示，请前往微博视频观看。\nAI 摘要 在GTC大会上，老黄展示了迪士尼机器人，并发布了机器人物理引擎Newton和基础模型GROOT N1。他预测机器人将成为未来最大的行业。这一展示突显了机器人在娱乐和技术领域的潜力，预示着机器人技术的快速发展及其在各行业的广泛应用。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:04:06+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/gtc-gtcnewtongroot-n1-/","summary":"在GTC大会上，老黄展示了迪士尼机器人，并发布了机器人物理引擎Newton和基础模型GROOT N1。他预测机器人将成为未来最大的行业。这一展示突显了机器人在娱乐和技术领域的潜力，预示着机器人技术的快速发展及其在各行业的广泛应用。","title":"#老黄带迪士尼机器人炸场GTC# 老黄带着迪士尼机器人炸场GTC，现场发布机器人物理引擎Newton、基础模型GROOT N1，他还预言：机器人将会是未来最大的行业。 量子位..."},{"content":"#苹果限制第三方智能手表##智能手表鼻祖Pebble回归#Pebble，这款智能手表的鼻祖，凭借超长续航和极简设计，被无数科技极客奉为神器。尽管Pebble品牌早已停止运营\u0026hellip; 原始链接: 查看原文\n原始摘要 #苹果限制第三方智能手表##智能手表鼻祖Pebble回归#\nPebble，这款智能手表的鼻祖，凭借超长续航和极简设计，被无数科技极客奉为神器。\n尽管Pebble品牌早已停止运营，但一群忠实的开发者重燃了它的生命，推出了两款全新的PebbleOS手表：Core 2 Duo和Core Time 2。\n先来看Core 2 Duo，它不仅延续了Pebble的经典设计，还带来了现代化的硬件升级：\n- 1.26英寸黑白电子墨水屏，显示清晰\n- 30天超长续航，比旧版Pebble 2的7天大幅提升\n- 轻量化聚碳酸酯机身，提供黑白两种配色\n- 物理按键操控，避免触控屏误触问题\n- 内置扬声器和线性马达，震动反馈更强，声音更安静\n- 售价$149（约人民币1078元），预计7月发货\n接下来是Core Time 2，它是一款更先进的Pebble智能手表：\n- 1.5英寸64色电子墨水屏，显示面积比Pebble Time 2大53%\n- 金属机身+触控屏，外观更精致\n- 预计30天续航，无需频繁充电\n- 额外支持心率监测，增加健康功能\n- 标准22mm表带，可自由更换\n- 售价$225（约人民币1627元），预计12月发货\n此外，这两款手表还开放了源代码，允许开发者自由修改和拓展功能。\n然而，PebbleOS手表在iPhone上的使用却受到了诸多限制，包括：\n- 无法代替iPhone发送短信或iMessage\n- 无法“标记为已读”“删除”或“快速回复”消息\n- PebbleOS应用一旦被手动关闭或iOS自动清理，手表将无法连接手机或访问互联网\n- 无法检测用户是否在使用iPhone，哪怕你正在看手机，通知仍会震动提醒\n- 所有应用必须经过App Store审核，每次更新都有被拒的风险\n- 开发者无法通过iOS应用商店收费\n这些限制使得新款PebbleOS手表在Android上的体验远超iOS。\n苹果对外宣称这些限制是出于“安全、隐私和用户体验”考虑，但美国司法部对此展开了反垄断调查，指控苹果通过技术壁垒打压竞争对手。\nPebble的精神虽然延续了下来，但面对苹果的封闭生态，新款PebbleOS手表的未来仍充满挑战。\n拓展阅读：网页链接\n视频无法显示，请前往微博视频观看。\nAI 摘要 Pebble智能手表鼻祖回归，推出Core 2 Duo和Core Time 2两款新品。Core 2 Duo延续经典设计，配备1.26英寸黑白电子墨水屏和30天续航，售价149美元。Core Time 2则更先进，拥有1.5英寸64色电子墨水屏、金属机身和心率监测功能，售价225美元。两款手表均开放源代码，支持开发者自由修改。然而，PebbleOS手表在iPhone上使用受限，无法发送短信、快速回复消息等，导致其在Android上的体验优于iOS。苹果对此限制的解释是出于安全、隐私和用户体验考虑，但美国司法部已展开反垄断调查。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:04:46+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/pebblepebblepebble/","summary":"Pebble智能手表鼻祖回归，推出Core 2 Duo和Core Time 2两款新品。Core 2 Duo延续经典设计，配备1.26英寸黑白电子墨水屏和30天续航，售价149美元。Core Time 2则更先进，拥有1.5英寸64色电子墨水屏、金属机身和心率监测功能，售价225美元。两款手表均开放源代码，支持开发者自由修改。然而，PebbleOS手表在iPhone上使用受限，无法发送短信、快速回复消息等，导致其在Android上的体验优于iOS。苹果对此限制的解释是出于安全、隐私和用户体验考虑，但美国司法部已展开反垄断调查。","title":"#苹果限制第三方智能手表##智能手表鼻祖Pebble回归#Pebble，这款智能手表的鼻祖，凭借超长续航和极简设计，被无数科技极客奉为神器。尽管Pebble品牌早已停止运营..."},{"content":"#搜索竞技场上线##AI联网搜索竞技场#AI联网搜索这一块，也有竞技场了。Chatbot Arena做了个搜索竞技场（Search Arena），这是一个盲测AI的搜索能力的平台，用户\u0026hellip; 原始链接: 查看原文\n原始摘要 #搜索竞技场上线##AI联网搜索竞技场#\nAI联网搜索这一块，也有竞技场了。\nChatbot Arena做了个搜索竞技场（Search Arena），这是一个盲测AI的搜索能力的平台，用户可以投票选出哪个模型搜索信息更强。\n来看个例子，用户问：下次月全食是什么时候？Search Arena会提供两个不同模型的回答。\n投票结束后，系统揭示答案来源：模型A是GPT-4o的纯模型生成结果，而模型B则是GPT-4o-mini-search通过联网搜索得出的答案。\n官方还展示了其他对比案例：\n- 总结近期VibeCheck论文的主要发现（Perplexity Sonar VS Gemini-2.0-Flash-Grounding）【图2】\n- 撰写一篇人工智能在教育领域应用的简短文章（Perplexity Sonar-Reasoning VS Sonar-Pro）【图3】\n- 我在我的 gradio 应用程序中收到此错误：TypeError: init() 收到意外的关键字参数‘工具’【图4】\n至于哪个模型表现更好，大家自行判断。\n除了搜索对比，Search Arena还具备以下功能：\n- 支持上传图片，并根据图片对话\n- 支持文生图（Text-to-Image），而且DALL-E 3、Flux、Ideogram都能用\n- 支持用RepoChat，直接跟GitHub代码仓库对话，方便开发者查代码\n感兴趣的小伙伴可以点击：网页链接\nAI 摘要 Chatbot Arena推出了“搜索竞技场”（Search Arena），这是一个盲测AI搜索能力的平台。用户可以通过投票比较不同模型的搜索效果。例如，用户提问“下次月全食是什么时候？”，平台会展示两个模型的回答，并揭示答案来源。Search Arena还支持图片对话、文生图（Text-to-Image）以及与GitHub代码仓库对话等功能，方便开发者查代码。平台展示了多个对比案例，如总结论文、撰写文章等，用户可自行判断哪个模型表现更好。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:04:23+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/aiaichatbot-arenasearch-arenaai/","summary":"Chatbot Arena推出了“搜索竞技场”（Search Arena），这是一个盲测AI搜索能力的平台。用户可以通过投票比较不同模型的搜索效果。例如，用户提问“下次月全食是什么时候？”，平台会展示两个模型的回答，并揭示答案来源。Search Arena还支持图片对话、文生图（Text-to-Image）以及与GitHub代码仓库对话等功能，方便开发者查代码。平台展示了多个对比案例，如总结论文、撰写文章等，用户可自行判断哪个模型表现更好。","title":"#搜索竞技场上线##AI联网搜索竞技场#AI联网搜索这一块，也有竞技场了。Chatbot Arena做了个搜索竞技场（Search Arena），这是一个盲测AI的搜索能力的平台，用户..."},{"content":"#小米成中国市值第一车企#小米SU7的热销不仅助推公司营收增长，更是带动了股价暴涨。短短一年，小米市值翻了近四倍，突破1.36万亿元，超越比亚迪，成了中国市值\u0026hellip; 原始链接: 查看原文\n原始摘要 #小米成中国市值第一车企#\n小米SU7的热销不仅助推公司营收增长，更是带动了股价暴涨。\n短短一年，小米市值翻了近四倍，突破1.36万亿元，超越比亚迪，成了中国市值最高的汽车制造商。\n相比之下，比亚迪目前市值万亿元左右，而蔚来、小鹏等新势力车企则仍在千亿级别。\n这背后，离不开小米的“手机+AI+汽车”一体化的生态战略，以及其品牌在供应链、技术积累、用户流量上的优势。\n更重要的是，小米SU7自上市以来销量节节攀升，不仅3个月交付破万，7个月单月销量突破2万，还打破新势力最快十万台下线纪录。\n目前，小米已将年交付目标提升至35万辆，并计划推出第二款SUV车型YU7，意图进一步扩大市场份额。\n可以说，小米造车的第一年，交出了一份超出市场预期的成绩单，而市值的飙升正是资本市场对其前景的看好。\nAI 摘要 小米凭借其SU7车型的热销，市值在一年内翻了近四倍，达到1.36万亿元，超越比亚迪成为中国市值最高的汽车制造商。这一成就得益于小米的“手机+AI+汽车”一体化生态战略及其在供应链、技术积累和用户流量上的优势。小米SU7自上市以来销量迅速增长，7个月内单月销量突破2万，并创下新势力最快十万台下线纪录。小米已将年交付目标提升至35万辆，并计划推出第二款SUV车型YU7，以进一步扩大市场份额。市值的飙升反映了资本市场对小米前景的看好。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:04:58+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/su7136/","summary":"小米凭借其SU7车型的热销，市值在一年内翻了近四倍，达到1.36万亿元，超越比亚迪成为中国市值最高的汽车制造商。这一成就得益于小米的“手机+AI+汽车”一体化生态战略及其在供应链、技术积累和用户流量上的优势。小米SU7自上市以来销量迅速增长，7个月内单月销量突破2万，并创下新势力最快十万台下线纪录。小米已将年交付目标提升至35万辆，并计划推出第二款SUV车型YU7，以进一步扩大市场份额。市值的飙升反映了资本市场对小米前景的看好。","title":"#小米成中国市值第一车企#小米SU7的热销不仅助推公司营收增长，更是带动了股价暴涨。短短一年，小米市值翻了近四倍，突破1.36万亿元，超越比亚迪，成了中国市值..."},{"content":"【#AI实时汉化游戏# 】其实不止Galgame……各种其他类型的游戏都可以用AI来翻译游玩，这期视频用Gal举例是因为文字对这类游戏至关重要，体验起来会更明显。啊我\u0026hellip; 原始链接: 查看原文\n原始摘要 【#AI实时汉化游戏# 】\n其实不止Galgame……各种其他类型的游戏都可以用AI来翻译游玩，这期视频用Gal举例是因为文字对这类游戏至关重要，体验起来会更明显。啊我这么说绝对不是在找理由喔，呜哇你这人，哇。总不能是用过0d000721代码兑换过图书馆的桌子，不小心偷看了废柴萝卜子的笔记本，一边津津有味地看着FC圣体的败犬剧情，终于撤销了发送的第一封D-mail却到了1.055821线于是Amadeus把这份包含了LM-Studio和大模型文件的链接端了上来： pan.ba防止吞链接请删除这段文字idu.com/s/117tHdGHdG也把这段删了吧pJjGLxstqvk0Ut8g?pwd=73y4 量子位的微博视频视频无法显示，请前往微博视频观看。\nAI 摘要 该微博讨论了AI技术在游戏实时汉化中的应用，特别是以Galgame为例，因为这类游戏的文字内容对玩家体验至关重要。视频展示了AI如何通过大模型和LM-Studio等工具实现游戏的实时翻译，提升玩家的游戏体验。此外，视频还提供了相关文件的下载链接，供用户进一步了解和尝试。整体内容强调了AI在游戏翻译中的潜力及其对玩家体验的积极影响。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:03:34+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/ai-galgameaigal/","summary":"该微博讨论了AI技术在游戏实时汉化中的应用，特别是以Galgame为例，因为这类游戏的文字内容对玩家体验至关重要。视频展示了AI如何通过大模型和LM-Studio等工具实现游戏的实时翻译，提升玩家的游戏体验。此外，视频还提供了相关文件的下载链接，供用户进一步了解和尝试。整体内容强调了AI在游戏翻译中的潜力及其对玩家体验的积极影响。","title":"【#AI实时汉化游戏# 】其实不止Galgame……各种其他类型的游戏都可以用AI来翻译游玩，这期视频用Gal举例是因为文字对这类游戏至关重要，体验起来会更明显。啊我..."},{"content":"#OpenAI史上最贵模型来了# 比DeepSeek贵270倍比DeepSeek-R1贵270倍，OpenAI史上最贵模型来了！就在刚刚，OpenAI上线了推理模型o1-pro的API。本来大家还挺高兴，\u0026hellip; 原始链接: 查看原文\n原始摘要 #OpenAI史上最贵模型来了# 比DeepSeek贵270倍\n比DeepSeek-R1贵270倍，OpenAI史上最贵模型来了！\n就在刚刚，OpenAI上线了推理模型o1-pro的API。\n本来大家还挺高兴，结果一看到价格，悬着的心终于死了。\n100万输入/输出token价格分别为150美元和600美元\n对比DeepSeek模型，其输出价格整整比R1贵了270倍，以至于有网友大肆吐槽：\n除非其智能达到了爱因斯坦级别，否则不值得。\n与此同时，关于其价格的争议迅速在𝕏和Reddit平台上掀起。OpenAI史上最贵模型来了！比DeepSeek贵270倍，100万输出token 600美元\nAI 摘要 OpenAI最近推出了其史上最贵的推理模型o1-pro的API，其价格引发了广泛争议。该模型的100万输入/输出token价格分别为150美元和600美元，相比DeepSeek模型的输出价格贵了270倍。这一高价引发了网友的强烈反应，许多人认为除非模型的智能达到爱因斯坦级别，否则不值得如此高昂的价格。关于价格的争议迅速在𝕏和Reddit等平台上展开讨论。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:03:46+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/openai-deepseek270deepseek-r1270openaiopenaio1-pro/","summary":"OpenAI最近推出了其史上最贵的推理模型o1-pro的API，其价格引发了广泛争议。该模型的100万输入/输出token价格分别为150美元和600美元，相比DeepSeek模型的输出价格贵了270倍。这一高价引发了网友的强烈反应，许多人认为除非模型的智能达到爱因斯坦级别，否则不值得如此高昂的价格。关于价格的争议迅速在𝕏和Reddit等平台上展开讨论。","title":"#OpenAI史上最贵模型来了# 比DeepSeek贵270倍比DeepSeek-R1贵270倍，OpenAI史上最贵模型来了！就在刚刚，OpenAI上线了推理模型o1-pro的API。本来大家还挺高兴，..."},{"content":"#谷歌Gemini一句话P图# 没想到谷歌推出Gemini 2.0 Flash之后，最先遭殃的居然是PS。虽然不稳定，但这个靠对话就能免费P图的功能还是挺好玩的…… 量子位的微博视\u0026hellip; 原始链接: 查看原文\n原始摘要 #谷歌Gemini一句话P图# 没想到谷歌推出Gemini 2.0 Flash之后，最先遭殃的居然是PS。虽然不稳定，但这个靠对话就能免费P图的功能还是挺好玩的…… 量子位的微博视频 视频无法显示，请前往微博视频观看。\nAI 摘要 谷歌推出的Gemini 2.0 Flash版本，通过对话即可实现免费P图功能，这一创新功能虽然尚不稳定，但其便捷性和趣味性已引起广泛关注。该功能可能对传统图像处理软件如Photoshop构成挑战，展示了AI技术在图像编辑领域的潜力。用户可以通过简单的对话指令完成复杂的图像处理任务，预示着未来图像编辑工具的发展方向。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:03:56+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/geminip-gemini-20-flashpsp-/","summary":"谷歌推出的Gemini 2.0 Flash版本，通过对话即可实现免费P图功能，这一创新功能虽然尚不稳定，但其便捷性和趣味性已引起广泛关注。该功能可能对传统图像处理软件如Photoshop构成挑战，展示了AI技术在图像编辑领域的潜力。用户可以通过简单的对话指令完成复杂的图像处理任务，预示着未来图像编辑工具的发展方向。","title":"#谷歌Gemini一句话P图# 没想到谷歌推出Gemini 2.0 Flash之后，最先遭殃的居然是PS。虽然不稳定，但这个靠对话就能免费P图的功能还是挺好玩的…… 量子位的微博视..."},{"content":"A Role of Environmental Complexity on Representation Learning in Deep Reinforcement Learning Agents 原始链接: 查看原文\n原始摘要 We developed a simulated environment to train deep reinforcement learning agents on a shortcut usage navigation task, motivated by the Dual Solutions Paradigm test used for human navigators. We manipulated the frequency with which agents were exposed to a shortcut and a navigation cue, to investigate how these factors influence shortcut usage development. We find that all agents rapidly achieve optimal performance in closed shortcut trials once initial learning starts. However, their navigation speed and shortcut usage when it is open happen faster in agents with higher shortcut exposure. Analysis of the agents\u0026rsquo; artificial neural networks activity revealed that frequent presentation of a cue initially resulted in better encoding of the cue in the activity of individual nodes, compared to agents who encountered the cue less often. However, stronger cue representations were ultimately formed through the use of the cue in the context of navigation planning, rather than simply through exposure. We found that in all agents, spatial representations develop early in training and subsequently stabilize before navigation strategies fully develop, suggesting that having spatially consistent activations is necessary for basic navigation, but insufficient for advanced strategies. Further, using new analysis techniques, we found that the planned trajectory rather than the agent\u0026rsquo;s immediate location is encoded in the agent\u0026rsquo;s networks. Moreover, the encoding is represented at the population rather than the individual node level. These techniques could have broader applications in studying neural activity across populations of neurons or network nodes beyond individual activity patterns.\nAI 摘要 研究开发了一个模拟环境，用于训练深度强化学习代理在捷径使用导航任务中的表现，灵感来自人类导航者的双解决方案范式测试。研究发现，代理在封闭捷径试验中一旦开始学习，便能迅速达到最佳表现。高频率接触捷径的代理在捷径开放时导航速度和捷径使用更快。分析显示，频繁呈现的导航线索在初期能更好地编码于单个节点中，但更强的线索表示是通过导航规划中的使用形成的。空间表示在训练早期发展并稳定，而计划轨迹而非当前位置在代理网络中编码，且编码在群体而非单个节点水平上表示。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:03:15+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/a-role-of-environmental-complexity-on-representati/","summary":"研究开发了一个模拟环境，用于训练深度强化学习代理在捷径使用导航任务中的表现，灵感来自人类导航者的双解决方案范式测试。研究发现，代理在封闭捷径试验中一旦开始学习，便能迅速达到最佳表现。高频率接触捷径的代理在捷径开放时导航速度和捷径使用更快。分析显示，频繁呈现的导航线索在初期能更好地编码于单个节点中，但更强的线索表示是通过导航规划中的使用形成的。空间表示在训练早期发展并稳定，而计划轨迹而非当前位置在代理网络中编码，且编码在群体而非单个节点水平上表示。","title":"A Role of Environmental Complexity on Representation Learning in Deep Reinforcement Learning Agents"},{"content":"Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development 原始链接: 查看原文\n原始摘要 Despite the utility that Generative AI (GenAI) tools provide for tasks such as writing code, the use of these tools raises important legal questions and potential risks, particularly those associated with copyright law. As lawmakers and regulators engage with those questions, the views of users can provide relevant perspectives. In this paper, we provide: (1) a survey of 574 developers on the licensing and copyright aspects of GenAI for coding, as well as follow-up interviews; (2) a snapshot of developers\u0026rsquo; views at a time when GenAI and perceptions of it are rapidly evolving; and (3) an analysis of developers\u0026rsquo; views, yielding insights and recommendations that can inform future regulatory decisions in this evolving field. Our results show the benefits developers derive from GenAI, how they view the use of AI-generated code as similar to using other existing code, the varied opinions they have on who should own or be compensated for such code, that they are concerned about data leakage via GenAI, and much more, providing organizations and policymakers with valuable insights into how the technology is being used and what concerns stakeholders would like to see addressed.\nAI 摘要 本文探讨了生成式AI（GenAI）工具在编码任务中的应用及其引发的法律问题，特别是版权法相关的风险。通过对574名开发者的调查和后续访谈，研究捕捉了开发者对GenAI快速演变中的看法，并分析了他们的观点。结果显示，开发者从GenAI中获益，认为使用AI生成的代码与现有代码类似，但对代码所有权和补偿问题意见不一，且担忧数据泄露。这些发现为组织和政策制定者提供了关于技术使用和利益相关者关注点的宝贵见解，有助于未来监管决策。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:02:34+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/developer-perspectives-on-licensing-and-copyright-/","summary":"本文探讨了生成式AI（GenAI）工具在编码任务中的应用及其引发的法律问题，特别是版权法相关的风险。通过对574名开发者的调查和后续访谈，研究捕捉了开发者对GenAI快速演变中的看法，并分析了他们的观点。结果显示，开发者从GenAI中获益，认为使用AI生成的代码与现有代码类似，但对代码所有权和补偿问题意见不一，且担忧数据泄露。这些发现为组织和政策制定者提供了关于技术使用和利益相关者关注点的宝贵见解，有助于未来监管决策。","title":"Developer Perspectives on Licensing and Copyright Issues Arising from Generative AI for Software Development"},{"content":"Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces 原始链接: 查看原文\n原始摘要 Recent progress in quantum machine learning has sparked interest in using quantum methods to tackle classical control problems via quantum reinforcement learning. However, the classical reinforcement learning environments often scale to high dimensional problem spaces, which represents a challenge for the limited and costly resources available for quantum agent implementations. We propose to solve this dimensionality challenge by a classical autoencoder and a quantum agent together, where a compressed representation of observations is jointly learned in a hybrid training loop. The latent representation of such an autoencoder will serve as a tailored observation space best suited for both the control problem and the QPU architecture, aligning with the agent\u0026rsquo;s requirements. A series of numerical experiments are designed for a performance analysis of the latent-space learning method. Results are presented for different control problems and for both photonic (continuous-variable) and qubit-based agents, to show how the QNN learning process is improved by the joint training.\nAI 摘要 量子机器学习的最新进展激发了利用量子强化学习方法解决经典控制问题的兴趣。然而，经典强化学习环境通常涉及高维问题空间，这对量子代理实现的有限且昂贵的资源构成了挑战。为解决这一维度挑战，本文提出了一种结合经典自编码器和量子代理的混合训练方法，通过联合学习压缩的观测表示来优化控制问题和量子处理单元架构的需求。通过一系列数值实验，展示了该方法在不同控制问题和光子（连续变量）及基于量子比特的代理中的性能，证明了联合训练对量子神经网络学习过程的改进效果。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:02:55+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/hybrid-quantum-classical-reinforcement-learning-in/","summary":"量子机器学习的最新进展激发了利用量子强化学习方法解决经典控制问题的兴趣。然而，经典强化学习环境通常涉及高维问题空间，这对量子代理实现的有限且昂贵的资源构成了挑战。为解决这一维度挑战，本文提出了一种结合经典自编码器和量子代理的混合训练方法，通过联合学习压缩的观测表示来优化控制问题和量子处理单元架构的需求。通过一系列数值实验，展示了该方法在不同控制问题和光子（连续变量）及基于量子比特的代理中的性能，证明了联合训练对量子神经网络学习过程的改进效果。","title":"Hybrid Quantum-Classical Reinforcement Learning in Latent Observation Spaces"},{"content":"Modeling crystal defects using defect-informed neural networks 原始链接: 查看原文\n原始摘要 Machine learning has revolutionized the study of crystalline materials for enabling rapid predictions and discovery. However, most AI-for-Materials research to date has focused on ideal crystals, whereas real-world materials inevitably contain defects that play a critical role in modern functional technologies. The defects and dopants break geometric symmetry and increase interaction complexity, posing particular challenges for traditional ML models. Addressing these challenges requires models that are able to capture sparse defect-driven effects in crystals while maintaining adaptability and precision. Here, we introduce Defect-Informed Equivariant Graph Neural Network (DefiNet), a model specifically designed to accurately capture defect-related interactions and geometric configurations in point-defect structures. Trained on 14,866 defect structures, DefiNet achieves highly accurate structural predictions in a single step, avoiding the time-consuming iterative processes in modern ML relaxation models and possible error accumulation from iteration. We further validates DefiNet\u0026rsquo;s accuracy by using density functional theory (DFT) relaxation on DefiNet-predicted structures. For most defect structures, regardless of defect complexity or system size, only 3 ionic steps are required to reach the DFT-level ground state. Finally, comparisons with scanning transmission electron microscopy (STEM) images confirm DefiNet\u0026rsquo;s scalability and extrapolation beyond point defects, positioning it as a groundbreaking tool for defect-focused materials research.\nAI 摘要 机器学习在晶体材料研究中实现了快速预测和发现，但现有研究多集中于理想晶体，忽略了实际材料中的缺陷。这些缺陷和掺杂破坏了几何对称性，增加了相互作用的复杂性，对传统机器学习模型构成挑战。为此，研究者提出了缺陷信息等变图神经网络（DefiNet），专门用于捕捉点缺陷结构中的缺陷相关相互作用和几何配置。DefiNet在14,866个缺陷结构上训练，能一步实现高精度结构预测，避免了迭代过程中的误差积累。通过密度泛函理论验证，DefiNet预测的结构仅需3次离子步骤即可达到基态，展示了其在缺陷材料研究中的突破性潜力。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:02:10+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/modeling-crystal-defects-using-defect-informed-neu/","summary":"机器学习在晶体材料研究中实现了快速预测和发现，但现有研究多集中于理想晶体，忽略了实际材料中的缺陷。这些缺陷和掺杂破坏了几何对称性，增加了相互作用的复杂性，对传统机器学习模型构成挑战。为此，研究者提出了缺陷信息等变图神经网络（DefiNet），专门用于捕捉点缺陷结构中的缺陷相关相互作用和几何配置。DefiNet在14,866个缺陷结构上训练，能一步实现高精度结构预测，避免了迭代过程中的误差积累。通过密度泛函理论验证，DefiNet预测的结构仅需3次离子步骤即可达到基态，展示了其在缺陷材料研究中的突破性潜力。","title":"Modeling crystal defects using defect-informed neural networks"},{"content":"Resource-Efficient Quantum Correlation Measurement: A Multicopy Neural Network Approach for Practical Applications 原始链接: 查看原文\n原始摘要 Measuring complex properties in quantum systems, such as measures of quantum entanglement and Bell nonlocality, is inherently challenging. Traditional methods, like quantum state tomography (QST), necessitate a full reconstruction of the density matrix for a given system and demand resources that scale exponentially with system size. We propose an alternative strategy that reduces the required information by combining multicopy measurements with artificial neural networks (ANNs), resulting in a 67% reduction in measurement requirements compared to QST. We have successfully measured two-qubit quantum correlations of Bell states subjected to a depolarizing channel (resulting in Werner states) and an amplitude damping channel (leading to Horodecki states) using the multicopy approach on real quantum hardware. Our experiments, conducted with transmon qubits on IBMQ processors, quantified the violation of Bell\u0026rsquo;s inequality and the negativity of two-qubit entangled states. We compared these results with those obtained from the standard QST approach and applied a maximum likelihood method to mitigate noise. We trained ANNs to estimate entanglement and nonlocality measures using optimized sets of projections identified through Shapley\u0026rsquo;s (SHAP) analysis for the Werner and Horodecki states. The ANN output, based on this reduced set of projections, aligns well with expected values and enhances noise robustness. This approach simplifies and improves the error-robustness of multicopy measurements, eliminating the need for complex nonlinear equation analysis. It represents a significant advancement in AI-assisted quantum measurements, making practical implementation on current quantum hardware more feasible.\nAI 摘要 传统量子态层析（QST）方法需要重建密度矩阵，资源需求随系统规模指数增长。本文提出一种结合多拷贝测量和人工神经网络（ANN）的策略，将测量需求减少67%。实验在IBMQ处理器上使用transmon量子比特，成功测量了受退极化通道和振幅阻尼通道影响的两量子比特纠缠态。通过最大似然方法抑制噪声，并利用SHAP分析优化投影集训练ANN估计纠缠和非局域性。该方法简化了多拷贝测量，提高了噪声鲁棒性，无需复杂非线性方程分析，显著推动了AI辅助量子测量的实际应用。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:02:45+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/resource-efficient-quantum-correlation-measurement/","summary":"传统量子态层析（QST）方法需要重建密度矩阵，资源需求随系统规模指数增长。本文提出一种结合多拷贝测量和人工神经网络（ANN）的策略，将测量需求减少67%。实验在IBMQ处理器上使用transmon量子比特，成功测量了受退极化通道和振幅阻尼通道影响的两量子比特纠缠态。通过最大似然方法抑制噪声，并利用SHAP分析优化投影集训练ANN估计纠缠和非局域性。该方法简化了多拷贝测量，提高了噪声鲁棒性，无需复杂非线性方程分析，显著推动了AI辅助量子测量的实际应用。","title":"Resource-Efficient Quantum Correlation Measurement: A Multicopy Neural Network Approach for Practical Applications"},{"content":"Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols? 原始链接: 查看原文\n原始摘要 An AI control protocol is a plan for usefully deploying AI systems that prevents an AI from intentionally causing some unacceptable outcome. Previous work evaluated protocols by subverting them using an AI following a human-written strategy. This paper investigates how well AI systems can generate and act on their own strategies for subverting control protocols whilst operating statelessly (without shared memory between contexts). To do this, an AI system may need to reliably generate optimal plans in each context, take actions with well-calibrated probabilities, and coordinate plans with other instances of itself without communicating. We develop Subversion Strategy Eval, a suite of eight environments, covering a range of protocols and strategic capabilities, and six sets of affordances that help isolate individual capabilities. We implement the evaluation in Inspect-AI and release it open-source. We evaluate Claude 3.5 models, including helpful-only versions, as well as OpenAI reasoning models. None of the models demonstrate substantial capability in strategizing to subvert control protocols statelessly. However, providing models with additional affordances, such as the ability to share a plan between contexts, can substantially improve performance. We hope our evaluations can act as a leading indicator for when models are capable of subverting control protocols and also relax the worst-case assumption of perfect strategic ability in AI control evaluations.\nAI 摘要 本文研究了AI系统在无状态操作下（即上下文间无共享内存）生成并执行策略以规避控制协议的能力。通过开发“Subversion Strategy Eval”评估套件，包含八个环境和六组功能，测试了Claude 3.5和OpenAI模型。结果显示，当前模型在无状态下规避控制协议的能力有限，但增加功能（如上下文间共享计划）可显著提升性能。该评估旨在作为模型规避控制协议能力的早期指标，并缓解AI控制评估中对完美策略能力的极端假设。评估工具已开源发布。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:02:24+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/subversion-strategy-eval-can-language-models-state/","summary":"本文研究了AI系统在无状态操作下（即上下文间无共享内存）生成并执行策略以规避控制协议的能力。通过开发“Subversion Strategy Eval”评估套件，包含八个环境和六组功能，测试了Claude 3.5和OpenAI模型。结果显示，当前模型在无状态下规避控制协议的能力有限，但增加功能（如上下文间共享计划）可显著提升性能。该评估旨在作为模型规避控制协议能力的早期指标，并缓解AI控制评估中对完美策略能力的极端假设。评估工具已开源发布。","title":"Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?"},{"content":"From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment 原始链接: 查看原文\n原始摘要 Large language models (LLMs) have traditionally been aligned through one-size-fits-all approaches that assume uniform human preferences, fundamentally overlooking the diversity in user values and needs. This paper introduces a comprehensive framework for scalable personalized alignment of LLMs. We establish a systematic preference space characterizing psychological and behavioral dimensions, alongside diverse persona representations for robust preference inference in real-world scenarios. Building upon this foundation, we introduce \\textsc{AlignX}, a large-scale dataset of over 1.3 million personalized preference examples, and develop two complementary alignment approaches: \\textit{in-context alignment} directly conditioning on persona representations and \\textit{preference-bridged alignment} modeling intermediate preference distributions. Extensive experiments demonstrate substantial improvements over existing methods, with an average 17.06% accuracy gain across four benchmarks while exhibiting a strong adaptation capability to novel preferences, robustness to limited user data, and precise preference controllability. These results validate our framework\u0026rsquo;s effectiveness, advancing toward truly user-adaptive AI systems.\nAI 摘要 本文提出了一种可扩展的个性化对齐大语言模型（LLMs）的框架，突破了传统“一刀切”方法的局限。通过建立系统化的偏好空间和多样化的人物表征，结合包含130万个性化偏好示例的AlignX数据集，开发了两种对齐方法：基于人物表征的上下文对齐和基于中间偏好分布的偏好桥接对齐。实验表明，该方法在四个基准测试中平均提升了17.06%的准确率，展现出对新偏好的强适应性、对有限用户数据的鲁棒性以及精确的偏好控制能力，推动了真正适应用户的AI系统的发展。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:01:49+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/from-1000000-users-to-every-user-scaling-up-person/","summary":"本文提出了一种可扩展的个性化对齐大语言模型（LLMs）的框架，突破了传统“一刀切”方法的局限。通过建立系统化的偏好空间和多样化的人物表征，结合包含130万个性化偏好示例的AlignX数据集，开发了两种对齐方法：基于人物表征的上下文对齐和基于中间偏好分布的偏好桥接对齐。实验表明，该方法在四个基准测试中平均提升了17.06%的准确率，展现出对新偏好的强适应性、对有限用户数据的鲁棒性以及精确的偏好控制能力，推动了真正适应用户的AI系统的发展。","title":"From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment"},{"content":"More Information is Not Always Better: Connections between Zero-Sum Local Nash Equilibria in Feedback and Open-Loop Information Patterns 原始链接: 查看原文\n原始摘要 Non-cooperative dynamic game theory provides a principled approach to modeling sequential decision-making among multiple noncommunicative agents. A key focus has been on finding Nash equilibria in two-agent zero-sum dynamic games under various information structures. A well-known result states that in linear-quadratic games, unique Nash equilibria under feedback and open-loop information structures yield identical trajectories. Motivated by two key perspectives \u0026ndash; (i) many real-world problems extend beyond linear-quadratic settings and lack unique equilibria, making only local Nash equilibria computable, and (ii) local open-loop Nash equilibria (OLNE) are easier to compute than local feedback Nash equilibria (FBNE) \u0026ndash; it is natural to ask whether a similar result holds for local equilibria in zero-sum games. To this end, we establish that for a broad class of zero-sum games with potentially nonconvex-nonconcave objectives and nonlinear dynamics: (i) the state/control trajectory of a local FBNE satisfies local OLNE first-order optimality conditions, and vice versa, (ii) a local FBNE trajectory satisfies local OLNE second-order necessary conditions, (iii) a local FBNE trajectory satisfying feedback sufficiency conditions also constitutes a local OLNE, and (iv) with additional hard constraints on agents\u0026rsquo; actuations, a local FBNE where strict complementarity holds also satisfies local OLNE first-order optimality conditions, and vice versa.\nAI 摘要 非合作动态博弈理论为多智能体在无通信情况下的序贯决策提供了建模方法。研究重点在于寻找不同信息结构下双智能体零和动态博弈的纳什均衡。在线性二次博弈中，反馈和开环信息结构下的唯一纳什均衡轨迹相同。本文扩展至非线性动态和非凸非凹目标函数，证明局部反馈纳什均衡（FBNE）满足局部开环纳什均衡（OLNE）的一阶和二阶最优性条件，且在满足反馈充分条件时构成局部OLNE。此外，在严格互补条件下，局部FBNE也满足局部OLNE的一阶最优性条件。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:01:17+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/more-information-is-not-always-better-connections-/","summary":"非合作动态博弈理论为多智能体在无通信情况下的序贯决策提供了建模方法。研究重点在于寻找不同信息结构下双智能体零和动态博弈的纳什均衡。在线性二次博弈中，反馈和开环信息结构下的唯一纳什均衡轨迹相同。本文扩展至非线性动态和非凸非凹目标函数，证明局部反馈纳什均衡（FBNE）满足局部开环纳什均衡（OLNE）的一阶和二阶最优性条件，且在满足反馈充分条件时构成局部OLNE。此外，在严格互补条件下，局部FBNE也满足局部OLNE的一阶最优性条件。","title":"More Information is Not Always Better: Connections between Zero-Sum Local Nash Equilibria in Feedback and Open-Loop Information Patterns"},{"content":"SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks 原始链接: 查看原文\n原始摘要 Large language model (LLM) agents need to perform multi-turn interactions in real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM agents fail to perform effective credit assignment over multiple turns while leveraging the generalization capabilities of LLMs and it remains unclear how to develop such algorithms. To study this, we first introduce a new benchmark, ColBench, where an LLM agent interacts with a human collaborator over multiple turns to solve realistic tasks in backend programming and frontend design. Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with Step-WisE Evaluation from Training-time information), that uses a carefully designed optimization objective to train a critic model with access to additional training-time information. The critic provides step-level rewards for improving the policy model. Our experiments demonstrate that SWEET-RL achieves a 6% absolute improvement in success and win rates on ColBench compared to other state-of-the-art multi-turn RL algorithms, enabling Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic collaborative content creation.\nAI 摘要 大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:01:40+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo/","summary":"大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。","title":"SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks"},{"content":"Value Profiles for Encoding Human Variation 原始链接: 查看原文\n原始摘要 Modelling human variation in rating tasks is crucial for enabling AI systems for personalization, pluralistic model alignment, and computational social science. We propose representing individuals using value profiles \u0026ndash; natural language descriptions of underlying values compressed from in-context demonstrations \u0026ndash; along with a steerable decoder model to estimate ratings conditioned on a value profile or other rater information. To measure the predictive information in rater representations, we introduce an information-theoretic methodology. We find that demonstrations contain the most information, followed by value profiles and then demographics. However, value profiles offer advantages in terms of scrutability, interpretability, and steerability due to their compressed natural language format. Value profiles effectively compress the useful information from demonstrations (\u0026gt;70% information preservation). Furthermore, clustering value profiles to identify similarly behaving individuals better explains rater variation than the most predictive demographic groupings. Going beyond test set performance, we show that the decoder models interpretably change ratings according to semantic profile differences, are well-calibrated, and can help explain instance-level disagreement by simulating an annotator population. These results demonstrate that value profiles offer novel, predictive ways to describe individual variation beyond demographics or group information.\nAI 摘要 本研究提出了一种通过价值档案（自然语言描述）和可调控解码器模型来建模人类评分任务中的个体差异的方法。价值档案通过上下文演示压缩生成，能够有效保留超过70%的有用信息。研究发现，演示包含最多信息，其次是价值档案和人口统计信息。价值档案在可审查性、可解释性和可调控性方面具有优势，且通过聚类价值档案能更好地解释评分者差异。解码器模型能够根据语义差异调整评分，具有良好的校准性，并能模拟注释者群体以解释实例级分歧。价值档案为描述个体差异提供了新的预测方法。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T13:01:29+08:00 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/value-profiles-for-encoding-human-variation/","summary":"本研究提出了一种通过价值档案（自然语言描述）和可调控解码器模型来建模人类评分任务中的个体差异的方法。价值档案通过上下文演示压缩生成，能够有效保留超过70%的有用信息。研究发现，演示包含最多信息，其次是价值档案和人口统计信息。价值档案在可审查性、可解释性和可调控性方面具有优势，且通过聚类价值档案能更好地解释评分者差异。解码器模型能够根据语义差异调整评分，具有良好的校准性，并能模拟注释者群体以解释实例级分歧。价值档案为描述个体差异提供了新的预测方法。","title":"Value Profiles for Encoding Human Variation"},{"content":"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment网页链接本文提出了一种新的微调方法LiFT，通过利用人工反馈来优化文本到视频生成模型的匹\u0026hellip; 原始链接: 查看原文\n原始摘要 LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment网页链接\n本文提出了一种新的微调方法LiFT，通过利用人工反馈来优化文本到视频生成模型的匹配度。研究团队首先构建了一个包含大约1万个由人工评分及其理由组成的人类评分注释数据集LiFT-HRA。基于此数据集，训练了一个奖励模型LiFT-Critic，该模型可以有效地学习奖励函数，作为人类判断的代理，衡量给定视频与人类期望之间的匹配度。最后，利用学到的奖励函数通过最大化奖励加权的似然性来调整T2V模型。以CogVideoX-2B为案例，研究结果显示经过微调的模型在所有16项指标上均优于CogVideoX-5B，证明了人类反馈在提高生成视频的匹配度和质量方面的潜力。#生成模型##奖励模型#人工智能硕士论文#开源#\nAI 摘要 本文介绍了一种名为LiFT的新微调方法，旨在通过利用人工反馈来优化文本到视频生成模型的匹配度。研究团队首先构建了一个包含1万个人工评分及其理由的数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，该模型能够学习奖励函数，作为人类判断的代理，衡量视频与人类期望的匹配度。最后，利用学到的奖励函数通过最大化奖励加权的似然性来调整T2V模型。实验结果显示，经过微调的模型在所有16项指标上均优于CogVideoX-5B，证明了人类反馈在提高生成视频匹配度和质量方面的潜力。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:05:11Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/lift-leveraging-human-feedback-for-text-to-video-m/","summary":"本文介绍了一种名为LiFT的新微调方法，旨在通过利用人工反馈来优化文本到视频生成模型的匹配度。研究团队首先构建了一个包含1万个人工评分及其理由的数据集LiFT-HRA，并基于此训练了一个奖励模型LiFT-Critic，该模型能够学习奖励函数，作为人类判断的代理，衡量视频与人类期望的匹配度。最后，利用学到的奖励函数通过最大化奖励加权的似然性来调整T2V模型。实验结果显示，经过微调的模型在所有16项指标上均优于CogVideoX-5B，证明了人类反馈在提高生成视频匹配度和质量方面的潜力。","title":"LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment网页链接本文提出了一种新的微调方法LiFT，通过利用人工反馈来优化文本到视频生成模型的匹..."},{"content":"AMiner——新一代智能型科技情报挖掘与服务系统，能够为你提供查找论文、理解论文、分析论文、写作论文四位一体一站式服务，拥有中英文文献检索、文献辅助阅读、\u0026hellip; 原始链接: 查看原文\n原始摘要 AMiner——新一代智能型科技情报挖掘与服务系统，能够为你提供查找论文、理解论文、分析论文、写作论文四位一体一站式服务，拥有中英文文献检索、文献辅助阅读、翻译等功能，收录有42个学科3.5亿论文、0.6亿学者、1.5亿专利、2600万科研项目。\nAMiner使用网址：网页链接\n一、中、英文文献检索：\nAMiner是一个多功能的学术搜索平台，它高效地帮助用户定位到与他们的研究兴趣紧密相连的学术文献，显著减少了在大量文献中筛选关键信息所需的时间和精力。\n另外，该平台提供了一种互动式问答机制，用户通过简单的提问就能够迅速掌握检索到的文献内容，并且系统还能自动生成文献摘要，这极大地提高了撰写文章的效率。\n二、英文文献翻译：\nAMiner平台不仅提供了强大的学术文献搜索、语言转换和智能问答功能，还增加了文档识别和辅助阅读的实用工具。用户可以在统一的界面中直接从AMiner的文献库中挑选论文，或者自己上传PDF文档。\n利用平台的AI技术，这些论文可以快速生成简洁的摘要、章节概要，并提供相关问题建议。用户还可以在对话界面对文献内容进行提问，以便更深入、更准确地理解文献内容。\n三、科研情报订阅：\nAMiner平台提供了一项科技情报的定制化订阅服务，该服务可根据用户的个人喜好发送最新的科研动态。用户只需设定订阅偏好，包括关注的机构、学者和关键词，AMiner便会自动筛选相关论文并制作摘要，通过微信小程序或电子邮件发送给用户，使用户能够即时获取最新的学术资讯。\n此外，AMiner还提供学者画像、学术会议和必读论文专题和排行榜等功能。AMiner浏览器检索就可以免费使用，欢迎体验反馈！\nAMiner网站入口：网页链接\nAMiner AI使用页：网页链接考研[超话]#\nAI 摘要 AMiner是一个智能科技情报挖掘与服务系统，提供论文查找、理解、分析和写作的一站式服务。它支持中英文文献检索、辅助阅读和翻译功能，涵盖42个学科的3.5亿论文、0.6亿学者、1.5亿专利和2600万科研项目。AMiner通过互动式问答机制和自动生成文献摘要，提高研究效率。此外，它还提供科研情报订阅服务，根据用户偏好推送最新学术动态。AMiner还具备学者画像、学术会议和必读论文专题等功能，用户可通过浏览器免费使用。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:04:28Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/aminer/","summary":"AMiner是一个智能科技情报挖掘与服务系统，提供论文查找、理解、分析和写作的一站式服务。它支持中英文文献检索、辅助阅读和翻译功能，涵盖42个学科的3.5亿论文、0.6亿学者、1.5亿专利和2600万科研项目。AMiner通过互动式问答机制和自动生成文献摘要，提高研究效率。此外，它还提供科研情报订阅服务，根据用户偏好推送最新学术动态。AMiner还具备学者画像、学术会议和必读论文专题等功能，用户可通过浏览器免费使用。","title":"AMiner——新一代智能型科技情报挖掘与服务系统，能够为你提供查找论文、理解论文、分析论文、写作论文四位一体一站式服务，拥有中英文文献检索、文献辅助阅读、..."},{"content":"Evaluating and Aligning CodeLLMs on Human Preference网页链接这篇论文研究了代码生成大型语言模型（codeLLMs）的性能评估和与人类偏好对齐的问题。现有的大部\u0026hellip; 原始链接: 查看原文\n原始摘要 Evaluating and Aligning CodeLLMs on Human Preference网页链接\n这篇论文研究了代码生成大型语言模型（codeLLMs）的性能评估和与人类偏好对齐的问题。现有的大部分代码相关基准测试主要关注生成正确代码片段的能力，但忽略了与人类偏好的匹配。为此，论文提出了一种严格的人类策划的基准CodeArena，模拟现实世界编程任务的复杂性和多样性，并从用户查询中精心选取了397个高质量样本，涵盖了40个类别和44种编程语言。此外，论文还构建了一个多样化的合成指令语料库SynCode-Instruct（近20B个标记），通过扩展网站上的指令来验证大规模合成指令微调的有效性。实验表明，基于执行的基准测试与CodeArena之间存在性能差异，且在40多个大型语言模型上的系统实验显示，开源的顶级代码LLM（如Qwen2.5-Coder）与专有LLM（如OpenAI o1）之间存在显著的性能差距，突显了与人类偏好对齐的重要性。#语言模型#人工智能科研研究生\nAI 摘要 这篇论文探讨了代码生成大型语言模型（codeLLMs）在性能评估和与人类偏好对齐方面的问题。现有基准测试主要关注代码生成正确性，而忽略了人类偏好。为此，研究提出了CodeArena基准，模拟现实编程任务，包含397个高质量样本，涵盖40个类别和44种编程语言。此外，研究构建了SynCode-Instruct语料库（近20B标记），验证大规模合成指令微调的有效性。实验显示，开源顶级代码LLM（如Qwen2.5-Coder）与专有LLM（如OpenAI o1）存在显著性能差距，强调了与人类偏好对齐的重要性。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:04:06Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/evaluating-and-aligning-codellms-on-human-preferen/","summary":"这篇论文探讨了代码生成大型语言模型（codeLLMs）在性能评估和与人类偏好对齐方面的问题。现有基准测试主要关注代码生成正确性，而忽略了人类偏好。为此，研究提出了CodeArena基准，模拟现实编程任务，包含397个高质量样本，涵盖40个类别和44种编程语言。此外，研究构建了SynCode-Instruct语料库（近20B标记），验证大规模合成指令微调的有效性。实验显示，开源顶级代码LLM（如Qwen2.5-Coder）与专有LLM（如OpenAI o1）存在显著性能差距，强调了与人类偏好对齐的重要性。","title":"Evaluating and Aligning CodeLLMs on Human Preference网页链接这篇论文研究了代码生成大型语言模型（codeLLMs）的性能评估和与人类偏好对齐的问题。现有的大部..."},{"content":"Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion网页链接本文提出了一种名为Florence-VL的新型多模态\u0026hellip; 原始链接: 查看原文\n原始摘要 Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion网页链接\n本文提出了一种名为Florence-VL的新型多模态大型语言模型（MLLM），通过集成生成性视觉基础模型Florence-2来增强视觉表征。与常见的CLIP风格的视觉变换器不同，Florence-2能够捕捉到视觉特征的不同层次和方面，使其能够适应多种下游任务。研究团队设计了一种新颖的特征融合架构和创新的训练方法，将Florence-2的视觉特征有效融合到预训练的语言模型Phi 3.5和LLama 3中。特别提出了一种“深度-广度融合（DBFusion）”方法，用于融合不同深度和多个提示下提取的视觉特征。模型的训练包括整个模型的端到端预训练，以及对投影层和语言模型的微调，使用精心设计的包含高质量图像标题和指令调整对的多种开源数据集。定量分析和视觉化结果显示，Florence-VL在视觉语言对齐方面优于流行的视觉编码器，其中增强的深度和广度发挥了重要作用。Florence-VL在各种多模态和视觉为中心的基准测试中，如通用VQA、感知、幻觉、OCR、图表、知识密集型理解等方面，都取得了显著改进。为促进未来研究，研究团队开源了模型和完整的训练方法。#模型训练##大语言模型#人工智能硕士论文论文写作\nAI 摘要 本文介绍了Florence-VL，一种新型的多模态大型语言模型（MLLM），通过集成生成性视觉基础模型Florence-2来增强视觉表征。Florence-VL采用了一种名为“深度-广度融合（DBFusion）”的方法，有效融合了Florence-2的视觉特征到预训练的语言模型Phi 3.5和LLama 3中。该模型在多种多模态和视觉为中心的基准测试中表现出色，特别是在视觉语言对齐方面优于流行的视觉编码器。研究团队开源了模型和训练方法，以促进未来研究。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:04:55Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/florence-vl-enhancing-vision-language-models-with-/","summary":"本文介绍了Florence-VL，一种新型的多模态大型语言模型（MLLM），通过集成生成性视觉基础模型Florence-2来增强视觉表征。Florence-VL采用了一种名为“深度-广度融合（DBFusion）”的方法，有效融合了Florence-2的视觉特征到预训练的语言模型Phi 3.5和LLama 3中。该模型在多种多模态和视觉为中心的基准测试中表现出色，特别是在视觉语言对齐方面优于流行的视觉编码器。研究团队开源了模型和训练方法，以促进未来研究。","title":"Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion网页链接本文提出了一种名为Florence-VL的新型多模态..."},{"content":"From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge网页链接本文全面探讨了大型语言模型（LLM）作为评判者的应用与评估，对LLM在评\u0026hellip; 原始链接: 查看原文\n原始摘要 From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge网页链接\n本文全面探讨了大型语言模型（LLM）作为评判者的应用与评估，对LLM在评分、排名和选择等多种任务中的应用进行了深入研究。文章首先从输入和输出两个角度详细定义了LLM评判者的概念，接着从评判内容、评判方式和评判领域三个维度构建了分类体系。最后，文章汇总了评估LLM评判者的基准，并突出了关键挑战和潜在的研究方向，旨在为这一新兴研究领域提供有价值的见解并激发未来的研究。#LLM# 语言模型人工智能硕士论文论文写作#大模型#\nAI 摘要 本文探讨了大型语言模型（LLM）作为评判者的应用与评估，详细定义了LLM评判者的概念，并从评判内容、方式和领域三个维度构建了分类体系。文章还汇总了评估LLM评判者的基准，突出了关键挑战和潜在研究方向，旨在为这一新兴领域提供见解并激发未来研究。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:04:15Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/from-generation-to-judgment-opportunities-and-chal/","summary":"本文探讨了大型语言模型（LLM）作为评判者的应用与评估，详细定义了LLM评判者的概念，并从评判内容、方式和领域三个维度构建了分类体系。文章还汇总了评估LLM评判者的基准，突出了关键挑战和潜在研究方向，旨在为这一新兴领域提供见解并激发未来研究。","title":"From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge网页链接本文全面探讨了大型语言模型（LLM）作为评判者的应用与评估，对LLM在评..."},{"content":"Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis网页链接本文介绍了一种名为Infinity的位运算视觉自回归模型，该模型\u0026hellip; 原始链接: 查看原文\n原始摘要 Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis网页链接\n本文介绍了一种名为Infinity的位运算视觉自回归模型，该模型能够根据语言指令生成高分辨率、逼真的图像。Infinity在位运算标记预测框架下重新定义了视觉自回归模型，通过无限词汇量的标记分类器和位运算自我校正机制，显著提高了生成能力和细节表现。该方法理论上将标记词汇量扩展至无限，并同步扩大了变换器规模，相较于传统的自回归模型，展现出了更强大的扩展能力。Infinity在自回归文本到图像模型中创下新纪录，性能超过了顶尖的扩散模型如SD3-Medium和SDXL。特别地，Infinity将GenEval基准分数从0.62提升至0.73，ImageReward基准分数从0.87提升至0.96，在0.8秒内完成1024x1024图像的生成，速度是SD3-Medium的2.6倍，成为最快的文本到图像模型。研究团队将发布模型和代码，以推动Infinity在视觉生成和统一标记建模领域的进一步探索。#回归模型##扩散模型##语言模型#人工智能硕士论文论文写作\nAI 摘要 本文介绍了一种名为Infinity的位运算视觉自回归模型，能够根据语言指令生成高分辨率、逼真的图像。Infinity通过无限词汇量的标记分类器和位运算自我校正机制，显著提高了生成能力和细节表现。该方法将标记词汇量扩展至无限，并同步扩大了变换器规模，展现出更强大的扩展能力。Infinity在自回归文本到图像模型中创下新纪录，性能超过了顶尖的扩散模型如SD3-Medium和SDXL，并在0.8秒内完成1024x1024图像的生成，速度是SD3-Medium的2.6倍。研究团队将发布模型和代码以推动进一步探索。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:04:42Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/infinity-scaling-bitwise-autoregressive-modeling-f/","summary":"本文介绍了一种名为Infinity的位运算视觉自回归模型，能够根据语言指令生成高分辨率、逼真的图像。Infinity通过无限词汇量的标记分类器和位运算自我校正机制，显著提高了生成能力和细节表现。该方法将标记词汇量扩展至无限，并同步扩大了变换器规模，展现出更强大的扩展能力。Infinity在自回归文本到图像模型中创下新纪录，性能超过了顶尖的扩散模型如SD3-Medium和SDXL，并在0.8秒内完成1024x1024图像的生成，速度是SD3-Medium的2.6倍。研究团队将发布模型和代码以推动进一步探索。","title":"Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis网页链接本文介绍了一种名为Infinity的位运算视觉自回归模型，该模型..."},{"content":"LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision 网页链接本研究提出了一种名为LatentSync的端到端唇同\u0026hellip; 原始链接: 查看原文\n原始摘要 LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision 网页链接\n本研究提出了一种名为LatentSync的端到端唇同步框架，基于音频条件的潜在扩散模型，无需中间运动表示，与先前的基于像素空间扩散或两阶段生成的唇同步方法不同。该框架能够利用Stable Diffusion的强大能力，直接建模复杂的音频视觉相关性。研究中发现，基于扩散的唇同步方法在不同帧的扩散过程中存在时间一致性不足的问题。为此，研究提出了时间表示对齐（TREPA）方法以增强时间一致性，同时保持唇同步的准确性。TREPA利用大规模自监督视频模型提取的时间表示来对齐生成的帧与真实帧。此外，研究还观察到了SyncNet收敛问题，并进行了全面的实证研究，识别了影响SyncNet收敛的关键因素，包括模型架构、训练超参数和数据预处理方法。通过改变SyncNet的整体训练框架，显著提高了SyncNet的准确性。这些经验也可应用于其他使用SyncNet的唇同步和音频驱动的人像动画方法。基于这些创新，该方法在HDTF和VoxCeleb2数据集上的多个指标上超越了现有的唇同步方法。人工智能大模型硕士论文\nAI 摘要 本研究提出了LatentSync，一种基于音频条件的潜在扩散模型的端到端唇同步框架，无需中间运动表示，直接建模音频与视觉的复杂关系。为解决扩散过程中时间一致性问题，提出了时间表示对齐（TREPA）方法，利用自监督视频模型增强生成帧与真实帧的时间一致性。研究还优化了SyncNet的训练框架，提升了其准确性。实验表明，该方法在HDTF和VoxCeleb2数据集上超越了现有唇同步方法，为音频驱动的人像动画提供了新的解决方案。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T04:03:43Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/latentsync-taming-audio-conditioned-latent-diffusi/","summary":"本研究提出了LatentSync，一种基于音频条件的潜在扩散模型的端到端唇同步框架，无需中间运动表示，直接建模音频与视觉的复杂关系。为解决扩散过程中时间一致性问题，提出了时间表示对齐（TREPA）方法，利用自监督视频模型增强生成帧与真实帧的时间一致性。研究还优化了SyncNet的训练框架，提升了其准确性。实验表明，该方法在HDTF和VoxCeleb2数据集上超越了现有唇同步方法，为音频驱动的人像动画提供了新的解决方案。","title":"LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision 网页链接本研究提出了一种名为LatentSync的端到端唇同..."},{"content":"#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，\u0026hellip; 原始链接: 查看原文\n原始摘要 #路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——\n共计收集和整理了涉及8500+个LLM，在12个Benchmark上的共2亿条性能记录！\n先来简单科普一下路由LLM。\n这种方法主要是把像ChatGPT、Qwen、DeepSeek这些成型的LLM当作 “专家” ，当给一个输入的时候，有分类能力的Router（路由器）就会把这个输入分配给合适的LLM处理。\n如此一来，就能实现高性能、低计算消耗、低幻觉等目标。\n而来自中山大学和普渡大学的研究人员在基于上述海量的记录做了一番探索之后，发现了一个现象，叫做Model-level Scaling Up。\n一言蔽之，就是一个好的Router，可以让路由LLM范式的性能随着LLM候选数量的增加迅速变强。\n随后，他们通过这些数据构建了针对Router设计的评测RouterEval。\n值得注意的是，其他研究人员，也可以通过RouterEval在很少的计算资源下（如笔记本、单卡GPU上）就能参与到该路由LLM的研究当中。网页链接\nAI 摘要 中山大学和普渡大学的研究团队对路由LLM（Routing LLM）进行了迄今为止最全面的研究，涉及8500多个LLM在12个基准测试中的2亿条性能记录。路由LLM通过将多个LLM视为“专家”，并由Router（路由器）根据输入选择最合适的LLM进行处理，从而实现高性能、低计算消耗和低幻觉的目标。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，称为“Model-level Scaling Up”。团队还开发了RouterEval评测工具，使其他研究人员能够在有限的计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:15:14Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/llmscaling-up-llmrouting-llm8500llm/","summary":"中山大学和普渡大学的研究团队对路由LLM（Routing LLM）进行了迄今为止最全面的研究，涉及8500多个LLM在12个基准测试中的2亿条性能记录。路由LLM通过将多个LLM视为“专家”，并由Router（路由器）根据输入选择最合适的LLM进行处理，从而实现高性能、低计算消耗和低幻觉的目标。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，称为“Model-level Scaling Up”。团队还开发了RouterEval评测工具，使其他研究人员能够在有限的计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。","title":"#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，..."},{"content":"#豆包编程能力升级##用豆包3分钟做个小游戏#豆包编程能力升级了，现在3分钟就能做出专属小游戏。升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整\u0026hellip; 原始链接: 查看原文\n原始摘要 #豆包编程能力升级##用豆包3分钟做个小游戏#\n豆包编程能力升级了，现在3分钟就能做出专属小游戏。\n升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整项目。\n我们先来看看HTML代码实时预览，做出的小游戏效果：\n1.点击和高亮的英文泡泡意思相同的中文泡泡，就能消除单词；【图1】\n2.点击屏幕中的贺卡，可以随机抽取一句生日祝福语；【图2】\n3.完成4道测试题，就能自动生成你的MBTI人格。【图3】\n而且，豆包生成代码后，多了一个运行按钮，在豆包内部就能运行Python代码了。【图4】\nAI 摘要 豆包编程能力升级，现支持3分钟内制作专属小游戏。新功能包括HTML代码实时预览、Python运行及完整项目生成。用户可通过点击操作消除单词、抽取生日祝福语及生成MBTI人格测试结果。豆包还新增运行按钮，可直接在平台内运行Python代码。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:14:54Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/33htmlpython/","summary":"豆包编程能力升级，现支持3分钟内制作专属小游戏。新功能包括HTML代码实时预览、Python运行及完整项目生成。用户可通过点击操作消除单词、抽取生日祝福语及生成MBTI人格测试结果。豆包还新增运行按钮，可直接在平台内运行Python代码。","title":"#豆包编程能力升级##用豆包3分钟做个小游戏#豆包编程能力升级了，现在3分钟就能做出专属小游戏。升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整..."},{"content":"Information Fusion in Smart Agriculture: Machine Learning Applications and Future Research Directions 原始链接: 查看原文\n原始摘要 Machine learning (ML) is a rapidly evolving technology with expanding applications across various fields. This paper presents a comprehensive survey of recent ML applications in agriculture for sustainability and efficiency. Existing reviews mainly focus on narrow subdomains or lack a fusion-driven perspectives. This study provides a combined analysis of ML applications in agriculture, structured around five key objectives: (i) Analyzing ML techniques across pre-harvesting, harvesting, and post-harvesting phases. (ii) Demonstrating how ML can be used with agricultural data and data fusion. (iii) Conducting a bibliometric and statistical analysis to reveal research trends and activity. (iv) Investigating real-world case studies of leading artificial intelligence (AI)-driven agricultural companies that use different types of multisensors and multisource data. (v) Compiling publicly available datasets to support ML model training. Going beyond existing previous reviews, this review focuses on how machine learning (ML) techniques, combined with multi-source data fusion (integrating remote sensing, IoT, and climate analytics), enhance precision agriculture by improving predictive accuracy and decision-making. Case studies and statistical insights illustrate the evolving landscape of AI driven smart farming, while future research directions also discusses challenges associated with data fusion for heterogeneous datasets. This review bridges the gap between AI research and agricultural applications, offering a roadmap for researchers, industry professionals, and policymakers to harness information fusion and ML for advancing precision agriculture.\nAI 摘要 本文综述了机器学习（ML）在农业可持续发展与效率提升中的广泛应用，填补了现有研究在跨领域融合视角上的不足。研究围绕五大目标展开：分析ML在农业各阶段的应用、探讨ML与农业数据及数据融合的结合、进行文献计量与统计分析、考察AI驱动农业企业的实际案例，以及整理公开数据集以支持ML模型训练。通过多源数据融合（如遥感、物联网和气候分析），ML提升了精准农业的预测精度与决策能力。本文为研究者、行业专家及政策制定者提供了利用信息融合与ML推动精准农业发展的路线图。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:12:05Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/information-fusion-in-smart-agriculture-machine-le/","summary":"本文综述了机器学习（ML）在农业可持续发展与效率提升中的广泛应用，填补了现有研究在跨领域融合视角上的不足。研究围绕五大目标展开：分析ML在农业各阶段的应用、探讨ML与农业数据及数据融合的结合、进行文献计量与统计分析、考察AI驱动农业企业的实际案例，以及整理公开数据集以支持ML模型训练。通过多源数据融合（如遥感、物联网和气候分析），ML提升了精准农业的预测精度与决策能力。本文为研究者、行业专家及政策制定者提供了利用信息融合与ML推动精准农业发展的路线图。","title":"Information Fusion in Smart Agriculture: Machine Learning Applications and Future Research Directions"},{"content":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions 原始链接: 查看原文\n原始摘要 This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.\nAI 摘要 本研究探讨了大型语言模型（LLMs）在人工智能领域的伦理问题，包括与其他AI系统共有的隐私和公平性挑战，以及LLMs特有的问题，如幻觉、可验证的责任和审查复杂性。研究强调需解决这些复杂性以确保责任性、减少偏见并增强透明度，特别是在信息传播方面。提出了缓解策略和未来方向，倡导跨学科合作，推荐针对特定领域的伦理框架和适应多样环境的动态审计系统，旨在指导LLMs的负责任开发和整合，设想伦理考虑主导AI社会进步的未来。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:12:21Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/navigating-llm-ethics-advancements-challenges-and-/","summary":"本研究探讨了大型语言模型（LLMs）在人工智能领域的伦理问题，包括与其他AI系统共有的隐私和公平性挑战，以及LLMs特有的问题，如幻觉、可验证的责任和审查复杂性。研究强调需解决这些复杂性以确保责任性、减少偏见并增强透明度，特别是在信息传播方面。提出了缓解策略和未来方向，倡导跨学科合作，推荐针对特定领域的伦理框架和适应多样环境的动态审计系统，旨在指导LLMs的负责任开发和整合，设想伦理考虑主导AI社会进步的未来。","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions"},{"content":"Cosmos World Foundation Model Platform for Physical AI 原始链接: 查看原文\n原始摘要 Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via https://github.com/nvidia-cosmos/cosmos-predict1.\nAI 摘要 本文介绍了Cosmos世界基础模型平台，旨在帮助开发者为其物理AI系统构建定制化的世界模型。该平台包括视频处理管道、预训练的世界基础模型、模型微调示例和视频标记器。世界基础模型作为通用模型，可微调为下游应用的定制模型。为促进物理AI解决社会关键问题，Cosmos平台开源并提供开放权重模型，许可宽松，可通过https://github.com/nvidia-cosmos/cosmos-predict1获取。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:11:51Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/cosmos-world-foundation-model-platform-for-physica/","summary":"本文介绍了Cosmos世界基础模型平台，旨在帮助开发者为其物理AI系统构建定制化的世界模型。该平台包括视频处理管道、预训练的世界基础模型、模型微调示例和视频标记器。世界基础模型作为通用模型，可微调为下游应用的定制模型。为促进物理AI解决社会关键问题，Cosmos平台开源并提供开放权重模型，许可宽松，可通过https://github.com/nvidia-cosmos/cosmos-predict1获取。","title":"Cosmos World Foundation Model Platform for Physical AI"},{"content":"EnvBench: A Benchmark for Automated Environment Setup 原始链接: 查看原文\n原始摘要 Recent advances in Large Language Models (LLMs) have enabled researchers to focus on practical repository-level tasks in software engineering domain. In this work, we consider a cornerstone task for automating work with software repositories-environment setup, i.e., a task of configuring a repository-specific development environment on a system. Existing studies on environment setup introduce innovative agentic strategies, but their evaluation is often based on small datasets that may not capture the full range of configuration challenges encountered in practice. To address this gap, we introduce a comprehensive environment setup benchmark EnvBench. It encompasses 329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on repositories that present genuine configuration challenges, excluding projects that can be fully configured by simple deterministic scripts. To enable further benchmark extension and usage for model tuning, we implement two automatic metrics: a static analysis check for missing imports in Python and a compilation check for JVM languages. We demonstrate the applicability of our benchmark by evaluating three environment setup approaches, including a simple zero-shot baseline and two agentic workflows, that we test with two powerful LLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to successfully configure 6.69% repositories for Python and 29.47% repositories for JVM, suggesting that EnvBench remains challenging for current approaches. Our benchmark suite is publicly available at https://github.com/JetBrains-Research/EnvBench. The dataset and experiment trajectories are available at https://jb.gg/envbench.\nAI 摘要 近期大型语言模型（LLMs）的进展使得研究者能够专注于软件工程领域的实际仓库级任务。本文提出EnvBench，一个全面的环境设置基准，涵盖329个Python和665个基于JVM（Java、Kotlin）的仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括一个简单的零样本基线和两个代理工作流，使用GPT-4o和GPT-4o-mini进行测试，最佳方法成功配置了6.69%的Python仓库和29.47%的JVM仓库，表明EnvBench对当前方法仍具挑战性。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:11:01Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/envbench-a-benchmark-for-automated-environment-set/","summary":"近期大型语言模型（LLMs）的进展使得研究者能够专注于软件工程领域的实际仓库级任务。本文提出EnvBench，一个全面的环境设置基准，涵盖329个Python和665个基于JVM（Java、Kotlin）的仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括一个简单的零样本基线和两个代理工作流，使用GPT-4o和GPT-4o-mini进行测试，最佳方法成功配置了6.69%的Python仓库和29.47%的JVM仓库，表明EnvBench对当前方法仍具挑战性。","title":"EnvBench: A Benchmark for Automated Environment Setup"},{"content":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play 原始链接: 查看原文\n原始摘要 Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically \u0026ldquo;plays\u0026rdquo; with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.\nAI 摘要 为了解决大型语言模型（LLMs）在零样本工具使用中的挑战，研究者提出了PLAY2PROMPT框架。该框架通过自动“试错”探索工具的输入输出行为，无需标注数据即可优化工具文档并生成使用示例。这些示例不仅指导LLM推理，还作为验证进一步提升工具使用效果。实验表明，PLAY2PROMPT显著提升了开放和封闭模型在零样本任务中的表现，为领域特定工具集成提供了可扩展且有效的解决方案。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:11:13Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/","summary":"为了解决大型语言模型（LLMs）在零样本工具使用中的挑战，研究者提出了PLAY2PROMPT框架。该框架通过自动“试错”探索工具的输入输出行为，无需标注数据即可优化工具文档并生成使用示例。这些示例不仅指导LLM推理，还作为验证进一步提升工具使用效果。实验表明，PLAY2PROMPT显著提升了开放和封闭模型在零样本任务中的表现，为领域特定工具集成提供了可扩展且有效的解决方案。","title":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play"},{"content":"VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms 原始链接: 查看原文\n原始摘要 Escape rooms present a unique cognitive challenge that demands exploration-driven planning: players should actively search their environment, continuously update their knowledge based on new discoveries, and connect disparate clues to determine which elements are relevant to their objectives. Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape rooms specifically designed to evaluate AI models under these challenging conditions, where success depends not only on solving isolated puzzles but also on iteratively constructing and refining spatial-temporal knowledge of a dynamically changing environment. On VisEscape, we observed that even state-of-the-art multimodal models generally fail to escape the rooms, showing considerable variation in their levels of progress and trajectories. To address this issue, we propose VisEscaper, which effectively integrates Memory, Feedback, and ReAct modules, demonstrating significant improvements by performing 3.7 times more effectively and 5.0 times more efficiently on average.\nAI 摘要 VisEscape是一个包含20个虚拟密室逃脱的基准测试，旨在评估AI模型在探索驱动规划中的表现。该测试要求模型不仅解决孤立谜题，还需构建和优化动态变化环境的时空知识。研究发现，即使最先进的多模态模型也难以成功逃脱，表现差异显著。为此，研究者提出了VisEscaper模型，通过整合记忆、反馈和ReAct模块，显著提升了性能，平均效率提高了5倍，效果提升了3.7倍。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:11:35Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/visescape-a-benchmark-for-evaluating-exploration-d/","summary":"VisEscape是一个包含20个虚拟密室逃脱的基准测试，旨在评估AI模型在探索驱动规划中的表现。该测试要求模型不仅解决孤立谜题，还需构建和优化动态变化环境的时空知识。研究发现，即使最先进的多模态模型也难以成功逃脱，表现差异显著。为此，研究者提出了VisEscaper模型，通过整合记忆、反馈和ReAct模块，显著提升了性能，平均效率提高了5倍，效果提升了3.7倍。","title":"VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms"},{"content":"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control 原始链接: 查看原文\n原始摘要 We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.\nAI 摘要 Cosmos-Transfer是一种条件世界生成模型，能够基于多种空间控制输入（如分割、深度和边缘）生成世界模拟。其设计中的空间条件方案具有自适应性和可定制性，允许在不同空间位置对条件输入进行不同权重的调整，从而实现高度可控的世界生成。该模型在Sim2Real等世界到世界转换应用中具有广泛用途，包括物理AI、机器人Sim2Real和自动驾驶数据增强。通过NVIDIA GB200 NVL72机架，模型实现了实时世界生成。为加速该领域研究，模型和代码已在GitHub开源。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:10:03Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/cosmos-transfer1-conditional-world-generation-with/","summary":"Cosmos-Transfer是一种条件世界生成模型，能够基于多种空间控制输入（如分割、深度和边缘）生成世界模拟。其设计中的空间条件方案具有自适应性和可定制性，允许在不同空间位置对条件输入进行不同权重的调整，从而实现高度可控的世界生成。该模型在Sim2Real等世界到世界转换应用中具有广泛用途，包括物理AI、机器人Sim2Real和自动驾驶数据增强。通过NVIDIA GB200 NVL72机架，模型实现了实时世界生成。为加速该领域研究，模型和代码已在GitHub开源。","title":"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control"},{"content":"Don\u0026rsquo;t lie to your friends: Learning what you know from collaborative self-play 原始链接: 查看原文\n原始摘要 To be helpful assistants, AI agents must be aware of their own capabilities and limitations. This includes knowing when to answer from parametric knowledge versus using tools, when to trust tool outputs, and when to abstain or hedge. Such capabilities are hard to teach through supervised fine-tuning because they require constructing examples that reflect the agent\u0026rsquo;s specific capabilities. We therefore propose a radically new approach to teaching agents what they know: \\emph{collaborative self-play}. We construct multi-agent collaborations in which the group is rewarded for collectively arriving at correct answers. The desired meta-knowledge emerges from the incentives built into the structure of the interaction. We focus on small societies of agents that have access to heterogeneous tools (corpus-specific retrieval), and therefore must collaborate to maximize their success while minimizing their effort. Experiments show that group-level rewards for multi-agent communities can induce policies that \\emph{transfer} to improve tool use and selective prediction in settings where individual agents are deployed in isolation.\nAI 摘要 本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出对自身知识的元认知。该方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单个代理独立部署时也能改善工具使用和选择性预测的策略。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:10:46Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/dont-lie-to-your-friends-learning-what-you-know-fr/","summary":"本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出对自身知识的元认知。该方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单个代理独立部署时也能改善工具使用和选择性预测的策略。","title":"Don't lie to your friends: Learning what you know from collaborative self-play"},{"content":"Gricean Norms as a Basis for Effective Collaboration 原始链接: 查看原文\n原始摘要 Effective human-AI collaboration hinges not only on the AI agent\u0026rsquo;s ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks \u0026ndash; common ground, relevance theory, and theory of mind \u0026ndash; into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent\u0026rsquo;s pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.\nAI 摘要 本文提出了一种结合Gricean准则和认知框架的规范性框架，旨在提升基于大型语言模型（LLM）的AI代理在人类-AI协作中的表现。该框架通过整合Gricean准则（数量、质量、关联、方式）和推理，帮助AI代理处理模糊、不完整、无效或不相关的指令。实验表明，采用Gricean准则的Lamoid（基于GPT-4的代理）在任务准确性和生成清晰、准确、上下文相关的响应方面优于未采用准则的版本。这一改进源于规范性框架增强了代理的语用推理能力，促进了有效的人类-AI协作和上下文感知的沟通。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:10:28Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/gricean-norms-as-a-basis-for-effective-collaborati/","summary":"本文提出了一种结合Gricean准则和认知框架的规范性框架，旨在提升基于大型语言模型（LLM）的AI代理在人类-AI协作中的表现。该框架通过整合Gricean准则（数量、质量、关联、方式）和推理，帮助AI代理处理模糊、不完整、无效或不相关的指令。实验表明，采用Gricean准则的Lamoid（基于GPT-4的代理）在任务准确性和生成清晰、准确、上下文相关的响应方面优于未采用准则的版本。这一改进源于规范性框架增强了代理的语用推理能力，促进了有效的人类-AI协作和上下文感知的沟通。","title":"Gricean Norms as a Basis for Effective Collaboration"},{"content":"Measuring AI Ability to Complete Long Tasks 原始链接: 查看原文\n原始摘要 Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models\u0026rsquo; time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results \u0026ndash; including their degree of external validity \u0026ndash; and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.\nAI 摘要 本文提出了一种新指标“50%-任务完成时间范围”，用于量化AI系统在人类能力方面的表现。该指标指人类完成AI模型能以50%成功率完成的任务所需时间。研究发现，当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间范围约为50分钟，且自2019年以来每7个月翻倍，2024年可能加速。AI能力的提升主要源于更高的可靠性、错误适应能力、逻辑推理和工具使用能力。若趋势持续，5年内AI可能自动化许多目前人类需一个月完成的软件任务，但也带来潜在危险能力增加的担忧。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T03:09:45Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/measuring-ai-ability-to-complete-long-tasks/","summary":"本文提出了一种新指标“50%-任务完成时间范围”，用于量化AI系统在人类能力方面的表现。该指标指人类完成AI模型能以50%成功率完成的任务所需时间。研究发现，当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间范围约为50分钟，且自2019年以来每7个月翻倍，2024年可能加速。AI能力的提升主要源于更高的可靠性、错误适应能力、逻辑推理和工具使用能力。若趋势持续，5年内AI可能自动化许多目前人类需一个月完成的软件任务，但也带来潜在危险能力增加的担忧。","title":"Measuring AI Ability to Complete Long Tasks"},{"content":"HunyuanVideo: A Systematic Framework for Large Video Generative 网页链接本文介绍了一种名为HunyuanVideo的开源视频生成基础模型，其性能可以媲美或超越业界\u0026hellip; 原始链接: 查看原文\n原始摘要 HunyuanVideo: A Systematic Framework for Large Video Generative 网页链接\n本文介绍了一种名为HunyuanVideo的开源视频生成基础模型，其性能可以媲美或超越业界领先的闭源模型。HunyuanVideo包含数据筛选、高级架构设计、渐进式模型扩展与训练，以及针对大规模模型训练和推断的效率化基础设施等关键要素。研究者成功训练了一个拥有超过130亿参数的视频生成模型，成为所有开源模型中规模最大的一个。通过一系列实验和针对性设计，HunyuanVideo确保了高视觉质量、运动动态、文本-视频对齐和高级拍摄技术。经专业人士评估，HunyuanVideo超越了包括Runway Gen-3、Luma 1.6在内的先前最先进的模型，以及三个表现最佳的中文视频生成模型。研究者通过公开基础模型及其应用代码，旨在缩小闭源与开源社区之间的差距，并激发社区成员的创意实验，推动视频生成生态系统的活跃发展。相关代码已在GitHub公开。#开源##模型训练##开源模型##生成模型#人工智能#多模态模型#\nAI 摘要 HunyuanVideo是一个开源的大规模视频生成基础模型，性能媲美或超越业界领先的闭源模型。该模型包含数据筛选、高级架构设计、渐进式扩展与训练等关键要素，成功训练了超过130亿参数的视频生成模型，成为开源模型中规模最大的一个。HunyuanVideo在视觉质量、运动动态、文本-视频对齐等方面表现出色，超越了包括Runway Gen-3和Luma 1.6在内的先进模型。研究者公开了基础模型及应用代码，旨在缩小闭源与开源社区的差距，推动视频生成生态系统的发展。相关代码已在GitHub公开。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T02:33:19Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/hunyuanvideo-a-systematic-framework-for-large-vide/","summary":"HunyuanVideo是一个开源的大规模视频生成基础模型，性能媲美或超越业界领先的闭源模型。该模型包含数据筛选、高级架构设计、渐进式扩展与训练等关键要素，成功训练了超过130亿参数的视频生成模型，成为开源模型中规模最大的一个。HunyuanVideo在视觉质量、运动动态、文本-视频对齐等方面表现出色，超越了包括Runway Gen-3和Luma 1.6在内的先进模型。研究者公开了基础模型及应用代码，旨在缩小闭源与开源社区的差距，推动视频生成生态系统的发展。相关代码已在GitHub公开。","title":"HunyuanVideo: A Systematic Framework for Large Video Generative 网页链接本文介绍了一种名为HunyuanVideo的开源视频生成基础模型，其性能可以媲美或超越业界..."},{"content":"#8张GPU训出近SOTA模型# 超低成本图像生成预训练方案来了——仅需8张GPU训练，就能实现近SOTA的高质量图像生成效果。划重点：开源。模型名为LightGen，由港科大H\u0026hellip; 原始链接: 查看原文\n原始摘要 #8张GPU训出近SOTA模型# 超低成本图像生成预训练方案来了——\n仅需8张GPU训练，就能实现近SOTA的高质量图像生成效果。\n划重点：开源。\n模型名为LightGen，由港科大Harry Yang团队联合Everlyn AI等机构打造，借助知识蒸馏（KD）和直接偏好优化（DPO）策略，有效压缩了大规模图像生成模型的训练流程。\nLightGen不仅显著降低了数据规模与计算资源需求，而且在高质量图像生成任务上展现了与SOTA模型相媲美的性能。\nLightGen相较于现有的生成模型，尽管参数量更小、预训练数据规模更精简，却在geneval图像生成任务的基准评测中甚至超出了部分最先进SOTA模型。\n此外，LightGen在效率与性能之间实现了良好的平衡，成功地将传统上需要数千GPU days的预训练过程缩短至仅88个GPU days，即可完成高质量图像生成模型的训练。\n以下是更多细节。\n网页链接\nAI 摘要 港科大Harry Yang团队与Everlyn AI等机构合作开发的LightGen模型，通过知识蒸馏和直接偏好优化策略，显著降低了图像生成模型的训练成本。仅需8张GPU和88个GPU days，LightGen就能实现与当前最先进（SOTA）模型相媲美的高质量图像生成效果。尽管模型参数量和预训练数据规模较小，LightGen在基准评测中甚至超越部分SOTA模型，展示了高效与高性能的平衡。该模型已开源，为低成本图像生成预训练提供了新方案。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-20T00:04:57Z 目录日期: 2025-03-20 ","permalink":"https://example.org/papers/2025-03-20/8gpusota-8gpusotalightgenh/","summary":"港科大Harry Yang团队与Everlyn AI等机构合作开发的LightGen模型，通过知识蒸馏和直接偏好优化策略，显著降低了图像生成模型的训练成本。仅需8张GPU和88个GPU days，LightGen就能实现与当前最先进（SOTA）模型相媲美的高质量图像生成效果。尽管模型参数量和预训练数据规模较小，LightGen在基准评测中甚至超越部分SOTA模型，展示了高效与高性能的平衡。该模型已开源，为低成本图像生成预训练提供了新方案。","title":"#8张GPU训出近SOTA模型# 超低成本图像生成预训练方案来了——仅需8张GPU训练，就能实现近SOTA的高质量图像生成效果。划重点：开源。模型名为LightGen，由港科大H..."},{"content":"#8张GPU训出近SOTA模型# 超低成本图像生成预训练方案来了——仅需8张GPU训练，就能实现近SOTA的高质量图像生成效果。划重点：开源。模型名为LightGen，由港科大H\u0026hellip; 原始链接: 查看原文\n原始摘要 #8张GPU训出近SOTA模型# 超低成本图像生成预训练方案来了——\n仅需8张GPU训练，就能实现近SOTA的高质量图像生成效果。\n划重点：开源。\n模型名为LightGen，由港科大Harry Yang团队联合Everlyn AI等机构打造，借助知识蒸馏（KD）和直接偏好优化（DPO）策略，有效压缩了大规模图像生成模型的训练流程。\nLightGen不仅显著降低了数据规模与计算资源需求，而且在高质量图像生成任务上展现了与SOTA模型相媲美的性能。\nLightGen相较于现有的生成模型，尽管参数量更小、预训练数据规模更精简，却在geneval图像生成任务的基准评测中甚至超出了部分最先进SOTA模型。\n此外，LightGen在效率与性能之间实现了良好的平衡，成功地将传统上需要数千GPU days的预训练过程缩短至仅88个GPU days，即可完成高质量图像生成模型的训练。\n以下是更多细节。\n网页链接\nAI 摘要 港科大Harry Yang团队与Everlyn AI等机构合作开发了名为LightGen的开源图像生成模型。该模型通过知识蒸馏和直接偏好优化策略，显著降低了训练所需的数据规模和计算资源，仅需8张GPU即可实现接近SOTA的高质量图像生成效果。LightGen在参数量和预训练数据规模上更为精简，但在图像生成任务中表现优异，甚至超越部分现有SOTA模型。此外，LightGen将传统上需要数千GPU天的预训练过程缩短至仅88个GPU天，实现了效率与性能的良好平衡。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:04:39Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/8gpusota-8gpusotalightgenh/","summary":"港科大Harry Yang团队与Everlyn AI等机构合作开发了名为LightGen的开源图像生成模型。该模型通过知识蒸馏和直接偏好优化策略，显著降低了训练所需的数据规模和计算资源，仅需8张GPU即可实现接近SOTA的高质量图像生成效果。LightGen在参数量和预训练数据规模上更为精简，但在图像生成任务中表现优异，甚至超越部分现有SOTA模型。此外，LightGen将传统上需要数千GPU天的预训练过程缩短至仅88个GPU天，实现了效率与性能的良好平衡。","title":"#8张GPU训出近SOTA模型# 超低成本图像生成预训练方案来了——仅需8张GPU训练，就能实现近SOTA的高质量图像生成效果。划重点：开源。模型名为LightGen，由港科大H..."},{"content":"#豆包编程能力升级##用豆包3分钟做个小游戏#豆包编程能力升级了，现在3分钟就能做出专属小游戏。升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整\u0026hellip; 原始链接: 查看原文\n原始摘要 #豆包编程能力升级##用豆包3分钟做个小游戏#\n豆包编程能力升级了，现在3分钟就能做出专属小游戏。\n升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整项目。\n我们先来看看HTML代码实时预览，做出的小游戏效果：\n1.点击和高亮的英文泡泡意思相同的中文泡泡，就能消除单词；【图1】\n2.点击屏幕中的贺卡，可以随机抽取一句生日祝福语；【图2】\n3.完成4道测试题，就能自动生成你的MBTI人格。【图3】\n而且，豆包生成代码后，多了一个运行按钮，在豆包内部就能运行Python代码了。【图4】\nAI 摘要 豆包编程能力升级，现支持3分钟内制作专属小游戏。新功能包括HTML代码实时预览、Python运行及完整项目生成。示例游戏包括点击匹配中英文泡泡消除单词、点击贺卡随机抽取生日祝福语、完成测试题生成MBTI人格。此外，豆包新增运行按钮，可直接在平台内运行Python代码。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:04:18Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/33htmlpython/","summary":"豆包编程能力升级，现支持3分钟内制作专属小游戏。新功能包括HTML代码实时预览、Python运行及完整项目生成。示例游戏包括点击匹配中英文泡泡消除单词、点击贺卡随机抽取生日祝福语、完成测试题生成MBTI人格。此外，豆包新增运行按钮，可直接在平台内运行Python代码。","title":"#豆包编程能力升级##用豆包3分钟做个小游戏#豆包编程能力升级了，现在3分钟就能做出专属小游戏。升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整..."},{"content":"#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，\u0026hellip; 原始链接: 查看原文\n原始摘要 #路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——\n共计收集和整理了涉及8500+个LLM，在12个Benchmark上的共2亿条性能记录！\n先来简单科普一下路由LLM。\n这种方法主要是把像ChatGPT、Qwen、DeepSeek这些成型的LLM当作 “专家” ，当给一个输入的时候，有分类能力的Router（路由器）就会把这个输入分配给合适的LLM处理。\n如此一来，就能实现高性能、低计算消耗、低幻觉等目标。\n而来自中山大学和普渡大学的研究人员在基于上述海量的记录做了一番探索之后，发现了一个现象，叫做Model-level Scaling Up。\n一言蔽之，就是一个好的Router，可以让路由LLM范式的性能随着LLM候选数量的增加迅速变强。\n随后，他们通过这些数据构建了针对Router设计的评测RouterEval。\n值得注意的是，其他研究人员，也可以通过RouterEval在很少的计算资源下（如笔记本、单卡GPU上）就能参与到该路由LLM的研究当中。网页链接\nAI 摘要 中山大学和普渡大学的研究团队进行了一项关于路由LLM（Routing LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-level Scaling Up”。此外，团队开发了RouterEval评测工具，使其他研究人员能在有限计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:04:28Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/llmscaling-up-llmrouting-llm8500llm/","summary":"中山大学和普渡大学的研究团队进行了一项关于路由LLM（Routing LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-level Scaling Up”。此外，团队开发了RouterEval评测工具，使其他研究人员能在有限计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。","title":"#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，..."},{"content":"#小米汽车透露今年愿景#在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。卢伟冰\u0026hellip; 原始链接: 查看原文\n原始摘要 #小米汽车透露今年愿景#\n在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。\n2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。\n卢伟冰表示，要在智能驾驶领域实现“一年追三代”，并计划加大AI研发投入，力争在2025年跻身自动驾驶第一梯队。\n目前，小米的智驾系统已经实现高速NOA、端到端泊车、城区NOA全量推送。\n而今年目标则是，进一步优化端到端全场景智驾，让小米汽车能够实现真正的“车位到车位”自动驾驶体验。\n同时，在产品线扩张方面，除了持续提升SU7的产能，小米计划在2024年底推出SUV车型YU7。\nYU7这款车定位更符合中国市场偏好，预计售价区间比SU7更亲民，已经被市场提前预定为“爆款”。\n此外，小米正在加速全球化布局，计划于2027年正式启动海外销售，并在欧洲设立汽车研发中心，吸引国际顶级汽车人才加盟。\n从目前的发展节奏来看，小米汽车不仅有望在国内市场挑战特斯拉Model Y的销量霸主地位，在全球市场也可能掀起一场新的竞争风暴。\nAI 摘要 在2024年财报电话会议上，小米总裁卢伟冰透露了小米汽车的发展愿景。2024年关键词为“增长”，2025年则为“智能化”，计划在智能驾驶领域实现“一年追三代”，并加大AI研发投入，目标在2025年进入自动驾驶第一梯队。小米智驾系统已实现高速NOA、端到端泊车等功能，2024年将进一步优化全场景智驾。此外，小米计划年底推出SUV车型YU7，定位更亲民，并加速全球化布局，计划2027年启动海外销售，挑战特斯拉Model Y的销量地位。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:04:11Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/20242025/","summary":"在2024年财报电话会议上，小米总裁卢伟冰透露了小米汽车的发展愿景。2024年关键词为“增长”，2025年则为“智能化”，计划在智能驾驶领域实现“一年追三代”，并加大AI研发投入，目标在2025年进入自动驾驶第一梯队。小米智驾系统已实现高速NOA、端到端泊车等功能，2024年将进一步优化全场景智驾。此外，小米计划年底推出SUV车型YU7，定位更亲民，并加速全球化布局，计划2027年启动海外销售，挑战特斯拉Model Y的销量地位。","title":"#小米汽车透露今年愿景#在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。卢伟冰..."},{"content":"#小米史上最强年报#去年是小米集团成立以来，业绩最辉煌的一年。据小米年报显示，小米集团全年营收高达3659亿元，同比增长35%，第四季度更是首次突破千亿大关，\u0026hellip; 原始链接: 查看原文\n原始摘要 #小米史上最强年报#\n去年是小米集团成立以来，业绩最辉煌的一年。\n据小米年报显示，小米集团全年营收高达3659亿元，同比增长35%，第四季度更是首次突破千亿大关，达到1090亿元。\n净利润方面，小米全年调整后净利润272亿元，同比增长41.3%，其中第四季度利润同比增长近70%，远超市场预期。\n值得注意的是，小米的大手笔研发投入也是这份年报中的亮点之一：全年研发支出240.5亿元，同比增长26%，其中AI和智能驾驶成为投入重点。\n其中，智能手机业务依旧是小米的核心支柱，全年收入1918亿元，同比增长21.8%，高端化战略初见成效，手机ASP（平均售价）同比提升至1138.2元。\n此外，IoT与生活消费产品收入1041亿元，同比增长30%，互联网服务收入341亿元，同比增长13.3%。\n在多条业务线齐头并进的情况下，小米成功实现了利润率的提升，全年毛利率达到20.9%，其中汽车业务的毛利率在第四季度突破20%，这也为未来实现造车盈利奠定了基础。\n可以说，这份财报不仅展示了小米当前的强势增长，也表明其多元化布局正在进入收获期。\nAI 摘要 小米集团2022年业绩创历史新高，全年营收达3659亿元，同比增长35%，净利润272亿元，增长41.3%。智能手机业务收入1918亿元，高端化战略见效，平均售价提升至1138.2元。IoT与生活消费产品收入1041亿元，互联网服务收入341亿元。研发投入240.5亿元，重点在AI和智能驾驶。汽车业务毛利率突破20%，为未来盈利奠定基础。整体毛利率达20.9%，多元化布局进入收获期。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:04:02Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/365935/","summary":"小米集团2022年业绩创历史新高，全年营收达3659亿元，同比增长35%，净利润272亿元，增长41.3%。智能手机业务收入1918亿元，高端化战略见效，平均售价提升至1138.2元。IoT与生活消费产品收入1041亿元，互联网服务收入341亿元。研发投入240.5亿元，重点在AI和智能驾驶。汽车业务毛利率突破20%，为未来盈利奠定基础。整体毛利率达20.9%，多元化布局进入收获期。","title":"#小米史上最强年报#去年是小米集团成立以来，业绩最辉煌的一年。据小米年报显示，小米集团全年营收高达3659亿元，同比增长35%，第四季度更是首次突破千亿大关，..."},{"content":"#机器人会侧空翻了##全球首次人形机器人原地侧空翻#宇树科技的G1人形机器人，刚刚完成了全球首次原地侧空翻动作。官方发布的【视频】显示，机器人不仅顺利完成侧\u0026hellip; 原始链接: 查看原文\n原始摘要 #机器人会侧空翻了##全球首次人形机器人原地侧空翻#\n宇树科技的G1人形机器人，刚刚完成了全球首次原地侧空翻动作。\n官方发布的【视频】显示，机器人不仅顺利完成侧空翻，而且翻滚流畅，机身协调，丝毫没有多余的动作。\n宇树科技表示，该动作是团队抽空时顺便挑战的，而且开发程序和拍摄过程中，G1没有出现任何故障。\n去年，宇树科技曾用身高1.8米的H1机器人，完成全球首个电机驱动人形机器人后空翻动作。 而这次的G1侧空翻更进一步，体现了团队独创的动力学算法和机械设计实力。\n不过，评论区有网友表示，作为消费者，相比于侧空翻、回旋踢这类华丽的动作，他们更想看做家务这类实用性动作。 量子位的微博视频视频无法显示，请前往微博视频观看。\nAI 摘要 宇树科技的G1人形机器人成功完成了全球首次原地侧空翻动作，展示了其流畅的翻滚和协调的机身控制。这一成就体现了团队在动力学算法和机械设计方面的实力。去年，该公司还曾用H1机器人完成全球首个电机驱动人形机器人后空翻。尽管这一技术突破令人印象深刻，但部分网友更期待机器人能执行如做家务等实用性任务。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:03:29Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/g1/","summary":"宇树科技的G1人形机器人成功完成了全球首次原地侧空翻动作，展示了其流畅的翻滚和协调的机身控制。这一成就体现了团队在动力学算法和机械设计方面的实力。去年，该公司还曾用H1机器人完成全球首个电机驱动人形机器人后空翻。尽管这一技术突破令人印象深刻，但部分网友更期待机器人能执行如做家务等实用性任务。","title":"#机器人会侧空翻了##全球首次人形机器人原地侧空翻#宇树科技的G1人形机器人，刚刚完成了全球首次原地侧空翻动作。官方发布的【视频】显示，机器人不仅顺利完成侧..."},{"content":"#老黄带迪士尼机器人炸场GTC# 老黄带着迪士尼机器人炸场GTC，现场发布机器人物理引擎Newton、基础模型GROOT N1，他还预言：机器人将会是未来最大的行业。 量子位\u0026hellip; 原始链接: 查看原文\n原始摘要 #老黄带迪士尼机器人炸场GTC# 老黄带着迪士尼机器人炸场GTC，现场发布机器人物理引擎Newton、基础模型GROOT N1，他还预言：机器人将会是未来最大的行业。 量子位的微博视频 视频无法显示，请前往微博视频观看。\nAI 摘要 在GTC大会上，老黄展示了迪士尼机器人，并发布了机器人物理引擎Newton和基础模型GROOT N1。他预测机器人将成为未来最大的行业。这一展示突显了机器人在未来科技发展中的重要性，并展示了相关技术的创新进展。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:03:06Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/gtc-gtcnewtongroot-n1-/","summary":"在GTC大会上，老黄展示了迪士尼机器人，并发布了机器人物理引擎Newton和基础模型GROOT N1。他预测机器人将成为未来最大的行业。这一展示突显了机器人在未来科技发展中的重要性，并展示了相关技术的创新进展。","title":"#老黄带迪士尼机器人炸场GTC# 老黄带着迪士尼机器人炸场GTC，现场发布机器人物理引擎Newton、基础模型GROOT N1，他还预言：机器人将会是未来最大的行业。 量子位..."},{"content":"#苹果限制第三方智能手表##智能手表鼻祖Pebble回归#Pebble，这款智能手表的鼻祖，凭借超长续航和极简设计，被无数科技极客奉为神器。尽管Pebble品牌早已停止运营\u0026hellip; 原始链接: 查看原文\n原始摘要 #苹果限制第三方智能手表##智能手表鼻祖Pebble回归#\nPebble，这款智能手表的鼻祖，凭借超长续航和极简设计，被无数科技极客奉为神器。\n尽管Pebble品牌早已停止运营，但一群忠实的开发者重燃了它的生命，推出了两款全新的PebbleOS手表：Core 2 Duo和Core Time 2。\n先来看Core 2 Duo，它不仅延续了Pebble的经典设计，还带来了现代化的硬件升级：\n- 1.26英寸黑白电子墨水屏，显示清晰\n- 30天超长续航，比旧版Pebble 2的7天大幅提升\n- 轻量化聚碳酸酯机身，提供黑白两种配色\n- 物理按键操控，避免触控屏误触问题\n- 内置扬声器和线性马达，震动反馈更强，声音更安静\n- 售价$149（约人民币1078元），预计7月发货\n接下来是Core Time 2，它是一款更先进的Pebble智能手表：\n- 1.5英寸64色电子墨水屏，显示面积比Pebble Time 2大53%\n- 金属机身+触控屏，外观更精致\n- 预计30天续航，无需频繁充电\n- 额外支持心率监测，增加健康功能\n- 标准22mm表带，可自由更换\n- 售价$225（约人民币1627元），预计12月发货\n此外，这两款手表还开放了源代码，允许开发者自由修改和拓展功能。\n然而，PebbleOS手表在iPhone上的使用却受到了诸多限制，包括：\n- 无法代替iPhone发送短信或iMessage\n- 无法“标记为已读”“删除”或“快速回复”消息\n- PebbleOS应用一旦被手动关闭或iOS自动清理，手表将无法连接手机或访问互联网\n- 无法检测用户是否在使用iPhone，哪怕你正在看手机，通知仍会震动提醒\n- 所有应用必须经过App Store审核，每次更新都有被拒的风险\n- 开发者无法通过iOS应用商店收费\n这些限制使得新款PebbleOS手表在Android上的体验远超iOS。\n苹果对外宣称这些限制是出于“安全、隐私和用户体验”考虑，但美国司法部对此展开了反垄断调查，指控苹果通过技术壁垒打压竞争对手。\nPebble的精神虽然延续了下来，但面对苹果的封闭生态，新款PebbleOS手表的未来仍充满挑战。\n拓展阅读：网页链接\n视频无法显示，请前往微博视频观看。\nAI 摘要 Pebble智能手表鼻祖回归，推出Core 2 Duo和Core Time 2两款新品，延续经典设计并升级硬件。Core 2 Duo配备1.26英寸黑白电子墨水屏，续航30天，售价$149；Core Time 2配备1.5英寸64色电子墨水屏，支持心率监测，售价$225。两款手表开放源代码，允许开发者自由修改。然而，苹果对PebbleOS手表在iPhone上的使用施加了诸多限制，如无法发送短信、快速回复消息等，导致其在Android上的体验优于iOS。苹果此举引发美国司法部反垄断调查。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:03:39Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/pebblepebblepebble/","summary":"Pebble智能手表鼻祖回归，推出Core 2 Duo和Core Time 2两款新品，延续经典设计并升级硬件。Core 2 Duo配备1.26英寸黑白电子墨水屏，续航30天，售价$149；Core Time 2配备1.5英寸64色电子墨水屏，支持心率监测，售价$225。两款手表开放源代码，允许开发者自由修改。然而，苹果对PebbleOS手表在iPhone上的使用施加了诸多限制，如无法发送短信、快速回复消息等，导致其在Android上的体验优于iOS。苹果此举引发美国司法部反垄断调查。","title":"#苹果限制第三方智能手表##智能手表鼻祖Pebble回归#Pebble，这款智能手表的鼻祖，凭借超长续航和极简设计，被无数科技极客奉为神器。尽管Pebble品牌早已停止运营..."},{"content":"#小米成中国市值第一车企#小米SU7的热销不仅助推公司营收增长，更是带动了股价暴涨。短短一年，小米市值翻了近四倍，突破1.36万亿元，超越比亚迪，成了中国市值\u0026hellip; 原始链接: 查看原文\n原始摘要 #小米成中国市值第一车企#\n小米SU7的热销不仅助推公司营收增长，更是带动了股价暴涨。\n短短一年，小米市值翻了近四倍，突破1.36万亿元，超越比亚迪，成了中国市值最高的汽车制造商。\n相比之下，比亚迪目前市值万亿元左右，而蔚来、小鹏等新势力车企则仍在千亿级别。\n这背后，离不开小米的“手机+AI+汽车”一体化的生态战略，以及其品牌在供应链、技术积累、用户流量上的优势。\n更重要的是，小米SU7自上市以来销量节节攀升，不仅3个月交付破万，7个月单月销量突破2万，还打破新势力最快十万台下线纪录。\n目前，小米已将年交付目标提升至35万辆，并计划推出第二款SUV车型YU7，意图进一步扩大市场份额。\n可以说，小米造车的第一年，交出了一份超出市场预期的成绩单，而市值的飙升正是资本市场对其前景的看好。\nAI 摘要 小米凭借其SU7车型的热销，市值在一年内翻了近四倍，达到1.36万亿元，超越比亚迪成为中国市值最高的汽车制造商。这一成就得益于小米的“手机+AI+汽车”一体化生态战略及其在供应链、技术积累和用户流量上的优势。小米SU7自上市以来销量持续攀升，7个月内单月销量突破2万，并创下新势力最快十万台下线纪录。小米计划将年交付目标提升至35万辆，并推出第二款SUV车型YU7，以进一步扩大市场份额。市值的飙升反映了资本市场对小米前景的看好。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:03:50Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/su7136/","summary":"小米凭借其SU7车型的热销，市值在一年内翻了近四倍，达到1.36万亿元，超越比亚迪成为中国市值最高的汽车制造商。这一成就得益于小米的“手机+AI+汽车”一体化生态战略及其在供应链、技术积累和用户流量上的优势。小米SU7自上市以来销量持续攀升，7个月内单月销量突破2万，并创下新势力最快十万台下线纪录。小米计划将年交付目标提升至35万辆，并推出第二款SUV车型YU7，以进一步扩大市场份额。市值的飙升反映了资本市场对小米前景的看好。","title":"#小米成中国市值第一车企#小米SU7的热销不仅助推公司营收增长，更是带动了股价暴涨。短短一年，小米市值翻了近四倍，突破1.36万亿元，超越比亚迪，成了中国市值..."},{"content":"搜索竞技场上线 - AI联网搜索竞技场 原始链接: 查看原文\nAI 摘要 Chatbot Arena推出了\u0026quot;搜索竞技场\u0026quot;，这是一个盲测AI搜索能力的平台。用户可以通过投票比较不同AI模型的搜索效果。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:03:15Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/aiaichatbot-arenasearch-arenaai/","summary":"Chatbot Arena推出了搜索竞技场，这是一个盲测AI搜索能力的平台。用户可以通过投票比较不同AI模型的搜索效果。","title":"搜索竞技场上线 - AI联网搜索竞技场"},{"content":"Cosmos World Foundation Model Platform for Physical AI 原始链接: 查看原文\n原始摘要 Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make Cosmos open-source and our models open-weight with permissive licenses available via https://github.com/nvidia-cosmos/cosmos-predict1.\nAI 摘要 本文介绍了Cosmos世界基础模型平台，旨在帮助开发者为其物理AI系统构建定制化的世界模型。该平台包括视频处理管道、预训练的世界基础模型、模型微调示例和视频标记器。世界基础模型作为通用模型，可微调为下游应用的定制模型。为促进物理AI解决社会关键问题，Cosmos平台开源并提供开放权重模型，许可宽松，可通过https://github.com/nvidia-cosmos/cosmos-predict1获取。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:02:31Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/cosmos-world-foundation-model-platform-for-physica/","summary":"本文介绍了Cosmos世界基础模型平台，旨在帮助开发者为其物理AI系统构建定制化的世界模型。该平台包括视频处理管道、预训练的世界基础模型、模型微调示例和视频标记器。世界基础模型作为通用模型，可微调为下游应用的定制模型。为促进物理AI解决社会关键问题，Cosmos平台开源并提供开放权重模型，许可宽松，可通过https://github.com/nvidia-cosmos/cosmos-predict1获取。","title":"Cosmos World Foundation Model Platform for Physical AI"},{"content":"EnvBench: A Benchmark for Automated Environment Setup 原始链接: 查看原文\n原始摘要 Recent advances in Large Language Models (LLMs) have enabled researchers to focus on practical repository-level tasks in software engineering domain. In this work, we consider a cornerstone task for automating work with software repositories-environment setup, i.e., a task of configuring a repository-specific development environment on a system. Existing studies on environment setup introduce innovative agentic strategies, but their evaluation is often based on small datasets that may not capture the full range of configuration challenges encountered in practice. To address this gap, we introduce a comprehensive environment setup benchmark EnvBench. It encompasses 329 Python and 665 JVM-based (Java, Kotlin) repositories, with a focus on repositories that present genuine configuration challenges, excluding projects that can be fully configured by simple deterministic scripts. To enable further benchmark extension and usage for model tuning, we implement two automatic metrics: a static analysis check for missing imports in Python and a compilation check for JVM languages. We demonstrate the applicability of our benchmark by evaluating three environment setup approaches, including a simple zero-shot baseline and two agentic workflows, that we test with two powerful LLM backbones, GPT-4o and GPT-4o-mini. The best approach manages to successfully configure 6.69% repositories for Python and 29.47% repositories for JVM, suggesting that EnvBench remains challenging for current approaches. Our benchmark suite is publicly available at https://github.com/JetBrains-Research/EnvBench. The dataset and experiment trajectories are available at https://jb.gg/envbench.\nAI 摘要 近期大型语言模型（LLMs）的进展促使研究者关注软件工程领域的实际仓库级任务。本文聚焦于自动化软件仓库环境设置任务，提出了一个全面的环境设置基准EnvBench，涵盖329个Python和665个JVM（Java、Kotlin）仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括简单的零样本基线和两种代理工作流，使用GPT-4o和GPT-4o-mini进行测试，结果表明EnvBench对当前方法仍具挑战性。基准套件公开于https://github.com/JetBrains-Research/EnvBench。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:02:03Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/envbench-a-benchmark-for-automated-environment-set/","summary":"近期大型语言模型（LLMs）的进展促使研究者关注软件工程领域的实际仓库级任务。本文聚焦于自动化软件仓库环境设置任务，提出了一个全面的环境设置基准EnvBench，涵盖329个Python和665个JVM（Java、Kotlin）仓库，重点关注具有真实配置挑战的项目。我们实现了两种自动评估指标：Python的静态分析检查和JVM语言的编译检查。通过评估三种环境设置方法，包括简单的零样本基线和两种代理工作流，使用GPT-4o和GPT-4o-mini进行测试，结果表明EnvBench对当前方法仍具挑战性。基准套件公开于https://github.com/JetBrains-Research/EnvBench。","title":"EnvBench: A Benchmark for Automated Environment Setup"},{"content":"Information Fusion in Smart Agriculture: Machine Learning Applications and Future Research Directions 原始链接: 查看原文\n原始摘要 Machine learning (ML) is a rapidly evolving technology with expanding applications across various fields. This paper presents a comprehensive survey of recent ML applications in agriculture for sustainability and efficiency. Existing reviews mainly focus on narrow subdomains or lack a fusion-driven perspectives. This study provides a combined analysis of ML applications in agriculture, structured around five key objectives: (i) Analyzing ML techniques across pre-harvesting, harvesting, and post-harvesting phases. (ii) Demonstrating how ML can be used with agricultural data and data fusion. (iii) Conducting a bibliometric and statistical analysis to reveal research trends and activity. (iv) Investigating real-world case studies of leading artificial intelligence (AI)-driven agricultural companies that use different types of multisensors and multisource data. (v) Compiling publicly available datasets to support ML model training. Going beyond existing previous reviews, this review focuses on how machine learning (ML) techniques, combined with multi-source data fusion (integrating remote sensing, IoT, and climate analytics), enhance precision agriculture by improving predictive accuracy and decision-making. Case studies and statistical insights illustrate the evolving landscape of AI driven smart farming, while future research directions also discusses challenges associated with data fusion for heterogeneous datasets. This review bridges the gap between AI research and agricultural applications, offering a roadmap for researchers, industry professionals, and policymakers to harness information fusion and ML for advancing precision agriculture.\nAI 摘要 本文综述了机器学习（ML）在农业可持续发展与效率提升中的广泛应用，填补了现有研究在跨领域融合视角上的不足。研究围绕五大目标展开：分析ML在农业各阶段的应用、探讨ML与农业数据及数据融合的结合、进行文献计量与统计分析、研究AI驱动的农业公司案例、并整理公开数据集以支持ML模型训练。重点探讨了ML与多源数据融合（如遥感、物联网和气候分析）如何提升精准农业的预测精度与决策能力，同时指出了未来研究方向及异质数据集融合的挑战，为研究者、行业专家及政策制定者提供了参考。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:02:42Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/information-fusion-in-smart-agriculture-machine-le/","summary":"本文综述了机器学习（ML）在农业可持续发展与效率提升中的广泛应用，填补了现有研究在跨领域融合视角上的不足。研究围绕五大目标展开：分析ML在农业各阶段的应用、探讨ML与农业数据及数据融合的结合、进行文献计量与统计分析、研究AI驱动的农业公司案例、并整理公开数据集以支持ML模型训练。重点探讨了ML与多源数据融合（如遥感、物联网和气候分析）如何提升精准农业的预测精度与决策能力，同时指出了未来研究方向及异质数据集融合的挑战，为研究者、行业专家及政策制定者提供了参考。","title":"Information Fusion in Smart Agriculture: Machine Learning Applications and Future Research Directions"},{"content":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions 原始链接: 查看原文\n原始摘要 This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.\nAI 摘要 本研究探讨了大型语言模型（LLMs）在人工智能领域中的伦理问题，包括与其他AI系统共有的隐私和公平性挑战，以及LLMs特有的问题，如幻觉、可验证的责任性和解码审查复杂性。研究强调了解决这些复杂性的必要性，以确保责任性、减少偏见并增强LLMs在信息传播中的透明度。提出了缓解策略和未来方向，倡导跨学科合作，推荐针对特定领域的伦理框架和适应多样环境的动态审计系统，旨在指导LLMs的负责任开发和整合，展望伦理考量主导AI社会进步的未来。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:02:51Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/navigating-llm-ethics-advancements-challenges-and-/","summary":"本研究探讨了大型语言模型（LLMs）在人工智能领域中的伦理问题，包括与其他AI系统共有的隐私和公平性挑战，以及LLMs特有的问题，如幻觉、可验证的责任性和解码审查复杂性。研究强调了解决这些复杂性的必要性，以确保责任性、减少偏见并增强LLMs在信息传播中的透明度。提出了缓解策略和未来方向，倡导跨学科合作，推荐针对特定领域的伦理框架和适应多样环境的动态审计系统，旨在指导LLMs的负责任开发和整合，展望伦理考量主导AI社会进步的未来。","title":"Navigating LLM Ethics: Advancements, Challenges, and Future Directions"},{"content":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play 原始链接: 查看原文\n原始摘要 Large language models (LLMs) are increasingly integrated with specialized external tools, yet many tasks demand zero-shot tool usage with minimal or noisy documentation. Existing solutions rely on manual rewriting or labeled data for validation, making them inapplicable in true zero-shot settings. To address these challenges, we propose PLAY2PROMPT, an automated framework that systematically \u0026ldquo;plays\u0026rdquo; with each tool to explore its input-output behaviors. Through this iterative trial-and-error process, PLAY2PROMPT refines tool documentation and generates usage examples without any labeled data. These examples not only guide LLM inference but also serve as validation to further enhance tool utilization. Extensive experiments on real-world tasks demonstrate that PLAY2PROMPT significantly improves zero-shot tool performance across both open and closed models, offering a scalable and effective solution for domain-specific tool integration.\nAI 摘要 大型语言模型（LLMs）越来越多地与专用外部工具集成，但许多任务需要在零样本设置下使用工具，且文档可能不完整或存在噪声。现有方法依赖手动重写或标注数据进行验证，难以在真正的零样本环境中应用。为此，研究者提出了PLAY2PROMPT框架，通过自动化“试错”过程探索工具的输入输出行为，生成工具文档和使用示例，无需标注数据。这些示例不仅指导LLM推理，还可用于验证工具使用效果。实验表明，PLAY2PROMPT显著提升了零样本工具性能，为领域专用工具集成提供了可扩展的解决方案。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:02:13Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/play2prompt-zero-shot-tool-instruction-optimizatio/","summary":"大型语言模型（LLMs）越来越多地与专用外部工具集成，但许多任务需要在零样本设置下使用工具，且文档可能不完整或存在噪声。现有方法依赖手动重写或标注数据进行验证，难以在真正的零样本环境中应用。为此，研究者提出了PLAY2PROMPT框架，通过自动化“试错”过程探索工具的输入输出行为，生成工具文档和使用示例，无需标注数据。这些示例不仅指导LLM推理，还可用于验证工具使用效果。实验表明，PLAY2PROMPT显著提升了零样本工具性能，为领域专用工具集成提供了可扩展的解决方案。","title":"PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play"},{"content":"VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms 原始链接: 查看原文\n原始摘要 Escape rooms present a unique cognitive challenge that demands exploration-driven planning: players should actively search their environment, continuously update their knowledge based on new discoveries, and connect disparate clues to determine which elements are relevant to their objectives. Motivated by this, we introduce VisEscape, a benchmark of 20 virtual escape rooms specifically designed to evaluate AI models under these challenging conditions, where success depends not only on solving isolated puzzles but also on iteratively constructing and refining spatial-temporal knowledge of a dynamically changing environment. On VisEscape, we observed that even state-of-the-art multimodal models generally fail to escape the rooms, showing considerable variation in their levels of progress and trajectories. To address this issue, we propose VisEscaper, which effectively integrates Memory, Feedback, and ReAct modules, demonstrating significant improvements by performing 3.7 times more effectively and 5.0 times more efficiently on average.\nAI 摘要 VisEscape是一个包含20个虚拟密室逃脱的基准测试，旨在评估AI模型在探索驱动规划中的表现。该测试要求模型不仅解决孤立谜题，还需动态构建和优化时空知识。研究发现，即使最先进的多模态模型也难以成功逃脱，表现差异显著。为此，提出了VisEscaper模型，通过整合记忆、反馈和ReAct模块，显著提升了性能，平均效率提高了5倍，效果提升了3.7倍。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:02:22Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/visescape-a-benchmark-for-evaluating-exploration-d/","summary":"VisEscape是一个包含20个虚拟密室逃脱的基准测试，旨在评估AI模型在探索驱动规划中的表现。该测试要求模型不仅解决孤立谜题，还需动态构建和优化时空知识。研究发现，即使最先进的多模态模型也难以成功逃脱，表现差异显著。为此，提出了VisEscaper模型，通过整合记忆、反馈和ReAct模块，显著提升了性能，平均效率提高了5倍，效果提升了3.7倍。","title":"VisEscape: A Benchmark for Evaluating Exploration-driven Decision-making in Virtual Escape Rooms"},{"content":"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control 原始链接: 查看原文\n原始摘要 We introduce Cosmos-Transfer, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge. In the design, the spatial conditional scheme is adaptive and customizable. It allows weighting different conditional inputs differently at different spatial locations. This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real. We conduct extensive evaluations to analyze the proposed model and demonstrate its applications for Physical AI, including robotics Sim2Real and autonomous vehicle data enrichment. We further demonstrate an inference scaling strategy to achieve real-time world generation with an NVIDIA GB200 NVL72 rack. To help accelerate research development in the field, we open-source our models and code at https://github.com/nvidia-cosmos/cosmos-transfer1.\nAI 摘要 Cosmos-Transfer 是一种条件世界生成模型，能够基于多种空间控制输入（如分割、深度和边缘）生成世界模拟。其设计具有自适应和可定制的空间条件方案，允许在不同空间位置对不同条件输入进行加权，从而实现高度可控的世界生成。该模型在多种世界到世界转换应用（如 Sim2Real）中具有广泛用途，特别是在物理 AI、机器人 Sim2Real 和自动驾驶数据增强方面。通过 NVIDIA GB200 NVL72 机架，模型实现了实时世界生成。为加速研究发展，相关模型和代码已在 GitHub 开源。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:01:26Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/cosmos-transfer1-conditional-world-generation-with/","summary":"Cosmos-Transfer 是一种条件世界生成模型，能够基于多种空间控制输入（如分割、深度和边缘）生成世界模拟。其设计具有自适应和可定制的空间条件方案，允许在不同空间位置对不同条件输入进行加权，从而实现高度可控的世界生成。该模型在多种世界到世界转换应用（如 Sim2Real）中具有广泛用途，特别是在物理 AI、机器人 Sim2Real 和自动驾驶数据增强方面。通过 NVIDIA GB200 NVL72 机架，模型实现了实时世界生成。为加速研究发展，相关模型和代码已在 GitHub 开源。","title":"Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control"},{"content":"Don\u0026rsquo;t lie to your friends: Learning what you know from collaborative self-play 原始链接: 查看原文\n原始摘要 To be helpful assistants, AI agents must be aware of their own capabilities and limitations. This includes knowing when to answer from parametric knowledge versus using tools, when to trust tool outputs, and when to abstain or hedge. Such capabilities are hard to teach through supervised fine-tuning because they require constructing examples that reflect the agent\u0026rsquo;s specific capabilities. We therefore propose a radically new approach to teaching agents what they know: \\emph{collaborative self-play}. We construct multi-agent collaborations in which the group is rewarded for collectively arriving at correct answers. The desired meta-knowledge emerges from the incentives built into the structure of the interaction. We focus on small societies of agents that have access to heterogeneous tools (corpus-specific retrieval), and therefore must collaborate to maximize their success while minimizing their effort. Experiments show that group-level rewards for multi-agent communities can induce policies that \\emph{transfer} to improve tool use and selective prediction in settings where individual agents are deployed in isolation.\nAI 摘要 本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出必要的元知识。这种方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单独部署时也能改善工具使用和选择性预测的策略。这种方法避免了监督微调中难以构建反映代理特定能力的示例的问题。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:01:52Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/dont-lie-to-your-friends-learning-what-you-know-fr/","summary":"本文提出了一种名为“协作自玩”的新方法，用于教导AI代理了解自身能力和限制。通过构建多代理协作环境，团队因共同得出正确答案而获得奖励，从而促使代理发展出必要的元知识。这种方法特别适用于拥有不同工具（如特定语料库检索）的小型代理群体，它们必须协作以最大化成功并最小化努力。实验表明，多代理群体的集体奖励能够诱导出在单独部署时也能改善工具使用和选择性预测的策略。这种方法避免了监督微调中难以构建反映代理特定能力的示例的问题。","title":"Don't lie to your friends: Learning what you know from collaborative self-play"},{"content":"Gricean Norms as a Basis for Effective Collaboration 原始链接: 查看原文\n原始摘要 Effective human-AI collaboration hinges not only on the AI agent\u0026rsquo;s ability to follow explicit instructions but also on its capacity to navigate ambiguity, incompleteness, invalidity, and irrelevance in communication. Gricean conversational and inference norms facilitate collaboration by aligning unclear instructions with cooperative principles. We propose a normative framework that integrates Gricean norms and cognitive frameworks \u0026ndash; common ground, relevance theory, and theory of mind \u0026ndash; into large language model (LLM) based agents. The normative framework adopts the Gricean maxims of quantity, quality, relation, and manner, along with inference, as Gricean norms to interpret unclear instructions, which are: ambiguous, incomplete, invalid, or irrelevant. Within this framework, we introduce Lamoids, GPT-4 powered agents designed to collaborate with humans. To assess the influence of Gricean norms in human-AI collaboration, we evaluate two versions of a Lamoid: one with norms and one without. In our experiments, a Lamoid collaborates with a human to achieve shared goals in a grid world (Doors, Keys, and Gems) by interpreting both clear and unclear natural language instructions. Our results reveal that the Lamoid with Gricean norms achieves higher task accuracy and generates clearer, more accurate, and contextually relevant responses than the Lamoid without norms. This improvement stems from the normative framework, which enhances the agent\u0026rsquo;s pragmatic reasoning, fostering effective human-AI collaboration and enabling context-aware communication in LLM-based agents.\nAI 摘要 本文提出了一种整合Gricean准则和认知框架的规范框架，旨在提升基于大型语言模型（LLM）的AI代理在模糊、不完整、无效或不相关指令下的协作能力。通过引入Lamoids（基于GPT-4的代理），实验验证了在网格世界任务中，遵循Gricean准则的Lamoid在任务准确性和生成清晰、准确、上下文相关响应方面优于未遵循准则的版本。结果表明，该框架增强了代理的语用推理能力，促进了人机协作的有效性，并实现了基于LLM的代理的上下文感知通信。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:01:41Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/gricean-norms-as-a-basis-for-effective-collaborati/","summary":"本文提出了一种整合Gricean准则和认知框架的规范框架，旨在提升基于大型语言模型（LLM）的AI代理在模糊、不完整、无效或不相关指令下的协作能力。通过引入Lamoids（基于GPT-4的代理），实验验证了在网格世界任务中，遵循Gricean准则的Lamoid在任务准确性和生成清晰、准确、上下文相关响应方面优于未遵循准则的版本。结果表明，该框架增强了代理的语用推理能力，促进了人机协作的有效性，并实现了基于LLM的代理的上下文感知通信。","title":"Gricean Norms as a Basis for Effective Collaboration"},{"content":"Measuring AI Ability to Complete Long Tasks 原始链接: 查看原文\n原始摘要 Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models\u0026rsquo; time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results \u0026ndash; including their degree of external validity \u0026ndash; and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.\nAI 摘要 尽管AI基准测试进展迅速，但其实际意义仍不明确。为量化AI系统在人类能力方面的表现，研究者提出新指标“50%任务完成时间跨度”，即人类完成AI模型50%成功率任务所需时间。研究表明，当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间跨度约为50分钟，且自2019年以来每7个月翻倍，2024年可能加速。AI时间跨度的提升主要源于更高的可靠性、错误适应能力、逻辑推理和工具使用能力。若趋势持续，5年内AI或能自动化目前人类需一个月完成的软件任务。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T23:01:16Z 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/measuring-ai-ability-to-complete-long-tasks/","summary":"尽管AI基准测试进展迅速，但其实际意义仍不明确。为量化AI系统在人类能力方面的表现，研究者提出新指标“50%任务完成时间跨度”，即人类完成AI模型50%成功率任务所需时间。研究表明，当前前沿AI模型（如Claude 3.7 Sonnet）的50%时间跨度约为50分钟，且自2019年以来每7个月翻倍，2024年可能加速。AI时间跨度的提升主要源于更高的可靠性、错误适应能力、逻辑推理和工具使用能力。若趋势持续，5年内AI或能自动化目前人类需一个月完成的软件任务。","title":"Measuring AI Ability to Complete Long Tasks"},{"content":"#马斯克收购了一家视频生成初创公司# 马斯克也要打造自己的视频生成模型了？？就在最近，xAI收购了一家视频生成初创公司，这家仅4个人的公司过去两年打造出了Hot\u0026hellip; 原始链接: 查看原文\n原始摘要 #马斯克收购了一家视频生成初创公司# 马斯克也要打造自己的视频生成模型了？？\n就在最近，xAI收购了一家视频生成初创公司，这家仅4个人的公司过去两年打造出了Hotshot这款产品。\n据公告介绍，Hotshot至今已有3款视频生成基础模型。被收购之后，目前已停止推出新的视频创作功能，而且用户过往创作的视频截止下载时间为3月30日。\n一看这架势，网友们纷纷想起了老马在今年1月的一场直播活动中掷下的豪言：\n预计将在几个月内发布Grok视频模型\n而且就在Hotshot联创\u0026amp;CEO公布上述消息之后，老马也第一时间跑来卖关子：\n酷炫视频AI即将到来！\n那么，这是一家怎样的团队呢？为什么它能被马斯克“看上”？\n答案这就揭晓——\n概括而言，Hotshot之所以能入老马的眼，原因显然在于两方面：\n一是“小团队也有大能量”，据悉Hotshot团队一共只有4个人，但他们在13个月里连续训练出了3个视频生成模型，且获得了一定程度的用户关注；二是虽然成立不久，但投资者中不乏Reddit联合创始人Alexis Ohanian等大佬。\n从Hotshot官网公布的信息来看，这个4人小团队在两年时间里成功打造出了“Sora”模型。马斯克进军AI视频，收购视频生成初创公司，4人13个月打造类Sora模型\nAI 摘要 这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T09:16:53+08:00 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/-xai4hot/","summary":"这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。","title":"#马斯克收购了一家视频生成初创公司# 马斯克也要打造自己的视频生成模型了？？就在最近，xAI收购了一家视频生成初创公司，这家仅4个人的公司过去两年打造出了Hot..."},{"content":"#陶哲轩亲测点赞o3-mini# 陶哲轩亲测点赞o3-mini：它纠正了我一个数学错误，10分钟就能解决原本一小时才能完成的题目，事情究竟咋回事？ 量子位的微博视频 原始链接: 查看原文\n原始摘要 #陶哲轩亲测点赞o3-mini# 陶哲轩亲测点赞o3-mini：它纠正了我一个数学错误，10分钟就能解决原本一小时才能完成的题目，事情究竟咋回事？ 量子位的微博视频 视频无法显示，请前往微博视频观看。\nAI 摘要 这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T09:16:53+08:00 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/o3-mini-o3-mini10-/","summary":"这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。","title":"#陶哲轩亲测点赞o3-mini# 陶哲轩亲测点赞o3-mini：它纠正了我一个数学错误，10分钟就能解决原本一小时才能完成的题目，事情究竟咋回事？ 量子位的微博视频"},{"content":"Escaping Plato\u0026rsquo;s Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes 原始链接: 查看原文\n原始摘要 With the rise of neural networks, especially in high-stakes applications, these networks need two properties (i) robustness and (ii) interpretability to ensure their safety. Recent advances in classifiers with 3D volumetric object representations have demonstrated a greatly enhanced robustness in out-of-distribution data. However, these 3D-aware classifiers have not been studied from the perspective of interpretability. We introduce CAVE - Concept Aware Volumes for Explanations - a new direction that unifies interpretability and robustness in image classification. We design an inherently-interpretable and robust classifier by extending existing 3D-aware classifiers with concepts extracted from their volumetric representations for classification. In an array of quantitative metrics for interpretability, we compare against different concept-based approaches across the explainable AI literature and show that CAVE discovers well-grounded concepts that are used consistently across images, while achieving superior robustness.\nAI 摘要 这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T09:16:53+08:00 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/escaping-platos-cave-robust-conceptual-reasoning-t/","summary":"这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。","title":"Escaping Plato's Cave: Robust Conceptual Reasoning through Interpretable 3D Neural Object Volumes"},{"content":"VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning 原始链接: 查看原文\n原始摘要 Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning.\nAI 摘要 这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。\n元数据 来源: ArXiv 类型: 论文 保存时间: 2025-03-19T09:16:53+08:00 目录日期: 2025-03-19 ","permalink":"https://example.org/papers/2025-03-19/videomind-a-chain-of-lora-agent-for-long-video-rea/","summary":"这是一篇关于AI Agent领域的论文，主要探讨了最新进展和研究方向。论文提出了一种新的方法来提高Agent的性能，并通过实验证明了其有效性。这项研究对未来的AI Agent开发具有重要指导意义。","title":"VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning"}]