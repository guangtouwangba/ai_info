<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，... - AI研究资料库</title>
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <div class="header-content">
        <h1><a href="/">AI研究资料库</a></h1>
        <nav>
            
            <a href="/papers/">论文</a>
            
            <a href="/tags/">标签</a>
            
            <a href="/categories/">分类</a>
            
            <a href="/search/">搜索</a>
            
        </nav>
        
<div class="language-switcher">
    
    <a href="https://example.org/" class="active">
        中文
    </a>
    
    <a href="https://example.org/en/" class="">
        English
    </a>
    
</div>
 
    </div>
</header> 
    
    <main>
        

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://example.org/">首页</a>&nbsp;»&nbsp;<a href="https://example.org/papers/">Papers</a></div>
    <h1 class="post-title">
      #路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，...
    </h1>
    <div class="post-meta"><span title='2025-03-19 23:04:00 +0000 +0000'>三月 19, 2025</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;AI Research Repository&nbsp;|&nbsp;<a href="https://github.com/yourusername/yourrepository/tree/main/content/papers/2025-03-19/llmscaling-up-llmrouting-llm8500llm.md" rel="noopener noreferrer" target="_blank">建议修改</a>

</div>
  </header> 
  <div class="original-link">
    <a href="https://weibo.com/6105753431/Pjgszzzde" target="_blank" rel="noopener">查看原文</a>
  </div>

  <div class="post-content"><h1 id="路由llm最全面探索笔记本也能玩的大模型scaling-up研究-事关路由llmrouting-llm一项截至目前最全面的研究来了共计收集和整理了涉及8500个llm">#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，&hellip;<a hidden class="anchor" aria-hidden="true" href="#路由llm最全面探索笔记本也能玩的大模型scaling-up研究-事关路由llmrouting-llm一项截至目前最全面的研究来了共计收集和整理了涉及8500个llm">#</a></h1>
<p><strong>原始链接</strong>: <a href="https://weibo.com/6105753431/Pjgszzzde">查看原文</a></p>
<h2 id="原始摘要">原始摘要<a hidden class="anchor" aria-hidden="true" href="#原始摘要">#</a></h2>
<p><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%B7%AF%E7%94%B1LLM%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23&amp;extparam=%23%E8%B7%AF%E7%94%B1LLM%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23" data-hide=""><span class="surl-text">#路由LLM最全面探索#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BScaling+Up%E7%A0%94%E7%A9%B6%23&amp;extparam=%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BScaling+Up%E7%A0%94%E7%A9%B6%23" data-hide=""><span class="surl-text">#笔记本也能玩的大模型Scaling Up研究#</span></a> <br><br>事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——<br><br>共计收集和整理了涉及8500+个LLM，在12个Benchmark上的共2亿条性能记录！<br><br>先来简单科普一下路由LLM。<br><br>这种方法主要是把像ChatGPT、Qwen、DeepSeek这些成型的LLM当作 “专家” ，当给一个输入的时候，有分类能力的Router（路由器）就会把这个输入分配给合适的LLM处理。<br><br>如此一来，就能实现高性能、低计算消耗、低幻觉等目标。<br><br>而来自中山大学和普渡大学的研究人员在基于上述海量的记录做了一番探索之后，发现了一个现象，叫做Model-level Scaling Up。<br><br>一言蔽之，就是一个好的Router，可以让路由LLM范式的性能随着LLM候选数量的增加迅速变强。<br><br>随后，他们通过这些数据构建了针对Router设计的评测RouterEval。<br><br>值得注意的是，其他研究人员，也可以通过RouterEval在很少的计算资源下（如笔记本、单卡GPU上）就能参与到该路由LLM的研究当中。<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FOQFUcemTEmGC0eKUO_Fuiw" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1hzm99rc631j30u0091dit.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1hzm9aggvavj311w0juwlw.jpg" referrerpolicy="no-referrer"><br><br></p>
<h2 id="ai-摘要">AI 摘要<a hidden class="anchor" aria-hidden="true" href="#ai-摘要">#</a></h2>
<p>中山大学和普渡大学的研究团队进行了一项关于路由LLM（Routing LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-level Scaling Up”。此外，团队开发了RouterEval评测工具，使其他研究人员能在有限计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。</p>
<h2 id="元数据">元数据<a hidden class="anchor" aria-hidden="true" href="#元数据">#</a></h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-19T23:04:28Z</li>
<li><strong>目录日期</strong>: 2025-03-19</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://example.org/tags/llm%E4%B8%80%E9%A1%B9%E6%88%AA%E8%87%B3%E7%9B%AE%E5%89%8D%E6%9C%80%E5%85%A8%E9%9D%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%9D%A5%E4%BA%86%E5%85%B1%E8%AE%A1%E6%94%B6%E9%9B%86%E5%92%8C%E6%95%B4%E7%90%86%E4%BA%86%E6%B6%89%E5%8F%8A8500&#43;%E4%B8%AAllm/">LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，</a></li>
      <li><a href="https://example.org/tags/llm%E7%9A%84%E5%85%A8%E9%9D%A2%E7%A0%94%E7%A9%B6%E6%B6%89%E5%8F%8A8500%E5%A4%9A%E4%B8%AA%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8Bllm%E5%9C%A812%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E4%B8%8A%E7%9A%842%E4%BA%BF%E6%9D%A1%E6%80%A7%E8%83%BD%E8%AE%B0%E5%BD%95%E8%B7%AF%E7%94%B1llm%E9%80%9A%E8%BF%87%E5%B0%86%E8%BE%93%E5%85%A5%E5%88%86%E9%85%8D%E7%BB%99%E6%9C%80%E5%90%88%E9%80%82%E7%9A%84llm%E5%A4%84%E7%90%86%E6%97%A8%E5%9C%A8%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E4%BD%8E%E8%AE%A1%E7%AE%97%E6%B6%88%E8%80%97%E5%92%8C%E5%87%8F%E5%B0%91%E5%B9%BB%E8%A7%89%E7%A0%94%E7%A9%B6%E5%8F%91%E7%8E%B0%E9%9A%8F%E7%9D%80llm%E5%80%99%E9%80%89%E6%95%B0%E9%87%8F%E7%9A%84%E5%A2%9E%E5%8A%A0%E8%B7%AF%E7%94%B1llm%E7%9A%84%E6%80%A7%E8%83%BD%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%E8%BF%99%E4%B8%80%E7%8E%B0%E8%B1%A1%E8%A2%AB%E7%A7%B0%E4%B8%BAmodel-level/">LLM）的全面研究，涉及8500多个大型语言模型（LLM）在12个基准测试上的2亿条性能记录。路由LLM通过将输入分配给最合适的LLM处理，旨在实现高性能、低计算消耗和减少幻觉。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，这一现象被称为“Model-Level</a></li>
      <li><a href="https://example.org/tags/scaling/">Scaling</a></li>
      <li><a href="https://example.org/tags/up%E6%AD%A4%E5%A4%96%E5%9B%A2%E9%98%9F%E5%BC%80%E5%8F%91%E4%BA%86routereval%E8%AF%84%E6%B5%8B%E5%B7%A5%E5%85%B7%E4%BD%BF%E5%85%B6%E4%BB%96%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E8%83%BD%E5%9C%A8%E6%9C%89%E9%99%90%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E5%A6%82%E7%AC%94%E8%AE%B0%E6%9C%AC%E6%88%96%E5%8D%95%E5%8D%A1gpu%E4%B8%8A%E5%8F%82%E4%B8%8E%E8%B7%AF%E7%94%B1llm%E7%9A%84%E7%A0%94%E7%A9%B6/">Up”。此外，团队开发了RouterEval评测工具，使其他研究人员能在有限计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。</a></li>
      <li><a href="https://example.org/tags/%E8%B7%AF%E7%94%B1llm%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8Bscaling/">路由LLM最全面探索##笔记本也能玩的大模型Scaling</a></li>
    </ul>
    
    

    
    <div class="tagged-related">
      <h3>相关主题</h3>
      <ul>
        <li><a href="/papers/2025-03-20/llmscaling-up-llmrouting-llm8500llm/">#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，...</a></li>
        <li><a href="/papers/2025-03-20/infinity-scaling-bitwise-autoregressive-modeling-f/">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis网页链接本文介绍了一种名为Infinity的位运算视觉自回归模型，该模型...</a></li>
      </ul>
    </div>
<nav class="paginav">
  <a class="prev" href="https://example.org/papers/2025-03-19/33htmlpython/">
    <span class="title">« 上一页</span>
    <br>
    <span>#豆包编程能力升级##用豆包3分钟做个小游戏#豆包编程能力升级了，现在3分钟就能做出专属小游戏。升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整...</span>
  </a>
  <a class="next" href="https://example.org/papers/2025-03-19/20242025/">
    <span class="title">下一页 »</span>
    <br>
    <span>#小米汽车透露今年愿景#在财报电话会议上，小米总裁卢伟冰透露了小米汽车今年的新前景。2024年，小米汽车的关键词是“增长”，而2025年，则是“智能化”。卢伟冰...</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
    <footer>
        <p>&copy; 2025 AI研究资料库</p>
    </footer>
</body>
</html> 