<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks - AI研究资料库</title>
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <div class="header-content">
        <h1><a href="/">AI研究资料库</a></h1>
        <nav>
            
            <a href="/papers/">论文</a>
            
            <a href="/tags/">标签</a>
            
            <a href="/categories/">分类</a>
            
            <a href="/search/">搜索</a>
            
        </nav>
        
<div class="language-switcher">
    
    <a href="https://example.org/" class="active">
        中文
    </a>
    
    <a href="https://example.org/en/" class="">
        English
    </a>
    
</div>
 
    </div>
</header> 
    
    <main>
        

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://example.org/">首页</a>&nbsp;»&nbsp;<a href="https://example.org/papers/">Papers</a></div>
    <h1 class="post-title">
      SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks
    </h1>
    <div class="post-meta"><span title='2025-03-20 13:01:00 +0000 +0000'>三月 20, 2025</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;AI Research Repository&nbsp;|&nbsp;<a href="https://github.com/yourusername/yourrepository/tree/main/content/papers/2025-03-20/sweet-rl-training-multi-turn-llm-agents-on-collabo.md" rel="noopener noreferrer" target="_blank">建议修改</a>

</div>
  </header> 
  <div class="original-link">
    <a href="http://arxiv.org/abs/2503.15478v1" target="_blank" rel="noopener">查看原文</a>
  </div>

  <div class="post-content"><h1 id="sweet-rl-training-multi-turn-llm-agents-on-collaborative-reasoning-tasks">SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks<a hidden class="anchor" aria-hidden="true" href="#sweet-rl-training-multi-turn-llm-agents-on-collaborative-reasoning-tasks">#</a></h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.15478v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要<a hidden class="anchor" aria-hidden="true" href="#原始摘要">#</a></h2>
<p>Large language model (LLM) agents need to perform multi-turn interactions in
real-world tasks. However, existing multi-turn RL algorithms for optimizing LLM
agents fail to perform effective credit assignment over multiple turns while
leveraging the generalization capabilities of LLMs and it remains unclear how
to develop such algorithms. To study this, we first introduce a new benchmark,
ColBench, where an LLM agent interacts with a human collaborator over multiple
turns to solve realistic tasks in backend programming and frontend design.
Building on this benchmark, we propose a novel RL algorithm, SWEET-RL (RL with
Step-WisE Evaluation from Training-time information), that uses a carefully
designed optimization objective to train a critic model with access to
additional training-time information. The critic provides step-level rewards
for improving the policy model. Our experiments demonstrate that SWEET-RL
achieves a 6% absolute improvement in success and win rates on ColBench
compared to other state-of-the-art multi-turn RL algorithms, enabling
Llama-3.1-8B to match or exceed the performance of GPT4-o in realistic
collaborative content creation.</p>
<h2 id="ai-摘要">AI 摘要<a hidden class="anchor" aria-hidden="true" href="#ai-摘要">#</a></h2>
<p>大型语言模型（LLM）代理在现实任务中需进行多轮交互，但现有多轮强化学习（RL）算法难以有效分配多轮信用并利用LLM的泛化能力。为此，研究团队提出了新基准ColBench，模拟LLM代理与人类协作解决后端编程和前端设计任务。基于此，他们开发了SWEET-RL算法，通过训练时信息优化目标训练评论家模型，提供步骤级奖励以改进策略模型。实验表明，SWEET-RL在ColBench上成功率提升6%，使Llama-3.1-8B在协作内容创作中表现媲美或超越GPT4-o。</p>
<h2 id="元数据">元数据<a hidden class="anchor" aria-hidden="true" href="#元数据">#</a></h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T13:01:40+08:00</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://example.org/tags/training/">Training</a></li>
      <li><a href="https://example.org/tags/multi-turn/">Multi-Turn</a></li>
      <li><a href="https://example.org/tags/llm/">LLM</a></li>
      <li><a href="https://example.org/tags/agents/">Agents</a></li>
      <li><a href="https://example.org/tags/collaborative/">Collaborative</a></li>
    </ul>
    
    
    <div class="related-content">
      <h3>Related Content</h3>
      <ul>
        <li><a href="/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/">PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play</a></li>
      </ul>
    </div>

    
    <div class="tagged-related">
      <h3>相关主题</h3>
      <ul>
        <li><a href="/papers/2025-03-20/play2prompt-zero-shot-tool-instruction-optimizatio/">PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play</a></li>
        <li><a href="/papers/2025-03-20/a-role-of-environmental-complexity-on-representati/">A Role of Environmental Complexity on Representation Learning in Deep Reinforcement Learning Agents</a></li>
      </ul>
    </div>
<nav class="paginav">
  <a class="prev" href="https://example.org/papers/2025-03-20/more-information-is-not-always-better-connections-/">
    <span class="title">« 上一页</span>
    <br>
    <span>More Information is Not Always Better: Connections between Zero-Sum Local Nash Equilibria in Feedback and Open-Loop Information Patterns</span>
  </a>
  <a class="next" href="https://example.org/papers/2025-03-20/value-profiles-for-encoding-human-variation/">
    <span class="title">下一页 »</span>
    <br>
    <span>Value Profiles for Encoding Human Variation</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
    <footer>
        <p>&copy; 2025 AI研究资料库</p>
    </footer>
</body>
</html> 