<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，... - AI研究资料库</title>
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <div class="header-content">
        <h1><a href="/">AI研究资料库</a></h1>
        <nav>
            
            <a href="/papers/">论文</a>
            
            <a href="/tags/">标签</a>
            
            <a href="/categories/">分类</a>
            
            <a href="/search/">搜索</a>
            
        </nav>
        
<div class="language-switcher">
    
    <a href="https://example.org/" class="active">
        中文
    </a>
    
    <a href="https://example.org/en/" class="">
        English
    </a>
    
</div>
 
    </div>
</header> 
    
    <main>
        

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://example.org/">首页</a>&nbsp;»&nbsp;<a href="https://example.org/papers/">Papers</a></div>
    <h1 class="post-title">
      #路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，...
    </h1>
    <div class="post-meta"><span title='2025-03-20 03:15:00 +0000 +0000'>三月 20, 2025</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;AI Research Repository&nbsp;|&nbsp;<a href="https://github.com/yourusername/yourrepository/tree/main/content/papers/2025-03-20/llmscaling-up-llmrouting-llm8500llm.md" rel="noopener noreferrer" target="_blank">建议修改</a>

</div>
  </header> 
  <div class="original-link">
    <a href="https://weibo.com/6105753431/Pjgszzzde" target="_blank" rel="noopener">查看原文</a>
  </div>

  <div class="post-content"><h1 id="路由llm最全面探索笔记本也能玩的大模型scaling-up研究-事关路由llmrouting-llm一项截至目前最全面的研究来了共计收集和整理了涉及8500个llm">#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500+个LLM，&hellip;<a hidden class="anchor" aria-hidden="true" href="#路由llm最全面探索笔记本也能玩的大模型scaling-up研究-事关路由llmrouting-llm一项截至目前最全面的研究来了共计收集和整理了涉及8500个llm">#</a></h1>
<p><strong>原始链接</strong>: <a href="https://weibo.com/6105753431/Pjgszzzde">查看原文</a></p>
<h2 id="原始摘要">原始摘要<a hidden class="anchor" aria-hidden="true" href="#原始摘要">#</a></h2>
<p><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E8%B7%AF%E7%94%B1LLM%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23&amp;extparam=%23%E8%B7%AF%E7%94%B1LLM%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23" data-hide=""><span class="surl-text">#路由LLM最全面探索#</span></a><a href="https://m.weibo.cn/search?containerid=231522type%3D1%26t%3D10%26q%3D%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BScaling+Up%E7%A0%94%E7%A9%B6%23&amp;extparam=%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8BScaling+Up%E7%A0%94%E7%A9%B6%23" data-hide=""><span class="surl-text">#笔记本也能玩的大模型Scaling Up研究#</span></a> <br><br>事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——<br><br>共计收集和整理了涉及8500+个LLM，在12个Benchmark上的共2亿条性能记录！<br><br>先来简单科普一下路由LLM。<br><br>这种方法主要是把像ChatGPT、Qwen、DeepSeek这些成型的LLM当作 “专家” ，当给一个输入的时候，有分类能力的Router（路由器）就会把这个输入分配给合适的LLM处理。<br><br>如此一来，就能实现高性能、低计算消耗、低幻觉等目标。<br><br>而来自中山大学和普渡大学的研究人员在基于上述海量的记录做了一番探索之后，发现了一个现象，叫做Model-level Scaling Up。<br><br>一言蔽之，就是一个好的Router，可以让路由LLM范式的性能随着LLM候选数量的增加迅速变强。<br><br>随后，他们通过这些数据构建了针对Router设计的评测RouterEval。<br><br>值得注意的是，其他研究人员，也可以通过RouterEval在很少的计算资源下（如笔记本、单卡GPU上）就能参与到该路由LLM的研究当中。<a href="https://weibo.cn/sinaurl?u=https%3A%2F%2Fmp.weixin.qq.com%2Fs%2FOQFUcemTEmGC0eKUO_Fuiw" data-hide=""><span class="url-icon"><img style="width: 1rem;height: 1rem" src="https://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png" referrerpolicy="no-referrer"></span><span class="surl-text">网页链接</span></a><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1hzm99rc631j30u0091dit.jpg" referrerpolicy="no-referrer"><br><br><img style="" src="https://tvax3.sinaimg.cn/large/006Fd7o3ly1hzm9aggvavj311w0juwlw.jpg" referrerpolicy="no-referrer"><br><br></p>
<h2 id="ai-摘要">AI 摘要<a hidden class="anchor" aria-hidden="true" href="#ai-摘要">#</a></h2>
<p>中山大学和普渡大学的研究团队对路由LLM（Routing LLM）进行了迄今为止最全面的研究，涉及8500多个LLM在12个基准测试中的2亿条性能记录。路由LLM通过将多个LLM视为“专家”，并由Router（路由器）根据输入选择最合适的LLM进行处理，从而实现高性能、低计算消耗和低幻觉的目标。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，称为“Model-level Scaling Up”。团队还开发了RouterEval评测工具，使其他研究人员能够在有限的计算资源（如笔记本或单卡GPU）上参与路由LLM的研究。</p>
<h2 id="元数据">元数据<a hidden class="anchor" aria-hidden="true" href="#元数据">#</a></h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T03:15:14Z</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://example.org/tags/%E8%B7%AF%E7%94%B1llm%E6%9C%80%E5%85%A8%E9%9D%A2%E6%8E%A2%E7%B4%A2%23%23%E7%AC%94%E8%AE%B0%E6%9C%AC%E4%B9%9F%E8%83%BD%E7%8E%A9%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8Bscaling/">路由LLM最全面探索##笔记本也能玩的大模型Scaling</a></li>
      <li><a href="https://example.org/tags/up%E7%A0%94%E7%A9%B6%23/">Up研究#</a></li>
      <li><a href="https://example.org/tags/llm%E4%B8%80%E9%A1%B9%E6%88%AA%E8%87%B3%E7%9B%AE%E5%89%8D%E6%9C%80%E5%85%A8%E9%9D%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%9D%A5%E4%BA%86%E5%85%B1%E8%AE%A1%E6%94%B6%E9%9B%86%E5%92%8C%E6%95%B4%E7%90%86%E4%BA%86%E6%B6%89%E5%8F%8A8500&#43;%E4%B8%AAllm/">LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，</a></li>
      <li><a href="https://example.org/tags/llm%E8%BF%9B%E8%A1%8C%E4%BA%86%E8%BF%84%E4%BB%8A%E4%B8%BA%E6%AD%A2%E6%9C%80%E5%85%A8%E9%9D%A2%E7%9A%84%E7%A0%94%E7%A9%B6%E6%B6%89%E5%8F%8A8500%E5%A4%9A%E4%B8%AAllm%E5%9C%A812%E4%B8%AA%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E4%B8%AD%E7%9A%842%E4%BA%BF%E6%9D%A1%E6%80%A7%E8%83%BD%E8%AE%B0%E5%BD%95%E8%B7%AF%E7%94%B1llm%E9%80%9A%E8%BF%87%E5%B0%86%E5%A4%9A%E4%B8%AAllm%E8%A7%86%E4%B8%BA%E4%B8%93%E5%AE%B6%E5%B9%B6%E7%94%B1router%E8%B7%AF%E7%94%B1%E5%99%A8%E6%A0%B9%E6%8D%AE%E8%BE%93%E5%85%A5%E9%80%89%E6%8B%A9%E6%9C%80%E5%90%88%E9%80%82%E7%9A%84llm%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86%E4%BB%8E%E8%80%8C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%80%A7%E8%83%BD%E4%BD%8E%E8%AE%A1%E7%AE%97%E6%B6%88%E8%80%97%E5%92%8C%E4%BD%8E%E5%B9%BB%E8%A7%89%E7%9A%84%E7%9B%AE%E6%A0%87%E7%A0%94%E7%A9%B6%E5%8F%91%E7%8E%B0%E9%9A%8F%E7%9D%80llm%E5%80%99%E9%80%89%E6%95%B0%E9%87%8F%E7%9A%84%E5%A2%9E%E5%8A%A0%E8%B7%AF%E7%94%B1llm%E7%9A%84%E6%80%A7%E8%83%BD%E6%98%BE%E8%91%97%E6%8F%90%E5%8D%87%E7%A7%B0%E4%B8%BAmodel-level/">LLM）进行了迄今为止最全面的研究，涉及8500多个LLM在12个基准测试中的2亿条性能记录。路由LLM通过将多个LLM视为“专家”，并由Router（路由器）根据输入选择最合适的LLM进行处理，从而实现高性能、低计算消耗和低幻觉的目标。研究发现，随着LLM候选数量的增加，路由LLM的性能显著提升，称为“Model-Level</a></li>
      <li><a href="https://example.org/tags/scaling/">Scaling</a></li>
    </ul>
    
    
    <div class="related-content">
      <h3>Related Content</h3>
      <ul>
        <li><a href="/papers/2025-03-19/llmscaling-up-llmrouting-llm8500llm/">#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，...</a></li>
      </ul>
    </div>

    
    <div class="tagged-related">
      <h3>相关主题</h3>
      <ul>
        <li><a href="/papers/2025-03-19/llmscaling-up-llmrouting-llm8500llm/">#路由LLM最全面探索##笔记本也能玩的大模型Scaling Up研究# 事关路由LLM（Routing LLM），一项截至目前最全面的研究，来了——共计收集和整理了涉及8500&#43;个LLM，...</a></li>
        <li><a href="/papers/2025-03-20/infinity-scaling-bitwise-autoregressive-modeling-f/">Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis网页链接本文介绍了一种名为Infinity的位运算视觉自回归模型，该模型...</a></li>
      </ul>
    </div>
<nav class="paginav">
  <a class="prev" href="https://example.org/papers/2025-03-20/latentsync-taming-audio-conditioned-latent-diffusi/">
    <span class="title">« 上一页</span>
    <br>
    <span>LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision 网页链接本研究提出了一种名为LatentSync的端到端唇同...</span>
  </a>
  <a class="next" href="https://example.org/papers/2025-03-20/33htmlpython/">
    <span class="title">下一页 »</span>
    <br>
    <span>#豆包编程能力升级##用豆包3分钟做个小游戏#豆包编程能力升级了，现在3分钟就能做出专属小游戏。升级后，豆包支持HTML代码实时预览、Python运行，甚至是生成完整...</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
    <footer>
        <p>&copy; 2025 AI研究资料库</p>
    </footer>
</body>
</html> 