<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment - AI研究资料库</title>
    <link rel="stylesheet" href="/css/style.css">
</head>
<body>
    
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark');
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <div class="header-content">
        <h1><a href="/">AI研究资料库</a></h1>
        <nav>
            
            <a href="/papers/">论文</a>
            
            <a href="/tags/">标签</a>
            
            <a href="/categories/">分类</a>
            
            <a href="/search/">搜索</a>
            
        </nav>
        
<div class="language-switcher">
    
    <a href="https://example.org/" class="active">
        中文
    </a>
    
    <a href="https://example.org/en/" class="">
        English
    </a>
    
</div>
 
    </div>
</header> 
    
    <main>
        

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://example.org/">首页</a>&nbsp;»&nbsp;<a href="https://example.org/papers/">Papers</a></div>
    <h1 class="post-title">
      From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment
    </h1>
    <div class="post-meta"><span title='2025-03-20 13:01:00 +0000 +0000'>三月 20, 2025</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;AI Research Repository&nbsp;|&nbsp;<a href="https://github.com/yourusername/yourrepository/tree/main/content/papers/2025-03-20/from-1000000-users-to-every-user-scaling-up-person.md" rel="noopener noreferrer" target="_blank">建议修改</a>

</div>
  </header> 
  <div class="original-link">
    <a href="http://arxiv.org/abs/2503.15463v1" target="_blank" rel="noopener">查看原文</a>
  </div>

  <div class="post-content"><h1 id="from-1000000-users-to-every-user-scaling-up-personalized-preference-for-user-level-alignment">From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment<a hidden class="anchor" aria-hidden="true" href="#from-1000000-users-to-every-user-scaling-up-personalized-preference-for-user-level-alignment">#</a></h1>
<p><strong>原始链接</strong>: <a href="http://arxiv.org/abs/2503.15463v1">查看原文</a></p>
<h2 id="原始摘要">原始摘要<a hidden class="anchor" aria-hidden="true" href="#原始摘要">#</a></h2>
<p>Large language models (LLMs) have traditionally been aligned through
one-size-fits-all approaches that assume uniform human preferences,
fundamentally overlooking the diversity in user values and needs. This paper
introduces a comprehensive framework for scalable personalized alignment of
LLMs. We establish a systematic preference space characterizing psychological
and behavioral dimensions, alongside diverse persona representations for robust
preference inference in real-world scenarios. Building upon this foundation, we
introduce \textsc{AlignX}, a large-scale dataset of over 1.3 million
personalized preference examples, and develop two complementary alignment
approaches: \textit{in-context alignment} directly conditioning on persona
representations and \textit{preference-bridged alignment} modeling intermediate
preference distributions. Extensive experiments demonstrate substantial
improvements over existing methods, with an average 17.06% accuracy gain
across four benchmarks while exhibiting a strong adaptation capability to novel
preferences, robustness to limited user data, and precise preference
controllability. These results validate our framework&rsquo;s effectiveness,
advancing toward truly user-adaptive AI systems.</p>
<h2 id="ai-摘要">AI 摘要<a hidden class="anchor" aria-hidden="true" href="#ai-摘要">#</a></h2>
<p>本文提出了一种可扩展的个性化对齐大语言模型（LLMs）的框架，突破了传统“一刀切”方法的局限。通过建立系统化的偏好空间和多样化的人物表征，结合包含130万个性化偏好示例的AlignX数据集，开发了两种对齐方法：基于人物表征的上下文对齐和基于中间偏好分布的偏好桥接对齐。实验表明，该方法在四个基准测试中平均提升了17.06%的准确率，展现出对新偏好的强适应性、对有限用户数据的鲁棒性以及精确的偏好控制能力，推动了真正适应用户的AI系统的发展。</p>
<h2 id="元数据">元数据<a hidden class="anchor" aria-hidden="true" href="#元数据">#</a></h2>
<ul>
<li><strong>来源</strong>: ArXiv</li>
<li><strong>类型</strong>: 论文</li>
<li><strong>保存时间</strong>: 2025-03-20T13:01:49+08:00</li>
<li><strong>目录日期</strong>: 2025-03-20</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://example.org/tags/users/">Users</a></li>
      <li><a href="https://example.org/tags/user/">User</a></li>
      <li><a href="https://example.org/tags/up/">Up</a></li>
      <li><a href="https://example.org/tags/user-level/">User-Level</a></li>
      <li><a href="https://example.org/tags/alignment/">Alignment</a></li>
    </ul>
    
    

    
<nav class="paginav">
  <a class="prev" href="https://example.org/papers/2025-03-20/subversion-strategy-eval-can-language-models-state/">
    <span class="title">« 上一页</span>
    <br>
    <span>Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?</span>
  </a>
  <a class="next" href="https://example.org/papers/2025-03-20/more-information-is-not-always-better-connections-/">
    <span class="title">下一页 »</span>
    <br>
    <span>More Information is Not Always Better: Connections between Zero-Sum Local Nash Equilibria in Feedback and Open-Loop Information Patterns</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
    <footer>
        <p>&copy; 2025 AI研究资料库</p>
    </footer>
</body>
</html> 